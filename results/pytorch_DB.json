{
    "/torch/_functorch/aot_autograd.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"aot_autograd.py\" file is to provide a framework for Ahead-of-Time (AOT) compilation of PyTorch models and functions, enabling optimizations and performance improvements by converting them into a format suitable for execution on specialized hardware. This file includes utilities for tracing, functionalizing, and compiling PyTorch models and functions, handling various edge cases related to input mutations, metadata changes, and aliasing, and generating metadata and signatures for exported models. The file also supports different compilation modes, such as autograd, inference, and export, and allows customization through configuration options and compiler functions.",
            "total_tokens": 18344,
            "latency": 2.230268,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file `aot_autograd.py` is a part of PyTorch's AOT Autograd (Ahead-Of-Time Autograd) module. This module is designed to optimize the performance of PyTorch models by compiling the forward and backward passes of a model into a single, efficient function.\n\nThe main functionality of this file is to provide two main functions: `aot_function` and `aot_module`. These functions take in a Python function or a PyTorch module, respectively, and trace the forward and backward passes of the function or module using PyTorch's dispatch mechanism. The traced forward and backward graphs are then compiled using a specified compiler function, and the resulting compiled function is returned.\n\nThe `aot_function` function takes in a Python function, a forward compiler function, and an optional backward compiler function. It also takes in several optional parameters, such as a partition function, decompositions, and whether or not to enable dynamic shapes. The function first flattens the input arguments of the Python function and then traces the forward and backward graphs of the function using PyTorch's dispatch mechanism. The traced forward and backward graphs are then partitioned using the specified partition function, and the partitioned graphs are compiled using the specified compiler functions. The resulting compiled forward and backward functions are then wrapped up in a `torch.autograd.Function` object, and the original Python function is replaced with this new object.\n\nThe `aot_module` function takes in a PyTorch module, a forward compiler function, and an optional backward compiler function. It also takes in several optional parameters, such as a partition function, decompositions, and whether or not to enable dynamic shapes. The function first lifts the parameters and buffers of the module as inputs to a new callable, which is then compiled using the `aot_function` function. The resulting compiled callable is then wrapped up in a new `nn.Module` object, which retains the eager behavior of the original module but with the forward and backward graphs compiled.\n\nThe file also provides several helper functions, such as `aot_export_module`, `aot_export_joint_simple`, and `_aot_export_function`, which are used to export the forward and backward graphs of a module as a single FX graph. These functions are used by higher-",
            "total_tokens": 500,
            "latency": 13.617909,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The given Python file contains the implementation of \"Ahead-of-Time Autograd\" (AOT Autograd) functionality within PyTorch's framework. Its main purpose is to trace the forward and backward computational graphs ahead of time (AOT), handle mutation and aliasing edge cases systematically, and compile these functionalized graphs to optimize execution during training or inference. It provides advanced support for compiling PyTorch models and functions into optimized forms, respecting constraints like mutation elimination and alias handling for correct gradient computation.",
            "total_tokens": 15631,
            "latency": 16.967573,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file provides functionality for exporting PyTorch modules using Ahead-of-Time (AOT) compilation. It traces forward and backward graphs of functions or modules using torch dispatch tracing mechanisms and compiles them using specified compilers. The file contains functions to handle aliasing, mutations, and edge cases around inputs, outputs, metadata, and tensors, ensuring efficient compilation of graphs for export or inference purposes. Additionally, it includes functions for processing inputs, creating synthetic bases, and setting up calling conventions for joint graphs, enabling the generation of functionalized and efficient graphs for export.",
            "total_tokens": 15563,
            "latency": 2.938408,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"aot_autograd.py\" file is to provide ahead-of-time (AOT) compilation and optimization for PyTorch modules and functions. It includes implementations for tracing and compiling forward and backward graphs, handling input mutations and aliasing, and exporting compiled modules for various use cases including inference and training.",
            "total_tokens": 19782,
            "latency": 5.028594,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `aot_autograd.py` file is to provide a set of tools and decorators for tracing, compiling, and exporting PyTorch models and functions using Ahead-of-Time (AOT) compilation techniques. It includes functionalities for handling input and output mutations, aliasing, and functionalization of graphs, as well as utilities for creating and managing fake tensors and metadata. The file supports both training and inference modes, and offers simplified versions of its main functions for reduced overhead in certain use cases.",
            "total_tokens": 18324,
            "latency": 5.021529,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `\"aot_autograd.py\"` file provides a framework for advanced compilation and optimization of PyTorch functions and models using Ahead-of-Time (AOT) Autograd. It introduces tools for tracing functions, partitioning forward and backward graphs, managing complex edge cases involving input/output mutations, aliasing, and custom dispatch mechanisms while ensuring functional graphs for efficient compilation. The file also includes utilities to export models to functional intermediate representations and handle dynamic shape inference and specialized compilation scenarios.",
            "total_tokens": 15627,
            "latency": 2.631832,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the file is to provide a set of functions and classes for Ahead-of-Time (AOT) Autograd, which is a mechanism for compiling PyTorch models to improve their performance. The file includes functions for tracing and compiling forward and backward graphs, as well as classes for representing AOT Autograd configurations and metadata. The file also includes various utility functions for working with PyTorch tensors, modules, and graphs.",
            "total_tokens": 15537,
            "latency": 48.957108,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file 'aot_autograd.py' primarily deals with the augmentation and compilation of computational graphs in PyTorch, particularly targeting automatic differentiation and export functionality tailored for customized performance enhancements like GPU acceleration or streamlined deployment. This includes selectively compiling forward and backward graph computations based on runtime configurations, handling dynamic shapes, and ensuring the proper handling of module attributes to maintain support for PyTorch\u2019s module framework. The file supports generating both inference and training graphs according to the needs specified and handles specialized cases like mutations in inputs and side-effects from certain operations. It also incorporates graph partitioning and captures operational metadata that can be useful for debugging and further optimization processes.",
            "total_tokens": 15580,
            "latency": 5.020207,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement Ahead-of-Time (AOT) Autograd, a mechanism for compiling PyTorch models to improve performance by tracing and compiling the forward and backward graphs of a model ahead of time. The file provides functions such as `aot_function` and `aot_module` to compile PyTorch functions and modules, respectively, and `aot_export_module` to export a compiled model. The compilation process involves tracing the model's forward and backward graphs, applying optimizations, and generating a compiled function that can be executed at runtime.",
            "total_tokens": 15567,
            "latency": 4.278123,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file \"aot_autograd.py\" provides advanced functionality for tracing, transforming, and compiling PyTorch functions and nn.Modules using the Ahead-Of-Time (AOT) Autograd system. Its main role is to convert PyTorch code (including forward and backward computations) into functional, traceable graphs, allowing custom compilation, export, and optimization while carefully managing input/output aliasing, mutations, and tensor subclass behavior. The module exposes utilities and APIs for compiling standalone functions or full modules, handling decompositions, exporting computation graphs, and supporting backends that require functionalized, stateless computational graphs suitable for further optimization or deployment.",
            "total_tokens": 15660,
            "latency": 2.77618,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_functorch/aot_autograd.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file implements AOT (Ahead-of-Time) Autograd for PyTorch, providing mechanisms to trace and compile Python functions and `nn.Module`s. It handles complex tensor behaviors like mutations, aliasing, and subclasses by functionalizing the computation graph. The system generates compiled runtime functions or exportable FX graphs suitable for various backends and use cases, including training and inference.",
            "total_tokens": 19423,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.887094
        },
        "gemini-2.5": {
            "file_name": "/torch/_functorch/aot_autograd.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file implements the core logic for PyTorch's AOT Autograd mechanism. It traces user-defined functions or `nn.Module`s, functionalizes the computation graph to handle side effects like mutations and aliasing, and partitions the graph into forward and backward passes if needed. Finally, it provides hooks (`aot_function`, `aot_module`, `aot_export_module`) to compile these graphs using user-specified backend compilers or prepare them for export.",
            "total_tokens": 19465,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.432005
        }
    },
    "/torch/fx/graph.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file defines a `Graph` class and related functionalities for representing and manipulating computational graphs, primarily used in the context of symbolic tracing and intermediate representation in PyTorch's FX framework. The `Graph` class models a series of interconnected nodes, each representing a callsite or other syntactic constructs, which together form a valid Python function. The file includes methods for creating various types of nodes (placeholders, attribute fetches, module calls, method calls, function calls, and outputs), managing the graph's namespace and insertion points, generating Python code from the graph, and performing linting and dead code elimination.",
            "total_tokens": 19591,
            "latency": 2.323423,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file `graph.py` is a part of the PyTorch FX library, which provides tools for analyzing and transforming PyTorch models. The main functionality of this file is to define the `Graph` class, which represents a computation graph.\n\nThe `Graph` class has several methods for creating and manipulating nodes in the graph. These methods include:\n\n* `create_node`: Creates a new node with the given opcode, target, arguments, and keyword arguments.\n* `placeholder`: Creates a placeholder node with the given name and type annotation.\n* `get_attr`: Creates a node that gets the attribute with the given qualified name.\n* `call_module`: Creates a node that calls the forward method of the given module.\n* `call_method`: Creates a node that calls the given method on the first argument.\n* `call_function`: Creates a node that calls the given function with the given arguments and keyword arguments.\n* `output`: Creates an output node with the given value.\n* `erase_node`: Erases the given node from the graph.\n* `inserting_before` and `inserting_after`: Set the point at which new nodes will be inserted into the graph.\n* `node_copy`: Creates a copy of the given node in the current graph.\n* `python_code`: Generates Python code that represents the computation graph.\n* `lint`: Checks the graph for errors and warnings.\n* `eliminate_dead_code`: Removes all dead code from the graph.\n* `set_codegen`: Sets a custom code generation function.\n* `on_generate_code`: Registers a code transformation function that will be applied when generating Python code.\n\nThe `Graph` class also has several helper methods for working with nodes, such as `find_nodes`, `process_inputs`, and `process_outputs`. These methods are used to map the values in the graph to their corresponding Python expressions.\n\nOverall, the `Graph` class provides a high-level interface for working with computation graphs in PyTorch. It allows users to easily create, manipulate, and analyze graphs, as well as generate Python code that represents the graph.",
            "total_tokens": 499,
            "latency": 13.994906,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided file is a comprehensive implementation of a graph-based intermediate representation used for symbolic manipulation and transformation of Python code, specifically geared towards PyTorch models. It includes classes and mechanisms for representing and manipulating code as nodes, generating source code from graph representations, supporting code transformations, and providing utility functions to analyze and optimize graphs like eliminating dead code or linting. Essentially, this module serves as infrastructure for PyTorch's FX (functional transforms) subsystem in enabling tracing and transformation of neural network code.",
            "total_tokens": 16880,
            "latency": 16.092808,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "This Python file defines the Graph class, which is a core component of the PyTorch FX (Function Transforms) system. The main functionality of this file is to provide a data structure and methods for representing and manipulating computational graphs. Here's a summary of the key aspects:\n\n1. The Graph class represents a graph of nodes, where each node corresponds to an operation in a PyTorch computation.\n\n2. It provides methods for creating different types of nodes (e.g., placeholder, get_attr, call_module, call_method, call_function, output) and manipulating the graph structure.\n\n3. The class includes functionality for code generation, allowing the graph to be converted back into executable Python code.\n\n4. It also includes utility methods for graph analysis, such as dead code elimination and linting.\n\nIn essence, this file is crucial for the intermediate representation used in PyTorch's FX system, enabling various code transformations and optimizations on PyTorch models.",
            "total_tokens": 20990,
            "latency": 8.359343,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `graph.py` file is to define the `Graph` class, which represents the intermediate representation used in the FX framework. This class allows for the creation, manipulation, and transformation of computational graphs, enabling the generation of Python code from these graphs and facilitating various optimizations and analyses.",
            "total_tokens": 19529,
            "latency": 4.062797,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `graph.py` file is dedicated to implementing a flexible intermediate representation (IR) for computational graphs, primarily within PyTorch's FX module. It provides a `Graph` class that manages a collection of `Node` objects representing operations (`call_function`, `call_method`, `placeholder`, etc.), enabling tracing, manipulation, and code generation for Python functions representing deep learning models. Additionally, it includes utilities for generating Python source code, ensuring correctness via linting, handling name uniqueness, and optimizing graphs through dead code elimination.",
            "total_tokens": 16887,
            "latency": 2.37422,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the given Python file is to define a Graph class, which represents a data structure used in the FX Intermediate Representation. The Graph class consists of a series of Node objects, each representing callsites or other syntactic constructs, and provides methods for creating, manipulating, and analyzing the graph. The file also defines a CodeGen class, which is responsible for generating Python code from the graph, and a _PyTreeCodeGen class, which is a subclass of CodeGen that handles PyTree logic.",
            "total_tokens": 16761,
            "latency": 49.835752,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The provided Python file defines a module primarily focused on generating Python code from an intermediate representation, such as a computation graph, often utilized in frameworks like PyTorch. It includes classes and methods to transform, manage node relationships, maintain the correct execution order, and convert these relationships and operations into structured Python code. The file supports handling different data types, managing side effects in the code generation, and adapting to changes made to the computational graph through direct manipulation of nodes and operations, ensuring the integrity and accuracy of the generated Python code.",
            "total_tokens": 16759,
            "latency": 5.396311,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to define the `Graph` class, which represents the main data structure used in the FX Intermediate Representation. The `Graph` class consists of a series of `Node` objects, each representing callsites or other syntactic constructs, and provides methods for creating and manipulating these nodes, as well as generating Python code from the graph. The file also defines several related classes and functions, including `Node`, `CodeGen`, and `PythonCode`, which are used to support the functionality of the `Graph` class.",
            "total_tokens": 16771,
            "latency": 4.26742,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines and implements an FX Graph Intermediate Representation system used by PyTorch to capture, represent, and transform computation graphs as sequences of Nodes, enabling symbolic tracing and manipulation of Python code and models. It provides the core Graph class along with utilities for safe node naming, efficient node lookup, namespace management, graph copying, dead code elimination, and flexible Python code generation from the graph via the CodeGen and PythonCode classes. The module also supports advanced features such as maintaining pytree structure, on-the-fly code transformation, and integration with Torch's namespace and special operator formatting, to facilitate model analysis, transformation, and code export.",
            "total_tokens": 16909,
            "latency": 3.036516,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/fx/graph.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This file defines the `Graph` class, the core data structure for PyTorch FX's intermediate representation, which represents a Python function as a directed acyclic graph of `Node` objects corresponding to operations. It provides methods to construct and manipulate this graph, including adding, copying, erasing nodes, and managing insertion points. The file also includes the `CodeGen` class and associated utilities for generating executable Python code from the `Graph`, handling namespaces, formatting, and supporting features like PyTree integration.",
            "total_tokens": 20474,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.230827
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/fx/graph.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `Graph` class, a core data structure in PyTorch's FX library representing a computation graph as a sequence of `Node` objects. It provides methods for building and manipulating this graph, such as creating, erasing, and copying nodes. Additionally, it includes functionality to generate executable Python code from the graph representation, handling aspects like variable naming, imports, and type annotations.",
            "total_tokens": 20166,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.411046
        }
    },
    "/tools/linter/adapters/workflow_consistency_linter.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `workflow_consistency_linter.py` file is to lint GitHub workflow YAML files for consistency among jobs that share the same `sync-tag`. It checks that jobs with the same `sync-tag` have identical configurations, excluding certain fields like `if` and `test-matrix`, and reports any inconsistencies as lint errors. The script can be run from the command line, taking one or more workflow file paths as input.",
            "total_tokens": 1191,
            "latency": 1.051534,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The Python file \"workflow\\_consistency\\_linter.py\" is a script that checks for consistency of jobs between different GitHub workflows. It does this by comparing the jobs with the same \"sync-tag\" across all the provided files. If any inconsistencies are found, it prints out a lint message with the path, line number, and a description of the issue. The script also allows for different triggering conditions and test matrices for jobs with the same \"sync-tag\". The script takes a list of file paths as input and can be run from the command line.",
            "total_tokens": 120,
            "latency": 2.791189,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This script acts as a consistency linter for GitHub workflow files, verifying that jobs sharing a common `sync-tag` have identical configurations. It scans multiple YAML workflow files, omitting differences allowed in specified keys, and logs issues when jobs with the same tag differ. Detected inconsistencies are outputted as structured JSON lint messages that specify file locations and problem details.",
            "total_tokens": 1049,
            "latency": 3.961656,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The python file `workflow_consistency_linter.py` is a script that checks for consistency of jobs between different GitHub workflows. It reads YAML workflow files provided as arguments, validates if they contain jobs, and then compares jobs based on a specific `sync-tag`. If jobs with the same `sync-tag` do not match, it outputs lint messages indicating inconsistencies. The script aims to ensure that all jobs with a specific `sync-tag` are consistent within the workflow files.",
            "total_tokens": 1056,
            "latency": 1.262518,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"workflow_consistency_linter.py\" file is to check for consistency between jobs in different GitHub workflows that share the same \"sync-tag\". It analyzes YAML files containing workflow definitions, compares jobs with matching sync tags, and reports any inconsistencies as lint errors. The script also provides a command-line interface for specifying the files to be checked.",
            "total_tokens": 1274,
            "latency": 2.975642,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `workflow_consistency_linter.py` script checks for consistency of jobs between different GitHub workflows by ensuring that any job with a specific `sync-tag` matches all other jobs with the same `sync-tag`. It processes YAML workflow files, compares jobs with the same `sync-tag`, and prints lint messages for inconsistencies.",
            "total_tokens": 1166,
            "latency": 0.989755,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The Python file `workflow_consistency_linter.py` is a linter tool designed to ensure consistency across GitHub workflow jobs that share the same `sync-tag`. It analyzes YAML configuration files, identifies jobs with matching `sync-tag` values, and verifies that these jobs are identical apart from specific allowed variations, such as conditions in the \"if\" field or the \"test-matrix\". If inconsistencies are detected, the tool outputs structured lint messages indicating the discrepancies.",
            "total_tokens": 1069,
            "latency": 1.50831,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to check for consistency of jobs between different GitHub workflows. It does this by parsing YAML files, identifying jobs with a specific `sync-tag`, and ensuring that all jobs with the same `sync-tag` have the same configuration. If any inconsistencies are reported as lint messages in JSON format.",
            "total_tokens": 1033,
            "latency": 2.813256,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The file `workflow_consistency_linter.py` serves as a linter to ensure consistency across GitHub workflow files that use a specific synchronization tag (`sync-tag`). The script examines multiple YAML workflow files to detect jobs tagged with the same `sync-tag` and checks if their configurations match exactly, excluding conditional fields or test matrices which can vary. It prints out linting messages in JSON format for any discrepancies found, helping maintain standardized job configurations across similar workflows.",
            "total_tokens": 1053,
            "latency": 4.882183,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `workflow_consistency_linter.py`, is to check for consistency of jobs between different GitHub workflows, specifically ensuring that jobs with the same `sync-tag` have identical configurations. The script achieves this by parsing YAML files, aggregating jobs with matching `sync-tag` values, and comparing their configurations to detect any inconsistencies. It reports any inconsistencies found as lint messages in JSON format, highlighting the paths and lines where the inconsistencies occur.",
            "total_tokens": 1064,
            "latency": 1.392245,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The main functionality of workflow_consistency_linter.py is to lint multiple GitHub Actions workflow YAML files to ensure that any jobs sharing the same sync-tag have identical job definitions, except for permitted differences (e.g., the if field or with.test-matrix). It parses the specified YAML workflow files, groups jobs by their sync-tag, and compares them, reporting inconsistencies as lint error messages in JSON format indicating the file and job causing the mismatch. This tool helps maintain synchronization and consistency between shared workflow job definitions across multiple files.",
            "total_tokens": 1081,
            "latency": 1.536632,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/linter/adapters/workflow_consistency_linter.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script acts as a linter for GitHub Actions workflow files. It checks for consistency among jobs that share the same `sync-tag` across different workflow files. Jobs with the same `sync-tag` are expected to have identical configurations (excluding `if` and `test-matrix` fields), and inconsistencies are reported as lint errors.",
            "total_tokens": 1426,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.039555
        },
        "gemini-2.5": {
            "file_name": "/tools/linter/adapters/workflow_consistency_linter.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script acts as a linter for GitHub Actions workflow YAML files. It checks that jobs across different workflow files which share the same `sync-tag` value in their `with` section have identical configurations, ignoring specific fields like `if` conditions and `test-matrix`. If inconsistencies are detected between jobs with the same tag, it prints JSON-formatted error messages indicating the file and job causing the discrepancy.",
            "total_tokens": 1613,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.387007
        }
    },
    "/torch/fx/experimental/migrate_gradual_types/constraint.py": {
        "nova-lite-v1": {
            "file_summary": "The `constraint.py` file defines a set of constraint classes and utility functions for representing and manipulating constraints related to tensor operations and dimensions in a symbolic manner. These constraints include binary operations, conjunctions, disjunctions, and specific operations like reshaping, transposing, and convolution. The classes provide methods for equality checks and string representations, facilitating the symbolic reasoning and manipulation of tensor constraints.",
            "total_tokens": 4627,
            "latency": 0.960443,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "titan_premier": {
            "file_summary": "The python file \"constraint.py\" contains a class hierarchy for representing constraints in a system. The constraints include conjunctions, disjunctions, products, and binary operations on tensors and dimensions. The file also includes helper functions for checking if a constraint is an algebraic expression or a boolean expression. The constraints are used to represent various operations such as index selection, transpose, and convolution. The file is part of a larger system for migrating gradual types in PyTorch.",
            "total_tokens": 97,
            "latency": 2.819873,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.5": {
            "file_summary": "This Python file defines a structured way to represent various constraints for tensor operations, primarily geared towards representing types, dimensions, and shape constraints in computations involving tensors. It provides a hierarchy of constraint classes (such as binary constraints for tensor and dimension constraints, reshaping, broadcasting, indexing, transposing, convolutions, pooling, and algebraic operations), along with helper classes for tensor, dimension, and boolean variables. This structured approach is likely designed to facilitate constraint checking, validation, or type inference for dynamic tensor computation scenarios, particularly in frameworks involving tensor shape manipulations or type checking.",
            "total_tokens": 4158,
            "latency": 7.521991,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided python file \"constraint.py\" contains class definitions that represent various constraints applicable to tensors and dimensions in a program. These constraints include conjunctions, disjunctions, products, binary constraints, greatest upper bounds, reshape constraints, index selections, transpositions, convolutions, max pools, broadcasting, product calculations, and various variable types such as tensor variables (TVar), dimension variables (DVar), and boolean variables (BVar). The file also includes methods for comparison and string representation of the constraints.",
            "total_tokens": 4130,
            "latency": 1.615327,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"constraint.py\" file is to define a set of constraint classes and related data structures for representing and manipulating constraints in a tensor type system. It includes various constraint types such as conjunctions, disjunctions, binary constraints, and specialized constraints for tensor operations like convolution and broadcasting. The file also defines helper classes for representing tensor variables, dimension variables, and boolean variables, as well as utility functions for checking constraint types.",
            "total_tokens": 5093,
            "latency": 3.731095,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "nova-pro-v1": {
            "file_summary": "The `constraint.py` file defines various classes representing different types of constraints, primarily focused on tensors and their dimensions within a computational graph context. It includes classes for conjunctions, disjunctions, binary operations, and specific constraints like reshaping, transposing, and calculating convolutions. The file also provides utility functions to check the type of constraints and variables.",
            "total_tokens": 4622,
            "latency": 1.335093,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4o": {
            "file_summary": "The file defines a system of classes and methods for representing and manipulating type constraints, including operations on tensor shapes, dimensions, and algebraic expressions, largely tailored for use in PyTorch's type inference and shape system. It includes specialized classes for handling conjunctions, disjunctions, binary constraints, tensor-specific constraints, and operations like reshaping, indexing, broadcasting, and convolution. The focus is on creating a structured representation of constraints for dynamic type analysis and tensor computation verification.",
            "total_tokens": 4137,
            "latency": 2.46364,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a set of classes and functions for representing and manipulating constraints in the context of tensor operations, such as convolution, max pooling, and broadcasting. The constraints are used to enforce relationships between tensor dimensions, shapes, and values. The file provides a way to create, compare, and manipulate these constraints using various classes and functions.",
            "total_tokens": 4105,
            "latency": 3.752748,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4": {
            "file_summary": "The Python file defines a `Constraint` class and various subclasses to represent different types of constraints in a system, likely in tensor computations based on the Torch library. Classes such as `Conj`, `Disj`, `BinaryConstraint`, and `CalcConv` suggest handling logical, relational, and computational constraints. Further, the use of classes like `TVar` and operations from `torch.fx.experimental` indicate integration with tensor types and dynamic typing, supporting flexible dimension and type operations within tensor computations.",
            "total_tokens": 4128,
            "latency": 2.931484,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the given Python file is to define a set of classes and functions for representing and manipulating constraints related to tensor operations, such as convolution, max pooling, and broadcasting. These constraints are used to reason about the properties of tensors and their relationships, and are likely used in the context of a larger system for optimizing or analyzing neural networks. The file defines a range of constraint classes, including `BinaryConstraint`, `Conj`, `Disj`, and `Prod`, as well as classes for specific tensor operations like `CalcConv` and `CalcMaxPool`.",
            "total_tokens": 4147,
            "latency": 2.22207,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.1": {
            "file_summary": "This file defines a system of constraint classes for representing and reasoning about tensor, dimension, and boolean constraints within symbolic shape/type inference, likely connected to PyTorch's FX tracing tools. It provides a variety of constraint types (e.g. conjunctions, disjunctions, binary operations, reshape and indexing constraints, convolution/maxpool/broadcasting operations) and variable types (tensor, dimension, boolean), along with utility functions to identify the category of a constraint. The primary purpose of the file is to formalize constraints and operations between tensors and their shapes to enable compile-time or symbolic reasoning about tensor computations.",
            "total_tokens": 4163,
            "latency": 2.058685,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/fx/experimental/migrate_gradual_types/constraint.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a hierarchy of classes inheriting from `Constraint` to represent various types of constraints, primarily focused on tensor shapes and dimensions. It includes classes for logical combinations (Conj, Disj), binary operations, tensor/dimension bounds, and specific tensor operations like reshaping, indexing, convolution, and broadcasting. The file also defines variable types (`TVar`, `DVar`, `BVar`) used within these constraints and helper functions to categorize expressions.",
            "total_tokens": 5075,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.905499
        },
        "gemini-2.5": {
            "file_name": "/torch/fx/experimental/migrate_gradual_types/constraint.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a system for representing type constraints, primarily for tensors and their dimensions within the PyTorch FX framework. It establishes a base `Constraint` class and numerous subclasses to model logical combinations (And/Or), binary operations, tensor-specific operations (like convolution, broadcasting, indexing), and dimension calculations. The file also defines variable types (TVar, DVar, BVar) to represent unknown tensors, dimensions, and booleans within these constraints.",
            "total_tokens": 5151,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.645665
        }
    },
    "/torch/nn/modules/rnn.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file defines various recurrent neural network (RNN) modules using PyTorch, including RNN, LSTM, and GRU, along with their cell counterparts. The modules support both forward and bidirectional processing, dropout for regularization, and different non-linearities such as tanh and ReLU. They are designed to handle variable-length sequences and can be stacked to create deeper architectures. The file also includes utility functions and classes for managing the weights and parameters of these RNN modules.",
            "total_tokens": 20734,
            "latency": 2.087994,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The given Python file, `rnn.py`, contains the implementation of various Recurrent Neural Network (RNN) modules, including RNN, LSTM, GRU, RNNCell, LSTMCell, and GRUCell. These modules are subclasses of the `Module` class in PyTorch and are used to create RNN models.\n\nThe `RNNBase` class is the base class for all RNN modules and defines common attributes and methods. It initializes the weights and biases of the RNN module and provides methods for checking input and hidden states.\n\nThe `RNN` class is a specific implementation of the RNN module that uses the tanh or ReLU non-linearity. It takes in the input size, hidden size, number of layers, bias, batch first, dropout, bidirectional, and proj size as arguments.\n\nThe `LSTM` class is a specific implementation of the LSTM module that uses the LSTM cell. It takes in the input size, hidden size, number of layers, bias, batch first, dropout, bidirectional, and proj size as arguments.\n\nThe `GRU` class is a specific implementation of the GRU module that uses the GRU cell. It takes in the input size, hidden size, number of layers, bias, batch first, dropout, and bidirectional as arguments.\n\nThe `RNNCellBase` class is the base class for all RNN cell modules and defines common attributes and methods. It initializes the weights and biases of the RNN cell module and provides methods for checking input and hidden states.\n\nThe `RNNCell` class is a specific implementation of the RNN cell module that uses the tanh or ReLU non-linearity. It takes in the input size, hidden size, bias, and nonlinearity as arguments.\n\nThe `LSTMCell` class is a specific implementation of the LSTM cell module. It takes in the input size, hidden size, and bias as arguments.\n\nThe `GRUCell` class is a specific implementation of the GRU cell module. It takes in the input size, hidden size, and bias as arguments.\n\nOverall, the `rnn.py` file provides a comprehensive set of RNN modules and cells that can be used to create RNN models in PyTorch.",
            "total_tokens": 491,
            "latency": 13.756909,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python file implements recurrent neural network (RNN) modules including standard RNNs, Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRU), along with their corresponding cell-based variants using PyTorch. The core implementation is structured around an `RNNBase` class that handles initialization, parameter management, and internal checks for input and hidden states across these modules. The provided classes are ready-to-use PyTorch neural network layers designed for processing sequential data in tasks such as language modeling or sequence analysis.",
            "total_tokens": 18462,
            "latency": 12.468074,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"rnn.py\" file is to implement various Recurrent Neural Network (RNN) architectures and their cell variants in PyTorch. Specifically, it defines classes for RNN, LSTM, and GRU models, as well as their corresponding cell implementations (RNNCell, LSTMCell, and GRUCell). These classes provide the core building blocks for creating and using recurrent neural networks in PyTorch, including forward pass computations, parameter initialization, and handling of different input formats.",
            "total_tokens": 22509,
            "latency": 6.333466,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `rnn.py` file is to define various recurrent neural network (RNN) modules and cells in PyTorch, including RNN, LSTM, GRU, and their corresponding cell versions, along with their initialization, parameter management, and forward pass implementations.",
            "total_tokens": 20694,
            "latency": 3.949821,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The file `rnn.py` provides the implementation of core Recurrent Neural Network (RNN) modules, including general-purpose RNNs (`RNNBase`, `RNN`), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU), along with their corresponding cell-level classes (`RNNCell`, `LSTMCell`, `GRUCell`). These classes serve as the backbone for defining and managing recurrent layers in PyTorch, supporting features like multi-layer architecture, bidirectionality, dropout, and custom initialization of weights. The file also includes utility methods for handling parameter management, input validation, compatibility with batch-first tensors, and enabling efficient forward computations using the cuDNN library when possible.",
            "total_tokens": 18503,
            "latency": 4.880212,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to implement various types of Recurrent Neural Networks (RNNs) in PyTorch, including RNN, LSTM, and GRU.",
            "total_tokens": 18168,
            "latency": 50.650161,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The provided Python file defines several classes and functions for constructing and working with different types of recurrent neural networks (RNNs) such as vanilla RNNs, Long Short-Term Memory networks (LSTMs), and Gated Recurrent Unit networks (GRUs). It includes base classes and cell variations for each RNN type, and contains implementations for initializing RNN parameters, managing data flow through the network, and executing forward passes. The code leverages PyTorch's functionalities and modules for its operations, focusing on aspects like weight initializations, forward computations, and parameter management.",
            "total_tokens": 18244,
            "latency": 6.775216,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement various types of Recurrent Neural Network (RNN) cells and layers, including RNN, LSTM, and GRU, as part of the PyTorch library. These classes provide the core functionality for building and training RNN models, including forward passes, weight initialization, and input validation. The file also includes documentation and example usage for each type of RNN cell and layer.",
            "total_tokens": 18221,
            "latency": 4.602141,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "This file defines the core PyTorch neural network modules for recurrent neural networks, including multi-layer and single-layer (cell) implementations of vanilla RNNs, LSTMs, and GRUs. It provides classes (RNN, LSTM, GRU, RNNCell, LSTMCell, GRUCell) that encapsulate all parameters, initialization, and the forward computation needed for these recurrent units, supporting options for bias, batch layout, dropout, bidirectionality, and projections (for LSTM). The file handles both batched and unbatched inputs, allows packed sequences for variable-length input, and manages parameter flattening for backend optimization.",
            "total_tokens": 18488,
            "latency": 3.028873,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/modules/rnn.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides implementations of standard Recurrent Neural Network (RNN) modules for the PyTorch library. It defines classes for multi-layer RNN, LSTM, and GRU networks, including support for features like bidirectional processing, dropout, and batch handling. Additionally, it includes corresponding single-step cell implementations for RNN, LSTM, and GRU.",
            "total_tokens": 21449,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.311906
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/modules/rnn.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines core PyTorch modules for Recurrent Neural Networks (RNNs), including base classes (`RNNBase`, `RNNCellBase`) and specific implementations like `RNN`, `LSTM`, and `GRU`. It also provides the corresponding single-step cell versions (`RNNCell`, `LSTMCell`, `GRUCell`) for more granular control. These modules handle sequence processing, hidden/cell state management, parameter initialization, and support features like multi-layer stacking, bidirectionality, dropout, and batching options.",
            "total_tokens": 21543,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 5.83553
        }
    },
    "/torch/nn/utils/parametrizations.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file defines three functions: `orthogonal`, `weight_norm`, and `spectral_norm`. These functions are used to apply orthogonal, weight, and spectral normalization, respectively, to the weights of a given PyTorch module. The file provides a way to constrain the weights of neural network layers to specific manifolds, which can help stabilize training and improve the performance of models, especially in generative adversarial networks (GANs).",
            "total_tokens": 7256,
            "latency": 1.968425,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file `parametrizations.py` contains three main functionalities:\n\n1. `orthogonal`: This function applies an orthogonal or unitary parametrization to a matrix or a batch of matrices. It uses three different orthogonal maps: `matrix_exp`, `cayley`, and `householder`. The `matrix_exp` and `cayley` maps are applied to a skew-symmetric matrix to give an orthogonal matrix, while the `householder` map computes a product of Householder reflectors. The function also supports the dynamic trivialization framework, which helps the convergence of the parametrized layer at the expense of some extra memory use.\n\n2. `weight_norm`: This function applies weight normalization to a parameter in the given module. Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. It replaces the parameter specified by the `name` argument with two parameters: one specifying the magnitude and one specifying the direction.\n\n3. `spectral_norm`: This function applies spectral normalization to a parameter in the given module. Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by reducing the Lipschitz constant of the model. It approximates the spectral norm of the weight tensor by performing one iteration of the power method every time the weight is accessed.\n\nOverall, the file provides various parametrization techniques for matrices and weight tensors, which can be useful for improving the performance and stability of deep learning models.",
            "total_tokens": 329,
            "latency": 9.431468,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python file implements three tensor parametrizations: orthogonal (or unitary) parametrization via matrix exponential, Cayley map, or Householder reflectors; weight normalization, which reparameterizes weights to separate magnitude and direction; and spectral normalization, which constrains weights by their spectral norm to stabilize GAN training. These parametrizations leverage PyTorch's parametrization framework, enabling modules to have mathematical constraints automatically enforced during training.",
            "total_tokens": 6509,
            "latency": 7.758492,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the given Python file is to provide parametrizations for orthogonal/unitary matrices, weight normalization, and spectral normalization in PyTorch. It includes functions such as ```orthogonal```, ```weight_norm```, and ```spectral_norm``` that allow users to apply these parametrizations to specific modules and parameters within a neural network, enabling better training stability and convergence through constraint enforcement and reparameterization. The file contains implementations of the parametrization logic, including the use of different techniques such as the matrix exponential, Cayley transform, Householder reflections, and power iterations for spectral normalization, to ensure the desired properties are maintained during optimization.",
            "total_tokens": 6548,
            "latency": 8.982642,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"parametrizations.py\" file is to implement various parametrization techniques for neural network layers, specifically focusing on orthogonal, weight normalization, and spectral normalization. It provides functions to apply these parametrizations to module weights, allowing for more stable and controlled training of neural networks, particularly in scenarios like Generative Adversarial Networks (GANs).",
            "total_tokens": 7821,
            "latency": 6.322465,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `parametrizations.py` file provides functions to apply various normalizations to parameters within PyTorch modules. Specifically, it includes implementations for orthogonal parametrization, weight normalization, and spectral normalization, each designed to impose constraints on the parameters to improve training stability and performance.",
            "total_tokens": 7225,
            "latency": 1.51616,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `parametrizations.py` file provides implementations of advanced parameterization techniques for PyTorch models, including orthogonal transformations, weight normalization, and spectral normalization. It defines custom modules and utilities to parametrize or modify weight tensors in terms of orthogonality, norm decomposition, or spectral constraints, enabling easier optimization and improving the stability of network training. These methods are especially useful for tasks like regularization, improving convergence, or stabilizing adversarial training setups like GANs.",
            "total_tokens": 6519,
            "latency": 2.054276,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide three functions: `orthogonal`, `weight_norm`, and `spectral_norm`, which are used to apply different types of parametrizations to the weights of a PyTorch module. These parametrizations are used to constrain the weights of the module to have certain properties, such as being orthogonal or having a specific norm. The functions take in a PyTorch module, the name of the weight to be parametrized, and other optional arguments, and return the modified module with the parametrization applied.",
            "total_tokens": 6534,
            "latency": 6.401872,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The provided Python file, named `parametrizations.py`, defines three main parametrization techniques for neural network weights in PyTorch: orthogonal, weight norm, and spectral norm. These techniques are implemented as classes (`_Orthogonal`, `_WeightNorm`, `_SpectralNorm`) and corresponding functions (`orthogonal`, `weight_norm`, `spectral_norm`) that can be applied to `torch.nn.Module` parameters. These parametrizations enable training stability and better convergence by enforcing specific constraints on the weights\u2014orthogonality, weight normalization, and spectral normalization, respectively. The methods provided also handle complex tensors and multi-dimensional weight matrices, ensuring the appropriate constraints and transformations are applied to maintain the desired parametrizations during training.",
            "total_tokens": 6563,
            "latency": 4.008268,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement various parametrization techniques for PyTorch modules, specifically orthogonal, weight norm, and spectral norm parametrizations. These techniques are used to constrain or normalize the weights of neural network layers, which can help improve the stability and performance of the network. The file defines several classes and functions that implement these parametrizations, including `_Orthogonal`, `_WeightNorm`, and `_SpectralNorm`, as well as the `orthogonal`, `weight_norm`, and `spectral_norm` functions that apply these parametrizations to PyTorch modules.",
            "total_tokens": 6544,
            "latency": 3.297651,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file provides utilities to apply parameterizations to PyTorch neural network layers, specifically implementing orthogonal (or unitary), spectral norm, and weight norm constraints on module weights via parametrization hooks. It defines custom parametrization classes for each method, enabling their registration to arbitrary layer parameters, ensuring the weight matrices maintain the desired mathematical properties (e.g., orthogonality, unit spectral norm, or normalized magnitude/direction) during training. These parameterizations help in stabilizing and regularizing deep learning models, supporting advanced optimization and convergence behavior by leveraging PyTorch's parametrize API.",
            "total_tokens": 6540,
            "latency": 2.072698,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/utils/parametrizations.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines functions (`orthogonal`, `weight_norm`, `spectral_norm`) that apply specific mathematical parametrizations to tensors within PyTorch modules, typically network weights. These parametrizations enforce constraints like orthogonality, decouple weight magnitude and direction, or limit the spectral norm using PyTorch's `parametrize` utility. The overall goal is often to improve training stability or enforce desired properties on the learned parameters.",
            "total_tokens": 7720,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 14.851756
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/utils/parametrizations.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides implementations for common weight parametrizations in PyTorch modules, leveraging the `torch.nn.utils.parametrize` utility. It defines functions to apply orthogonal, weight normalization, and spectral normalization constraints to module weights. These parametrizations modify how the weight tensor is computed from underlying parameters to enforce desired mathematical properties like orthogonality, unit norm, or a spectral norm of one.",
            "total_tokens": 7563,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.764533
        }
    },
    "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file is a part of a larger framework for optimizing matrix multiplication operations in PyTorch. It defines functions to decompose certain matrix multiplication operations (like `bmm` and `mm`) into more manageable parts based on specific conditions, such as tensor dimensions and device type. The decomposition is intended to improve performance by breaking down large matrix multiplications into smaller, more efficient operations.",
            "total_tokens": 1936,
            "latency": 0.951171,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"decompose_mem_bound_mm.py\" is to decompose matrix multiplication operations (mm, bmm, addmm) into smaller sub-operations, based on certain conditions related to the dimensions of the input matrices and the device they are on. The file uses the torch.fx library to match and replace the matrix multiplication operations with the decomposed sub-operations. The file also includes logging statements to track the number of decompositions performed.",
            "total_tokens": 99,
            "latency": 2.480415,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines transformations for decomposing certain large or specifically shaped matrix multiplication operations (including matrix-matrix multiplication like `mm`, batched matrix multiplication `bmm`, and addition-matrix multiplication `addmm`) in PyTorch models into more memory-efficient, element-wise operations followed by reductions. These decompositions are conditionally applied based on the input tensor dimensions, devices (GPU or CPU), and configurable thresholds. When certain conditions are met, matrix operations are replaced by equivalent computations that leverage element-wise multiplication and summation, potentially improving execution efficiency on memory-bound operations.",
            "total_tokens": 1649,
            "latency": 6.214685,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file \"decompose_mem_bound_mm.py\" defines functions to decompose matrix multiplication operations based on certain conditions like matrix shape and device type. It also includes pattern matcher passes for decomposing specific torch operations such as 'bmm', 'addmm', and 'mm'. The decomposed operations aim to optimize memory usage during matrix multiplications by breaking down the computation into smaller pieces.",
            "total_tokens": 1591,
            "latency": 1.093899,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"decompose_mem_bound_mm.py\" file is to decompose memory-bound matrix multiplication operations in PyTorch. It defines patterns and conditions for decomposing various matrix multiplication operations (bmm, addmm, mm) into more memory-efficient computations, particularly for large matrices on CUDA devices or small matrices on CPU. The file also includes utility functions for checking device compatibility and validating node metadata, as well as registering graph patterns for the decomposition process.",
            "total_tokens": 2084,
            "latency": 3.434122,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The file `decompose_mem_bound_mm.py` defines functions to decompose specific matrix multiplication operations (`bmm`, `addmm`, and `mm`) in PyTorch graphs under certain conditions related to tensor dimensions and device type, aiming to optimize memory usage. It utilizes pattern matching to identify and replace these operations within the graph. The decomposition strategy and conditions are configurable via the `config.post_grad_fusion_options`.",
            "total_tokens": 1950,
            "latency": 1.386178,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The script `decompose_mem_bound_mm.py` defines strategies and patterns to decompose certain matrix operations (`mm`, `bmm`, and `addmm`) in PyTorch for optimization purposes. It implements checks based on tensor shapes, devices (CPU or CUDA), and configurable thresholds to determine whether decomposition should be applied. This decomposition is done using pattern matching to replace operations with alternative implementations for better memory or computational efficiency.",
            "total_tokens": 1618,
            "latency": 1.77769,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to decompose matrix multiplication (mm) and batched matrix multiplication (bmm) operations in PyTorch graphs, specifically for CUDA and CPU devices, based on certain conditions such as matrix dimensions and device type. The file defines several functions to check if decomposition is necessary and to perform the decomposition, and registers these functions as graph patterns to be applied during the graph transformation process.",
            "total_tokens": 1602,
            "latency": 3.58213,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `decompose_mem_bound_mm.py` is primarily focused on implementing pattern matching and decomposition strategies for matrix multiplication operations (e.g., `mm`, `bmm`, and `addmm`) in PyTorch, specifically tailored for optimization on different hardware devices like CPU and CUDA. It uses configuration settings to define decomposition strategies based on dimensions, and integrates these decompositions into a pattern matching registration system using decorators, which allows for transforming specific computational graph patterns according to the defined decomposition functions. The file also includes logging and debugging utilities to assist with the monitoring of the transformations applied during the decomposition process.",
            "total_tokens": 1634,
            "latency": 3.145152,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to decompose matrix multiplication operations (such as `bmm`, `addmm`, and `mm`) in PyTorch into more efficient forms, specifically for large matrices on CUDA devices. The decomposition is done based on certain conditions, including the size of the matrices and the device they are on, with the goal of improving performance. The file uses PyTorch's `torch.fx` module to analyze and modify the computation graph, and it registers pattern matching passes to apply the decomposition transformations.",
            "total_tokens": 1627,
            "latency": 1.797343,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file decompose_mem_bound_mm.py defines pattern-matching and decomposition logic for certain memory-bound matrix multiplication operations (specifically mm, bmm, and addmm) in PyTorch computation graphs. It provides heuristics to detect when these operations are likely memory-bound based on tensor shapes and device, and replaces them with manually decomposed operations optimized for specific scenarios. This functionality is integrated into PyTorch's FX graph rewriting infrastructure to enable automatic transformation and optimization during model graph compilation.",
            "total_tokens": 1630,
            "latency": 1.507814,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file implements graph pattern matchers within the PyTorch FX framework to identify specific matrix multiplication operations (`aten.mm`, `aten.bmm`, `aten.addmm`). It checks if the input tensors meet certain shape and device conditions, particularly for memory-bound cases on CUDA or specific CPU patterns. If the conditions are met, the matrix multiplication is decomposed into an equivalent expression using element-wise multiplication and summation.",
            "total_tokens": 2320,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.006164
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines `torch.fx` graph patterns to identify specific memory-bound matrix multiplication operations (`aten.bmm`, `aten.addmm`, `aten.mm`). Based on configurable tensor shape and device criteria (primarily large first dimension and small other dimensions on CUDA, or specific small shapes on CPU), it decomposes these operations into element-wise multiplications followed by sums. This decomposition aims to potentially improve performance in specific scenarios by replacing the original matrix multiplication call within the graph.",
            "total_tokens": 2525,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.668705
        }
    },
    "/torch/autograd/profiler.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"profiler.py\" file is to provide a profiling tool for PyTorch autograd operations. It allows developers to measure the performance of their PyTorch code by tracking the execution time of various operations, memory usage, and other metrics. The profiler includes a context manager (`profile`) to wrap code sections for profiling, a decorator (`record_function`) to label specific code blocks, and support for emitting annotations for external profilers like Intel VTune and NVTX. The profiler can also accumulate events across multiple profiling cycles and export profiling results in various formats.",
            "total_tokens": 12551,
            "latency": 2.015811,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file `profiler.py` is a part of the PyTorch library and contains various functionalities related to profiling. It includes classes and functions that allow users to profile their PyTorch code, analyze the performance, and identify bottlenecks.\n\nThe main functionalities of the file are:\n\n1. `profile`: A context manager that manages the autograd profiler state and holds a summary of results. It allows users to enable and disable profiling, specify profiling options, and access profiling results.\n\n2. `record_function`: A context manager and function decorator that adds a label to a code block or function when running the autograd profiler. It is useful for annotating code blocks or functions for better understanding of the profiling results.\n\n3. `emit_itt`: A context manager that makes every autograd operation emit an ITT range. It is useful for profiling with Intel(R) VTune Profiler.\n\n4. `emit_nvtx`: A context manager that makes every autograd operation emit an NVTX range. It is useful for profiling with nvprof.\n\n5. `load_nvprof`: A function that loads an nvprof trace file and parses autograd annotations. It allows users to analyze the profiling results from an nvprof trace file.\n\n6. `EnforceUnique`: A class that raises an error if a key is seen more than once. It is used to ensure uniqueness of keys in certain contexts.\n\n7. `parse_nvprof_trace`: A function that parses an nvprof trace file and returns a list of FunctionEvents. It allows users to extract and analyze the profiling results from an nvprof trace file.\n\n8. `KinetoStepTracker`: A class that provides an abstraction for incrementing the step count globally. It is used to maintain the step count for different requesters and ensure correct increments of the step count.\n\nOverall, the `profiler.py` file provides a comprehensive set of tools for profiling PyTorch code and analyzing the performance. It allows users to enable and disable profiling, specify profiling options, annotate code blocks or functions, and analyze profiling results from different profiling tools.",
            "total_tokens": 486,
            "latency": 12.220889,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements a comprehensive performance profiling module using PyTorch's autograd system, specifically designed to profile CPU and hardware accelerator activities (e.g., CUDA, XPU, MTIA, HPU). It allows users to measure runtime, memory utilization, floating-point operations (FLOPs), and stack information of PyTorch operations within a context manager or decorators like `profile`, `record_function`, `emit_nvtx`, and `emit_itt`. Additionally, the module includes utilities to parse, summarize, and export profiling results for detailed performance analysis and debugging.",
            "total_tokens": 10597,
            "latency": 8.680461,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given `profiler.py` file contains classes and functions related to the autograd profiler in PyTorch. It includes functionality for profiling code blocks, emitting ITT and NVTX ranges, as well as loading and parsing nvprof traces. Additionally, it provides a `KinetoStepTracker` class for tracking step counts globally. The main functionality revolves around context managers for profiling, managing step counts, and parsing trace files for analysis and visualization.",
            "total_tokens": 10539,
            "latency": 2.016911,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"profiler.py\" file is to provide a profiling system for PyTorch, allowing users to measure and analyze the performance of their code. Here's a summary in three sentences:\n\n1. The file implements a profiling context manager and various utility functions for collecting, processing, and visualizing performance data in PyTorch applications.\n\n2. It supports different profiling modes, including CPU profiling, CUDA profiling, and integration with external profiling tools like Intel VTune and NVIDIA's nvprof.\n\n3. The profiler can record function events, memory usage, and device-specific metrics, and provides methods for generating performance reports and exporting data in various formats for further analysis.",
            "total_tokens": 13421,
            "latency": 6.518788,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `profiler.py` file provides various context managers and utilities for profiling PyTorch operations, including detailed timing, memory usage, and event tracing. It supports both CPU and various device-specific profiling, with options for recording shapes, flops, and stack traces. Additionally, it includes mechanisms for annotating code blocks and integrating with external profiling tools like Intel VTune and NVIDIA nvprof.",
            "total_tokens": 12513,
            "latency": 1.915225,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `profiler.py` file provides tools and utilities for profiling PyTorch operations, enabling developers to measure and analyze the performance of their code. It includes context managers (`profile`, `record_function`, `emit_itt`, `emit_nvtx`) that allow users to collect runtime and memory usage data during execution, optionally including details like input shapes, module hierarchy, and floating-point operations. Additionally, the file provides mechanisms for tracking optimization steps (`KinetoStepTracker`), exporting profiling results in various formats, and interacting with external profiling tools like Intel VTune and NVIDIA nvprof.",
            "total_tokens": 10600,
            "latency": 2.837901,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide a set of tools for profiling and tracing the execution of PyTorch code, including the ability to record function calls, emit ITT (Intel Trace Tool) and NVTX (NVIDIA Tools Extension) ranges, and load nvprof traces. The file also includes a context manager for managing autograd profiler state and holding a summary of results.",
            "total_tokens": 10533,
            "latency": 48.310432,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file defines a profiling utility leveraging PyTorch's autograd profiler capabilities. It provides a context manager `profile` to gather performance data about PyTorch operations, measuring execution times and memory usage on CPU and GPU devices. The profiler supports various configurations, such as including device-specific events or accumulating events across multiple runs. Additional utilities include functions to emit profiling markers for ITT and NVTX tools, and handle related profiling functionalities for better traceability and debugging of PyTorch applications.",
            "total_tokens": 10548,
            "latency": 4.558577,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement a profiling tool for PyTorch, allowing users to measure the execution time and memory usage of their models and identify performance bottlenecks. The profiler provides various features, including CPU and CUDA profiling, memory profiling, and support for Intel VTune Amplifier and NVIDIA Nsight Systems. The profiler can be used as a context manager or as a decorator to profile specific sections of code.",
            "total_tokens": 10544,
            "latency": 9.17942,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The provided \"profiler.py\" file implements a robust profiling toolset for PyTorch, allowing developers to record and analyze the performance of PyTorch operations, including profiling CPU and device activities, tracking memory usage, and annotating code ranges for visualization in external tools like Intel VTune and NVIDIA's nvprof. It provides high-level context managers such as profile, record_function, emit_nvtx, and emit_itt to instrument and collect detailed execution traces, and utility functions to parse and process profiling results from both internal (Kineto) and nvprof traces. Additional classes and utilities within the module help enforce event uniqueness, manage global step tracking across requesters, and present comprehensive summaries and exports of collected profiling data.",
            "total_tokens": 10626,
            "latency": 2.721833,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/autograd/profiler.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides tools for profiling PyTorch code execution, primarily through the `profile` context manager which records performance metrics like execution time, memory usage, and operator details for CPU and various device types (CUDA, XPU, etc.). It allows users to analyze performance bottlenecks by collecting data on function calls, input shapes, FLOPs, and call stacks. Additionally, the file includes utilities like `record_function` for adding custom labels to code blocks and context managers (`emit_itt`, `emit_nvtx`) for exporting traces compatible with external profiling tools like Intel VTune and NVIDIA Nsight.",
            "total_tokens": 13092,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.574149
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/autograd/profiler.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides tools for profiling PyTorch code execution, including CPU and device activities. It defines context managers like `profile`, `record_function`, `emit_itt`, and `emit_nvtx` to enable and control profiling sessions and add custom annotations. The module also includes utilities for processing and exporting profiling results, such as generating tables, Chrome traces, and parsing nvprof traces.",
            "total_tokens": 13099,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.413076
        }
    },
    "/torch/distributed/tensor/parallel/fsdp.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"fsdp.py\" file is to provide extensions and utilities for handling distributed tensors in a 2D Fully Sharded Data Parallel (FSDP) setup, specifically focusing on tensor flattening, chunking, and redistribution across devices. It includes methods for transforming tensors to and from sharded and distributed formats, managing tensor metadata, and facilitating communication between distributed processes. This file is designed to work with PyTorch's distributed package, particularly for optimizing model training across multiple devices or nodes by efficiently managing data distribution and synchronization.",
            "total_tokens": 4033,
            "latency": 1.402418,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to provide a class DTensorExtensions which is an extension for FSDP (Fully Sharded Data Parallel) and TP (Tensor Parallelism). The class has methods to flatten and unflatten tensors, chunk tensors and DTensors, preprocess state dictionary, and all gather DTensors. The file also has utility functions to get boxes for tensors, create ShardedTensorMetadata from DTensor, get process group from DTensor, rewrite ShardingSpec if needed, and create chunked ShardedTensor or DTensor. The file is a part of PyTorch's distributed package and is used for distributed training of models.",
            "total_tokens": 141,
            "latency": 3.931634,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements functionality related to PyTorch's Fully Sharded Data Parallel (FSDP) technique, particularly focusing on tensor partitioning, sharding logic, and state dict handling using DTensor and ShardedTensor types. It defines helper functions and a main class \"DTensorExtensions\" derived from \"FSDPExtensions,\" which provides operations to manage data parallelism and tensor parallelism in distributed training scenarios. Key operations include flattening/unflattening tensors, chunking tensors across device meshes, and redistributing DTensors to facilitate efficient distributed training.",
            "total_tokens": 3407,
            "latency": 8.990164,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided Python file `fsdp.py` is a module that contains functions and classes related to handling sharded tensors in distributed deep learning using the FullyShardedDataParallel (FSDP) technique. The file includes functions for creating sharded tensors, chunking tensors for distribution across multiple devices, all-gathering sharded tensors in the FSDP dimension, and transforming tensors for state dictionary loading. Additionally, the file defines a `DTensorExtensions` class that extends FSDPExtensions and provides methods for transforming and processing tensors in the distributed setting.",
            "total_tokens": 3362,
            "latency": 1.502411,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"fsdp.py\" file is to implement the DTensorExtensions class, which extends the FSDPExtensions for handling Distributed Tensors (DTensors) in the context of Fully Sharded Data Parallel (FSDP) training. This file provides utility functions for tensor manipulation, including sharding, chunking, and gathering operations, specifically tailored for distributed training scenarios involving DTensors and FSDP. The code also includes helper functions for creating and managing sharded tensors and their metadata in a distributed environment.",
            "total_tokens": 4441,
            "latency": 4.237489,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `fsdp.py` file provides extensions and utilities for handling Distributed Tensors (`DTensor`) and Sharded Tensors (`ShardedTensor`) within the Fully Sharded Data Parallelism (FSDP) framework in PyTorch. It includes functions for flattening, unflattening, chunking tensors, and managing state dictionaries, aimed at optimizing distributed training across multiple devices.",
            "total_tokens": 3998,
            "latency": 1.294451,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The \"fsdp.py\" file provides utilities for managing Fully Sharded Data Parallel (FSDP) and Tensor Parallel (TP) operations in PyTorch, focusing on distributing and managing tensors across devices efficiently. It includes methods for chunking, sharding, flattening, and all-gather operations on tensors (both standard and distributed tensors) using device meshes and distributed process groups. Additionally, the file defines the `DTensorExtensions` class for customizing tensor handling, specifically tailored for 2D FSDP + TP scenarios.",
            "total_tokens": 3400,
            "latency": 2.429268,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide extensions for PyTorch's Fully Sharded Data Parallel (FSDP) module to work with Distributed Tensors (DTensors). The extensions include methods for chunking tensors, loading state dictionaries, and all-gathering DTensors. The file defines a class `DTensorExtensions` that inherits from `FSDPExtensions` and provides the necessary functionality for FSDP to work with DTensors.",
            "total_tokens": 3346,
            "latency": 4.732086,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `fsdp.py` primarily deals with distributed training using sharded tensors in PyTorch. It provides functions and classes to manage data distribution and computation across different devices and processes, specifically focusing on sharded versions of tensors and distributed tensors (DTensors). The file includes utility functions for creating, managing sharding metadata, reshaping tensors according to device meshes, and handling state dictionaries in fully sharded data-parallel (FSDP) setups. Additionally, it incorporates extensions for PyTorch's FSDP, enabling optimized tensor operations in distributed environments.",
            "total_tokens": 3365,
            "latency": 3.327466,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement extensions for Fully Sharded Data Parallelism (FSDP) with Distributed Tensors (DTensors) in PyTorch, enabling the integration of FSDP with Tensor Parallelism (TP) for 2D parallelism. The file defines a class `DTensorExtensions` that inherits from `FSDPExtensions` and provides methods for tensor flattening, unflattening, chunking, and all-gathering, specifically designed to work with DTensors. These extensions aim to facilitate the use of FSDP with TP, allowing for more efficient and flexible parallelization of deep learning models.",
            "total_tokens": 3390,
            "latency": 2.81118,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file fsdp.py defines utilities and extension mechanisms for integrating FSDP (Fully Sharded Data Parallel) with DTensor, ShardedTensor, and device mesh approaches in PyTorch distributed training. It provides functions for chunking, sharding, gathering, and transforming tensors (both standard and DTensor) appropriately across distributed devices, enabling efficient tensor handling and parallelization strategies (such as 2D FSDP + TP). The DTensorExtensions class implements the required FSDPExtensions interface to manage tensor flattening/unflattening, sharding, and device-aware tensor allocation, facilitating seamless distributed model state management.",
            "total_tokens": 3419,
            "latency": 2.110755,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/tensor/parallel/fsdp.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines `DTensorExtensions`, a class implementing `FSDPExtensions` to integrate PyTorch's DTensor and ShardedTensor with the FSDP framework. It provides methods for sharding, flattening, unflattening, and state management of tensors, particularly supporting configurations with 2D device meshes like combined FSDP and Tensor Parallelism. The code includes helper functions to manage tensor metadata and sharding logic for these distributed tensor types within the FSDP context.",
            "total_tokens": 4993,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 5.476781
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/tensor/parallel/fsdp.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This file defines extensions to PyTorch's Fully Sharded Data Parallel (FSDP) framework to enable compatibility with `DTensor`s, which are used for tensor parallelism across a `DeviceMesh`. It provides helper functions and the `DTensorExtensions` class to manage the specific sharding, flattening/unflattening, and state dictionary handling required when using `DTensor`s within an FSDP context. This integration allows combining FSDP data parallelism with tensor parallelism for training large models.",
            "total_tokens": 4690,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.143324
        }
    },
    "/tools/stats/upload_utilization_stats/upload_utilization_stats.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `upload_utilization_stats.py` script is to download utilization logs from an S3 bucket, convert these logs into a database-friendly format, and then upload the processed data back to S3. The script includes functionalities to parse command-line arguments, handle JSON data, and perform data transformations using pandas. It also supports dry-run and debug modes for testing and troubleshooting.",
            "total_tokens": 4082,
            "latency": 1.817755,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"upload_utilization_stats.py\" is to convert raw log data into a database model and upload it to an S3 bucket. The script fetches raw log data from an S3 bucket, converts it into a log model, and then converts it into a database model. It also generates test segments from utilization records and uploads the converted data to an S3 bucket. The script can be run with command line arguments to specify the workflow run ID, workflow run attempt, workflow name, job ID, job name, and repository. The script also supports dry-run and debug modes.",
            "total_tokens": 127,
            "latency": 5.832449,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The Python script fetches CI job utilization logs from AWS S3, processes and converts them into structured data models (metadata and time series data), and identifies continuous segments of executed Python commands. It ultimately uploads the processed utilization stats back into S3 in a structured format for storage and analysis. The script supports running in debug and dry-run modes for safer testing and verification.",
            "total_tokens": 3372,
            "latency": 5.057978,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file \"upload_utilization_stats.py\" handles the conversion and upload of utilization data to an S3 bucket. It includes functionalities such as generating test segments from utilization records, converting log models to database models, handling raw log data retrieval from S3, and uploading the converted data to the S3 bucket. The script also supports command line arguments for workflow information and has classes and methods for data conversion and uploading.",
            "total_tokens": 3361,
            "latency": 1.585529,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"upload_utilization_stats.py\" file is to process and upload utilization data from GitHub workflow runs to Amazon S3. It fetches raw log data from S3, converts it to a log model, then to a database model, and finally uploads the processed data back to S3. The script includes functionality for generating test segments, converting utilization records, and handling metadata and time series data for CI/CD workflow utilization statistics.",
            "total_tokens": 4548,
            "latency": 5.454349,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `upload_utilization_stats.py` script is designed to process utilization data logs from a specified GitHub workflow, convert them into a database-friendly format, and upload them to an S3 bucket. It includes functionalities for downloading artifacts, segmenting utilization records, converting log models to database models, and handling dry-run and debug modes.",
            "total_tokens": 4069,
            "latency": 1.473245,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The file `upload_utilization_stats.py` provides functionality to process and manage utilization statistics for specific workflows, primarily by downloading raw utilization logs from S3, converting them into structured database models, and subsequently uploading the processed data back to S3. It includes classes to generate segments of utilization data based on activity, transform raw logs into database-friendly schemas, and handle the complete data flow, including validation and debugging options. This script is designed to be run as a command-line tool, with various arguments for workflow and job identifiers, and supports both production and dry-run modes. ",
            "total_tokens": 3413,
            "latency": 2.733887,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to upload utilization data to Amazon S3. It fetches raw log data from S3, converts it to a log model, then converts it to a database model, and finally uploads it to S3. The file also includes classes for generating segments from utilization records, converting utilization log models to database models, and uploading utilization data to S3.",
            "total_tokens": 3360,
            "latency": 3.824086,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python script \"upload_utilization_stats.py\" is designed to handle and process utilization data, specifically for command line Python usage in workflows, and upload this data to an AWS S3 bucket. It includes functionality to download raw utilization logs, parse these logs into data models, generate segmentation for continuous command usage, and convert these into a database model format for storage. The script supports arguments for workflow identification, debug options, and a dry-run mode to simulate uploads without actual data transfer. The script uses Pandas for data manipulation and handles JSON data for easy transfer and storage.",
            "total_tokens": 3391,
            "latency": 3.141176,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `upload_utilization_stats.py`, is to collect utilization data from GitHub workflow logs, convert it into a database model, and upload it to Amazon S3. The script fetches raw log data from S3, processes it into a log model, and then converts it into a database model before uploading it to S3 for storage and potential analysis.",
            "total_tokens": 3362,
            "latency": 2.118446,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The main functionality of upload_utilization_stats.py is to extract and process raw utilization log artifacts from S3 associated with CI workflows, convert them into structured database models including time series and metadata (such as resource usage and detected command segments), and then upload these processed statistics back to S3 for further analysis or storage. It achieves this through a command-line interface that accepts workflow-related identifiers, handles log parsing, data validation, and various error scenarios, and supports both dry-run and debug modes. The script also provides mechanisms for segmenting usage data (e.g., identifying Python command segments) to enrich the utilization statistics.",
            "total_tokens": 3421,
            "latency": 1.895835,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/stats/upload_utilization_stats/upload_utilization_stats.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script downloads raw utilization logs from S3 artifacts associated with a specific CI workflow run and job. It processes these logs, identifies continuous execution segments (specifically for Python commands), and converts the data into structured database models for metadata and time series. Finally, the script uploads these processed utilization statistics to a dedicated S3 bucket for storage and analysis.",
            "total_tokens": 4977,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.412693
        },
        "gemini-2.5": {
            "file_name": "/tools/stats/upload_utilization_stats/upload_utilization_stats.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This script downloads CI job usage logs from S3 artifacts, specifically looking for a `usage_log.txt` file. It then parses these logs, identifies continuous segments of Python command execution, and converts the raw log data and segments into structured metadata and time-series database models. Finally, the script uploads these processed utilization statistics back to a designated S3 bucket, organized by workflow and job identifiers.",
            "total_tokens": 4767,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.440765
        }
    },
    "/torch/_inductor/cpp_builder.py": {
        "nova-lite-v1": {
            "file_summary": "The \"cpp_builder.py\" file is a Python script designed to facilitate the compilation of C++ code into shared libraries or executables, supporting multiple operating systems including Windows, Linux, and MacOS. It provides a flexible and configurable way to specify build options such as compiler selection, include directories, and various flags for optimization and compatibility. The script includes functionalities for handling different compilers, managing build environments, and generating build commands that can be executed directly or saved into CMake files for integration with CMake-based build systems. It is particularly tailored for building components related to PyTorch, with support for specific PyTorch-related build options and configurations.",
            "total_tokens": 18299,
            "latency": 2.895052,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The `cpp_builder.py` file is a Python script that provides functionality for building C++ code. It includes classes for managing build options and a `CppBuilder` class for building C++ code.\n\nThe `BuildOptionsBase` class is a base class for storing C++ build options. It includes attributes for the compiler, definitions, include directories, C++ flags, linker flags, library directories, and libraries. The class also includes methods for finalizing the options and saving the options to a JSON file.\n\nThe `CppOptions` class is a subclass of `BuildOptionsBase` that includes additional attributes for the AOT mode, use of relative paths, and compile-only mode. The class also includes methods for getting the compiler, definitions, include directories, C++ flags, linker flags, library directories, and libraries.\n\nThe `CppTorchOptions` class is a subclass of `CppOptions` that includes additional attributes for the vector instruction set architecture (ISA), inclusion of PyTorch, AOT mode, compile-only mode, use of relative paths, use of mmap weights, and shared mode. The class also includes methods for getting the definitions, include directories, C++ flags, linker flags, library directories, libraries, and passthrough arguments.\n\nThe `CppTorchDeviceOptions` class is a subclass of `CppTorchOptions` that includes additional attributes for the device type (CUDA or XPU), AOT mode, compile-only mode, and use of relative paths. The class also includes methods for getting the definitions, include directories, C++ flags, linker flags, library directories, libraries, and passthrough arguments.\n\nThe `CppBuilder` class is a class for building C++ code. It includes attributes for the name, sources, build options, output directory, and target file. The class also includes methods for getting the command line, target file path, building the code, and saving the build command to a CMake file.\n\nThe `CppBuilder` class includes a `build` method that builds the C++ code using the command line. The method creates a temporary directory for the build and runs the command line in the temporary directory. If the build is successful, the temporary directory is deleted.\n\nThe `CppBuilder` class includes a `save_compile_cmd_to_cmake",
            "total_tokens": 500,
            "latency": 15.544391,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements a comprehensive JIT compilation utility named 'CppBuilder', which is designed to handle cross-platform (Windows, Linux, macOS) compilation of C++ source code for PyTorch-related projects. It features OS-specific compiler flags, supports detection and configuration of different compilers (gcc, clang, MSVC, Intel), manages paths and libraries for various hardware backends (CUDA, ROCm, XPU), and includes special considerations such as OpenMP loading and Python Torch embedding. Overall, it streamlines building C++ extensions or components compatible with PyTorch and Python runtime environments across different systems.",
            "total_tokens": 14948,
            "latency": 15.226072,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file is a script designed to build a C++ library, supporting both Windows and Linux OS. It includes functionality to find the compiler, set up compilation and linking options, handle Windows-specific temp directories, and prepare the build commands. The script organizes and executes the build process based on the specified build options, including defining macros, setting compile flags, including directories, linking flags, and libraries. It also provides the functionality to save the build commands to a CMake configuration file.",
            "total_tokens": 14701,
            "latency": 2.453614,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"cpp_builder.py\" file is to provide a flexible and cross-platform C++ build system for PyTorch's inductor compiler. It includes classes and functions for:\n\n1. Configuring and managing C++ build options for different platforms (Windows, Linux, macOS) and compilers (GCC, Clang, MSVC).\n\n2. Handling various build scenarios, including AOT (Ahead-of-Time) compilation, OpenMP support, and device-specific options (CUDA, ROCm, XPU).\n\n3. Generating and executing compilation commands for C++ source files, including preprocessing, compiling, and linking stages.",
            "total_tokens": 19380,
            "latency": 8.418087,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `cpp_builder.py` file is to provide a comprehensive C++ builder for compiling C++ code, supporting both Windows and Linux operating systems. It includes functionalities for setting up build options, handling different compilers, managing build processes, and generating CMake files for further build automation.",
            "total_tokens": 18229,
            "latency": 4.041063,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The file `cpp_builder.py` serves as a utility for building compiled C++ libraries for PyTorch, supporting configurations compatible for both Windows and Linux operating systems. It includes logic for detecting and configuring various C++ compilers, setting build options, handling dependencies like OpenMP and Python, and managing multiple compilation modes such as precompiled headers, AOT, and device-specific options (e.g., CUDA or XPU). It also supports generating CMake files and remote builds for FBCode environments.",
            "total_tokens": 14924,
            "latency": 2.25934,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the file is to provide a C++ builder class, `CppBuilder`, which can be used to compile C++ code on various platforms, including Windows, Linux, and MacOS. The class takes in source code files, build options, and an output directory, and generates a command line to compile the code. It also provides methods to save the compile command to a CMake file and to build the code.",
            "total_tokens": 14694,
            "latency": 50.258223,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file, named `cpp_builder.py`, is a detailed and complex builder module primarily designed to dynamically compile and link C++ code across different operating systems, specifically targeting the construction of Python extensions or other C++ libraries. It heavily utilizes the Torch libraries and C++ features, supporting varying compilation flags, linker configurations, and definitions based on the OS and user requirements. Additionally, the script handles platform-specific challenges such as handling dynamic libraries, precompiling headers, conditions for different compilers, and optimizing for performance using specialized tools or flags. Moreover, the script is tailored to enable support and usage within Facebook's code base which introduces additional branch handling and environment setup, especially around CUDA and GPU settings for compilation and linking processes.",
            "total_tokens": 14747,
            "latency": 5.191262,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `cpp_builder.py`, is to build and compile C++ code for PyTorch, a popular deep learning framework, on various operating systems including Windows, Linux, and MacOS. The file contains classes and functions that handle tasks such as setting up build options, compiling C++ code, and linking libraries. It also provides functionality for building and compiling PyTorch-related C++ code, including support for CUDA and XPU devices. The file is designed to be used in conjunction with PyTorch's just-in-time (JIT) compiler and provides a way to build and compile C++ code for use with PyTorch models.",
            "total_tokens": 14745,
            "latency": 6.411135,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file \"cpp_builder.py\" defines a cross-platform C++ JIT builder for PyTorch, enabling dynamic compilation of C++ extensions on Linux, Windows, and MacOS, with specialized support for various compilers and device types (e.g., CUDA, XPU). It provides classes and functions for selecting appropriate compilers, constructing compilation and linking options, managing build directories, and generating or invoking build commands, with abstraction layers to support features like Torch AOT compilation, OpenMP, and integration with Python and PyTorch build systems. The code also includes utilities for generating CMake files and handling system-specific quirks, ensuring consistent extension building across environments.",
            "total_tokens": 14957,
            "latency": 2.872681,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/cpp_builder.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python module provides a cross-platform C++ builder for PyTorch's inductor backend, handling compiler detection (GCC, Clang, MSVC, Intel) and flag configuration across Linux, Windows, and macOS. It defines classes to manage build options, including Torch-specific and device-specific (CUDA/XPU) settings. The core `CppBuilder` class takes source files and options to execute the compilation process and generate the final shared library or object file.",
            "total_tokens": 18926,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.315763
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/cpp_builder.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file implements a cross-platform C++ build system for PyTorch's Inductor backend. It provides classes to manage complex compilation options, supporting various operating systems, compilers (GCC, Clang, MSVC, Intel), and hardware targets (CPU, CUDA, XPU). The module handles build flags, dependencies, and executes the compilation and linking process to generate shared libraries or object files.",
            "total_tokens": 19070,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.147013
        }
    },
    "/torch/distributed/fsdp/_dynamo_utils.py": {
        "nova-lite-v1": {
            "file_summary": "The `_dynamo_utils.py` file provides a utility function `_annotate_modules_for_dynamo` that annotates submodules within a given PyTorch module, excluding those specified in an ignored set, to indicate they are managed by Fully Sharded Data Parallel (FSDP). This annotation also saves the `use_orig_params` setting for FSDP, ensuring that Dynamo treats these modules as unspecialized, which is crucial for optimizing the interleaving of communication and computation during the backward pass.",
            "total_tokens": 793,
            "latency": 1.111393,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file \"_dynamo_utils.py\" contains a function named \"_annotate_modules_for_dynamo\" that takes three parameters: \"module\", \"ignored_modules\", and \"use_orig_params\". This function iterates through the submodules of the given module and annotates them as FSDP-managed modules, indicating that they should be treated as unspecialized NNModules by Dynamo. Additionally, it saves the \"use_orig_params\" setting passed to the FSDP constructor for each submodule. This annotation is necessary because Dynamo does not see the FSDP-wrapped modules during tracing, and it ensures that the wrapped modules are treated as unspecialized NNModules, which allows for better overlap of communication and computation during backward passes.",
            "total_tokens": 164,
            "latency": 3.718337,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines a utility function `_annotate_modules_for_dynamo` used specifically to annotate PyTorch modules managed by FullyShardedDataParallel (FSDP) for proper compatibility with Torch Dynamo (a tracing and compilation framework). The function identifies which modules need to be treated as unspecialized by Dynamo, an essential step to correctly manage parameter views and ensure proper computation-communication overlap during backward execution. Additionally, it sets internal module flags (`_is_fsdp_managed_module` and `_fsdp_use_orig_params`) to signal Dynamo how to handle FSDP modules appropriately.",
            "total_tokens": 773,
            "latency": 6.105133,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `_dynamo_utils.py` file is to define a function `_annotate_modules_for_dynamo` that annotates submodules in a given module's tree to indicate they are managed by FullyShardedDataParallel (FSDP) and saves the setting for using original parameters. The function helps Dynamo treat the wrapped modules inside FSDP as 'unspecialized' to avoid issues during graph execution, specifically related to handling original parameters and ensuring proper execution order during backward passes for overlapping computation and communication operations.",
            "total_tokens": 756,
            "latency": 6.783788,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"_dynamo_utils.py\" file is to annotate submodules in a PyTorch neural network module tree for use with FSDP (Fully Sharded Data Parallel) and Dynamo. It marks FSDP-managed modules and saves the \"use_orig_params\" setting, which is crucial for Dynamo to treat these modules as UnspecializedNNModules during tracing, enabling better performance and correct behavior in distributed training scenarios.",
            "total_tokens": 879,
            "latency": 2.617717,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `_dynamo_utils.py` file contains utility functions for integrating PyTorch's Fully Sharded Data Parallel (FSDP) with Dynamo, a PyTorch feature for optimizing model execution. Specifically, the file defines a function `_annotate_modules_for_dynamo` that annotates submodules within a given PyTorch module to indicate they are managed by FSDP and sets a flag to ensure Dynamo treats these modules as \"UnspecializedNNModule\" to handle parameters correctly during execution. This annotation is crucial for Dynamo to interleave communication and computation effectively during backward passes.",
            "total_tokens": 807,
            "latency": 1.60498,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `_dynamo_utils.py` file provides a utility function `_annotate_modules_for_dynamo` designed to annotate PyTorch submodules within a module tree to inform the Dynamo system about modules managed by `FullyShardedDataParallel` (FSDP). It excludes specified ignored modules and associates metadata flags to indicate FSDP management and the `use_orig_params` setting. This ensures Dynamo handles FSDP-wrapped modules as `UnspecializedNNModules`, enabling flexible tracing, effective graph handling, and improved computation-communication overlap in distributed training scenarios.",
            "total_tokens": 767,
            "latency": 1.623754,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the `_dynamo_utils.py` file is to annotate PyTorch modules for use with the Dynamo compiler, specifically for Fully Sharded Data Parallel (FSDP) modules. The `_annotate_modules_for_dynamo` function recursively annotates submodules in a given module, indicating that they are FSDP-managed and saving the `use_orig_params` setting. This annotation is necessary because Dynamo treats FSDP-wrapped modules as unspecialized NNModules, which allows for more thorough tracing and guards, and is required for correct backward execution.",
            "total_tokens": 771,
            "latency": 4.391246,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python script `_dynamo_utils.py` is designed to work with neural network modules, specifically to annotate and handle submodules within the primary module provided they are not part of the `ignored_modules` set. The annotation process mainly deals with managing Fully Sharded Data Parallel (FSDP) modules used in distributed training, marking them as managed by Dynamo for specialized processing during tracing and execution. This process includes handling FSDP specific conventions, like ensuring that original parameters are used (`use_orig_params=True`) and that dynamo treats these modules as unspecialized for optimization purposes during backward execution, aiding in the more efficient overlap of computation and communication tasks.",
            "total_tokens": 782,
            "latency": 4.376885,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `_dynamo_utils.py` file is to annotate submodules in a PyTorch module tree, excluding ignored modules, to indicate that they are managed by Fully Sharded Data Parallel (FSDP) and to store the `use_orig_params` setting. This annotation is necessary because Dynamo, a PyTorch compiler, treats FSDP-wrapped modules as unspecialized neural network modules, allowing for more flexible and efficient tracing and execution. The annotation is done by setting two attributes, `_is_fsdp_managed_module` and `_fsdp_use_orig_params`, on each submodule.",
            "total_tokens": 780,
            "latency": 1.909557,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file \"_dynamo_utils.py\" defines a utility function that traverses a given PyTorch module tree and annotates its submodules to indicate they are managed by Fully Sharded Data Parallel (FSDP), unless they are in a specified set of ignored modules. This annotation is necessary for integration with TorchDynamo, ensuring that FSDP-managed modules are treated in a way that allows proper tracing and handling of parameter views during dynamic graph compilation. Additionally, the function records whether original parameters are used, as required for Dynamo's support of FSDP.",
            "total_tokens": 767,
            "latency": 1.624568,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/fsdp/_dynamo_utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a utility function `_annotate_modules_for_dynamo` designed to work with PyTorch's FSDP (Fully Sharded Data Parallel) and Dynamo compiler. The function iterates through the submodules of a given `nn.Module` and annotates them with special attributes (`_is_fsdp_managed_module` and `_fsdp_use_orig_params`). These annotations signal to Dynamo that the modules are managed by FSDP and should be traced \"unspecially\", ensuring correct behavior, especially regarding parameter handling when `use_orig_params` is true.",
            "total_tokens": 1199,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.506757
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/fsdp/_dynamo_utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a function `_annotate_modules_for_dynamo` that iterates through submodules of a given PyTorch module. It adds specific attributes (`_is_fsdp_managed_module` and `_fsdp_use_orig_params`) to these submodules, excluding those in an ignored list. These annotations are used by PyTorch Dynamo to identify FSDP-managed modules and ensure they are treated as 'UnspecializedNNModule' for correct tracing and compilation behavior.",
            "total_tokens": 1070,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.799161
        }
    },
    "/torch/distributions/studentT.py": {
        "nova-lite-v1": {
            "file_summary": "The file \"studentT.py\" defines a custom `StudentT` distribution class that inherits from PyTorch's `Distribution` class. This class models a Student's t-distribution with parameters for degrees of freedom (`df`), mean (`loc`), and scale (`scale`). It includes methods for sampling, calculating log probabilities, and determining entropy, mean, mode, and variance of the distribution.",
            "total_tokens": 1418,
            "latency": 1.009313,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The Python file \"studentT.py\" defines a class \"StudentT\" that inherits from the \"Distribution\" class in the PyTorch library. This class represents a Student's t-distribution with parameters for degrees of freedom, mean, and scale. The class includes methods for generating random samples, calculating the log probability, and computing the entropy of the distribution. The file also includes helper functions for validating input arguments and broadcasting parameters to match the desired batch shape. Overall, the main functionality of the file is to provide a way to work with Student's t-distributions in a PyTorch environment.",
            "total_tokens": 128,
            "latency": 2.999454,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements a Student's t-distribution class for PyTorch, allowing users to create and work with Student's t-distributed random variables parametrized by degrees of freedom, location, and scale parameters. It supports calculation of statistical properties like mean, mode, variance, entropy, sampling (including differentiable samples via `rsample`), and evaluation of log probabilities. This class is suitable for probabilistic modeling and Bayesian inference tasks using PyTorch.",
            "total_tokens": 1230,
            "latency": 3.66496,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The \"studentT.py\" file defines a Student's t-distribution class in PyTorch for generating samples and calculating properties such as mean, mode, variance, log probability, and entropy. The distribution is parameterized by degrees of freedom, mean, and scale, and includes methods for sampling from the distribution, computing probabilities, and calculating entropy. The file also includes parameter constraints and examples demonstrating the usage of the StudentT class.",
            "total_tokens": 1224,
            "latency": 1.035149,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"studentT.py\" file is to implement a Student's t-distribution in PyTorch. It defines a StudentT class that inherits from the Distribution class, providing methods for sampling, calculating log probabilities, and computing statistical properties such as mean, mode, and variance. The implementation allows for parameterization by degrees of freedom, location, and scale, making it a flexible tool for working with Student's t-distributions in PyTorch-based machine learning and statistical applications.",
            "total_tokens": 1514,
            "latency": 3.137438,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `studentT.py` file defines a `StudentT` class that implements a Student's t-distribution within the PyTorch framework. It includes methods for sampling, calculating log probability, and entropy, and properties for mean, mode, and variance.",
            "total_tokens": 1387,
            "latency": 0.888129,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The given Python file defines a `StudentT` class, which represents a statistical Student's t-distribution. It provides methods for sampling from the distribution (`rsample`), calculating log probabilities (`log_prob`), and entropy, while handling distribution parameters such as degrees of freedom, mean, and scale. The implementation also includes functionality for broadcasting, expanding batch shapes, and calculating properties like mean, mode, and variance.",
            "total_tokens": 1222,
            "latency": 1.962168,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a class `StudentT` that represents a Student's t-distribution, a continuous probability distribution used in statistics. The class provides methods for calculating the mean, mode, variance, and entropy of the distribution, as well as for sampling from the distribution and calculating the log probability of a given value. The class is designed to work with PyTorch tensors and is part of the PyTorch distribution library.",
            "total_tokens": 1235,
            "latency": 3.755238,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The `studentT.py` file defines a `StudentT` class, which is a PyTorch implementation of the Student's t-distribution with parameters for degrees of freedom (`df`), location (`loc`), and scale (`scale`). This class inherits from Torch's `Distribution` class and includes methods for sampling (`rsample`), calculating the logarithm of probability density (`log_prob`), and computing the entropy of the distribution. It also handles various properties such as `mean`, `mode`, and `variance` of the distribution, which are calculated based on the distribution's parameters.",
            "total_tokens": 1258,
            "latency": 2.833733,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement a Student's t-distribution using PyTorch, allowing for the creation of a distribution object with specified degrees of freedom, mean, and scale. The class `StudentT` inherits from PyTorch's `Distribution` and provides methods for calculating properties such as mean, mode, variance, and entropy, as well as sampling from the distribution. The implementation includes input validation, broadcasting, and support for various mathematical operations, making it a robust and flexible tool for working with Student's t-distributions in PyTorch.",
            "total_tokens": 1260,
            "latency": 1.581986,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file implements a Student's t-distribution for PyTorch by defining a StudentT class, inheriting from torch.distributions.Distribution. This class supports sampling (including reparameterized sampling), computing log probabilities, entropy, mean, mode, and variance, and handles broadcasting of parameters (degrees of freedom, location, and scale). Overall, it enables probabilistic modeling and inference with Student's t-distributions in a tensor-based, differentiable context.",
            "total_tokens": 1229,
            "latency": 2.058752,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributions/studentT.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `StudentT` class within the PyTorch framework, representing the Student's t-distribution. It allows users to create instances of this distribution parameterized by degrees of freedom (`df`), location (`loc`), and scale (`scale`). The class provides methods for sampling from the distribution (`rsample`), calculating log probabilities (`log_prob`), and computing statistical properties like mean, variance, and entropy.",
            "total_tokens": 1748,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 3.892629
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributions/studentT.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `StudentT` class, which represents the Student's t-distribution within the PyTorch distributions framework. The class is parameterized by degrees of freedom, location, and scale. It provides methods for sampling from the distribution, calculating the log probability density function, and computing statistical properties like mean, mode, and variance.",
            "total_tokens": 1602,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 1.722131
        }
    },
    "/torch/distributed/rpc/functions.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"functions.py\" file is to provide a decorator `@async_execution` for functions that need to execute asynchronously within a distributed computing environment, particularly when using PyTorch's distributed RPC (Remote Procedure Call) framework. This decorator ensures that the function returns a `Future` object, allowing the function to pause and resume as needed. The decorator can be combined with other decorators like `@staticmethod`, `@classmethod`, and TorchScript decorators, with specific ordering rules to ensure proper recognition and execution.",
            "total_tokens": 2179,
            "latency": 1.256884,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to provide a decorator for a function that indicates the return value of the function is guaranteed to be a Future object and can run asynchronously on the RPC callee. The decorator allows the callee to extract the Future returned by the wrapped function and install subsequent processing steps as a callback to that Future. The installed callback will read the value from the Future when completed and send the value back as the RPC response. The decorator also works with TorchScript decorators, static or class methods, and RRef helpers.",
            "total_tokens": 115,
            "latency": 2.847109,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines a decorator `async_execution`, which is meant to identify functions whose return values are guaranteed to be `Future` objects, allowing asynchronous remote procedure calls (RPC) in PyTorch's distributed framework. This decorator helps PyTorch RPC recognize the decorated functions as returning Futures and handle async execution accordingly, especially useful when interacting with `rpc_async` or other operations requiring waiting and callbacks. The extensive comments and examples illustrate usage with RPC APIs, TorchScript, and class or static methods within distributed PyTorch environments.",
            "total_tokens": 1953,
            "latency": 4.99254,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The file \"functions.py\" contains a Python decorator named \"async_execution\" that is used to mark functions whose return values are guaranteed to be a specific type of object and can run asynchronously on the RPC callee. The decorator allows for handling parallel, asynchronous execution of functions and their subsequent processing steps as callbacks on the RPC callee side. It provides examples and guidelines on how to use the decorator in various scenarios, including combining it with TorchScript decorators, static or class methods, and working with RRef helpers in distributed PyTorch applications.",
            "total_tokens": 1936,
            "latency": 1.509843,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"functions.py\" file is to define a decorator called \"async_execution\" for use in distributed RPC (Remote Procedure Call) scenarios in PyTorch. This decorator is designed to handle functions that return Future objects, allowing for asynchronous execution of RPC calls. It provides a way to manage and process the results of asynchronous operations in distributed computing environments using PyTorch's RPC framework.",
            "total_tokens": 2341,
            "latency": 2.846489,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `functions.py` file defines a decorator `async_execution` that marks a function to be executed asynchronously in a distributed computing environment using PyTorch's RPC framework. The decorator ensures that the function returns a `Future` object and handles the asynchronous execution on the RPC callee side.",
            "total_tokens": 2129,
            "latency": 1.027257,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `functions.py` file provides an `async_execution` decorator for enabling asynchronous execution of functions in distributed RPC (Remote Procedure Call) workflows using PyTorch. This decorator ensures that the function returns a `torch.futures.Future` object and allows subsequent processing steps to be installed as callbacks, facilitating asynchronous workflows across multiple workers. It supports integration with other decorators like `@staticmethod`, `@classmethod`, and TorchScript decorators, as demonstrated through examples in distributed computations.",
            "total_tokens": 1941,
            "latency": 1.84776,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to define a decorator `async_execution` that indicates a function returns a `Future` object and can run asynchronously on the RPC callee. The decorator allows the callee to extract the `Future` returned by the wrapped function and install subsequent processing steps as a callback to that `Future`. This enables asynchronous execution of the function, allowing it to pause and resume due to containing `rpc_async` or waiting for other signals.",
            "total_tokens": 1925,
            "latency": 3.644145,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file 'functions.py' defines a decorator `async_execution` that transforms a regular function into one that guarantees an asynchronous execution style that returns a torch Future object. This decorator is specifically designed for use with RPC (Remote Procedure Call) in PyTorch distributed computing environments, facilitating non-blocking operations that can wait for other asynchronous operations or signals before proceeding. Examples and explanations within the file illustrate how this decorator interacts with different types of functions (e.g., static methods, class methods) and how to use it in various scenarios within a distributed system using RPC and Future objects, along with its integration with TorchScript and RRef helpers.",
            "total_tokens": 1957,
            "latency": 6.245066,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to define a decorator `async_execution` that allows a function to run asynchronously on the RPC callee, enabling the function to pause and resume execution due to operations like `torch.distributed.rpc.rpc_async` or waiting for other signals. This decorator is useful for functions that return a `torch.futures.Future` object and need to handle asynchronous execution. The decorator can be combined with other decorators like `@staticmethod`, `@classmethod`, or TorchScript decorators, and it works with RRef helpers like `torch.distributed.rpc.RRef.rpc_sync` and `torch.distributed.rpc.RRef.rpc_async`.",
            "total_tokens": 1965,
            "latency": 1.570909,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "This file defines a decorator, async_execution, intended for use with PyTorch's RPC (remote procedure call) functions, marking that the decorated function will return a torch.futures.Future and can execute asynchronously on a remote worker. The decorator adds a special attribute to assist PyTorch RPC in recognizing such asynchronous functions, ensuring they are handled correctly within distributed settings. Extensive documentation and usage examples are provided, illustrating combinations with TorchScript, static/class methods, and RRef helpers, to guide users in various distributed computing scenarios.",
            "total_tokens": 1951,
            "latency": 1.973255,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/rpc/functions.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a decorator named `async_execution`. This decorator is used to mark functions that are intended to be executed asynchronously within the PyTorch RPC framework. It signals that the decorated function will return a `torch.futures.Future`, allowing the RPC system to manage the asynchronous completion and result retrieval on the remote worker.",
            "total_tokens": 2322,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 1.799999
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/rpc/functions.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `async_execution` decorator for use with PyTorch's distributed RPC framework. This decorator marks a function to indicate that it returns a `torch.futures.Future` and can be executed asynchronously on the remote worker (callee). The RPC system leverages this decorator to properly handle the Future's completion on the callee side and send the final result back to the caller, facilitating non-blocking remote procedure calls.",
            "total_tokens": 2757,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.093892
        }
    },
    "/torch/jit/_dataclass_impls.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file provides functionality for synthesizing magic methods (like `__init__`, `__repr__`, `__hash__`, `__eq__`, and `__ne__`) for dataclasses when they are JIT-compiled, specifically for use with PyTorch's TorchScript. It includes helper functions to generate and parse these methods, ensuring compatibility and proper behavior within the TorchScript environment. The file also handles special cases such as `InitVar` fields and optional types, while raising appropriate errors for unsupported features.",
            "total_tokens": 1978,
            "latency": 0.82649,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to synthesize magic methods for JIT-compiled dataclasses. It includes functions for generating the `__init__`, `__repr__`, `__hash__`, `__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, and `__ge__` methods. The file also handles InitVar annotations and optional fields in the dataclasses. The synthesized methods are stored in a dictionary called `DATACLASS_MAGIC_METHODS`.",
            "total_tokens": 113,
            "latency": 2.819236,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.5": {
            "file_summary": "This Python module provides functionality to dynamically synthesize dataclass magic methods (such as `__init__`, `__repr__`, `__hash__`, and comparison/equality methods) specifically tailored for compatibility with TorchScript compilation. It parses and generates method definitions as abstract syntax trees (ASTs), handles special dataclass features like `InitVar` and optional fields, and ensures compliance with TorchScript interpreter constraints. It allows automatic JIT compilation and integration of Python dataclasses into PyTorch's TorchScript by defining placeholder and custom synthesized implementations for dataclass magic methods.",
            "total_tokens": 1766,
            "latency": 5.187836,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the given Python file \"_dataclass_impls.py\" is to provide functions for synthesizing magic methods for JIT-compiled dataclasses using TorchScript. These functions include generating methods such as `__init__`, `__repr__`, `__hash__`, and other comparison methods like `__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, and `__ge__`. The file handles creating synthetic methods for dataclasses and dealing with specific cases like InitVars and optional field comparisons for TorchScript compatibility.",
            "total_tokens": 1778,
            "latency": 1.901149,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "This Python file contains functions for synthesizing magic methods for JIT-compiled dataclasses in the context of TorchScript. It provides implementations for various dunder methods such as __init__, __repr__, __hash__, and comparison methods (__eq__, __ne__, __lt__, __le__, __gt__, __ge__) for dataclasses. The file also includes helper functions to compose and parse these synthesized methods, handling special cases like InitVar annotations and optional fields.",
            "total_tokens": 2144,
            "latency": 4.374651,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `_dataclass_impls.py` file is to synthesize magic methods for JIT-compiled dataclasses in TorchScript, including `__init__`, `__repr__`, `__hash__`, and comparison methods like `__eq__`, `__ne__`, etc. It generates the appropriate method implementations based on the dataclass fields and their annotations. The synthesized methods are then parsed into abstract syntax trees (ASTs) for further processing.",
            "total_tokens": 1970,
            "latency": 1.356936,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4o": {
            "file_summary": "The `_dataclass_impls.py` file contains functions for synthesizing special magic methods, such as `__init__`, `__repr__`, `__hash__`, and comparison methods (`__eq__`, `__ne__`, etc.), for dataclasses in TorchScript, a subset of Python used in PyTorch for JIT compilation. It enables defining custom implementations of these methods by generating Python code snippets dynamically and parsing them into Abstract Syntax Trees (ASTs). The file also handles specific cases, such as optional fields, InitVar annotations, and raises appropriate errors for unsupported features like default factory initializers or `None` comparisons.",
            "total_tokens": 1781,
            "latency": 2.799993,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to synthesize magic methods for JIT-compiled dataclasses in PyTorch. It provides functions to generate implementations for special methods such as `__init__`, `__repr__`, `__hash__`, and comparison operators (`__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, `__ge__`) for dataclasses. These synthesized methods are used to support dataclass usage in TorchScript.",
            "total_tokens": 1766,
            "latency": 3.967819,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4": {
            "file_summary": "The Python file `_dataclass_impls.py` primarily provides functionality to synthesize magic methods for dataclasses that are meant to be compiled by TorchScript, a Python script to machine code compiler. It includes capabilities to synthesize constructors (`__init__`), representations (`__repr__`), hash functions (`__hash__`), and comparison operators (like `__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, `__ge__`) for dataclasses based on predefined templates and handling of special cases like optional fields or unsupported features (e.g., default factory initializers). The synthesized methods are transformed into parseable Python AST (Abstract Syntax Trees) structures, ensuring they are tailored to fit the requirements and limitations of TorchScript while still preserving original dataclass behavior where possible.",
            "total_tokens": 1829,
            "latency": 4.468397,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the given Python file `_dataclass_impls.py` is to synthesize magic methods for JIT-compiled dataclasses in PyTorch, including `__init__`, `__repr__`, `__hash__`, and comparison methods. These synthesized methods are used to support dataclasses in TorchScript, allowing for more seamless integration with PyTorch's just-in-time compilation. The file provides a set of functions that generate the necessary code for these magic methods based on the fields and annotations defined in the dataclass.",
            "total_tokens": 1775,
            "latency": 1.949948,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.1": {
            "file_summary": "This file provides functions to programmatically synthesize and generate magic methods (such as __init__, __repr__, __hash__, and rich comparison methods) for Python dataclasses to ensure compatibility with TorchScript\u2019s JIT compilation. It constructs method ASTs and signatures dynamically, handling Python dataclass features and TorchScript limitations (e.g., disallowing default_factory and handling InitVar and optional fields). The file exposes a mapping (DATACLASS_MAGIC_METHODS) that links method names to their synthetic generators, enabling automated injection of these methods into dataclasses during JIT compilation for PyTorch.",
            "total_tokens": 1771,
            "latency": 1.687974,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/jit/_dataclass_impls.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides functions to synthesize standard Python magic methods like `__init__`, `__repr__`, `__eq__`, etc., for dataclasses. These synthesized methods are specifically designed to be compatible with TorchScript's JIT compilation process. The code generates Python source strings for these methods and parses them into AST nodes wrapped in `ParsedDef` objects.",
            "total_tokens": 2240,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.240058
        },
        "gemini-2.5": {
            "file_name": "/torch/jit/_dataclass_impls.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides functionality to automatically synthesize the source code for standard magic methods (like `__init__`, `__repr__`, `__eq__`, etc.) for dataclasses. These generated methods are specifically designed to be compatible with PyTorch's TorchScript JIT compiler. The code inspects the dataclass fields and attributes to construct the method bodies as strings, which are then parsed into Abstract Syntax Trees (ASTs) for TorchScript consumption.",
            "total_tokens": 2678,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 8.757888
        }
    },
    "/tools/linter/adapters/shellcheck_linter.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"shellcheck_linter.py\" file is to run the shellcheck linter on specified files, capturing and formatting linting results as JSON output. It uses the `subprocess` module to execute shellcheck, processes the JSON output to create lint messages, and handles errors such as the absence of the shellcheck command. The script is designed to be run from the command line, accepting file paths as arguments and outputting lint messages in JSON format.",
            "total_tokens": 958,
            "latency": 1.092479,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the \"shellcheck_linter.py\" file is to run the shellcheck command on the provided list of files and output the results in JSON format. The file includes error handling for cases where shellcheck is not installed or the command fails. It also includes a function to run a command and log the execution time. The output includes information about the file path, line number, character position, code, severity, name, original text, replacement text, and description for each lint message.",
            "total_tokens": 104,
            "latency": 2.445549,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python script acts as a wrapper around ShellCheck, a tool used to lint shell scripts for common issues and potential errors. It executes ShellCheck on provided file names, parses its JSON-formatted output, and transforms results into structured lint messages. The script also includes proper error handling for the scenario where ShellCheck is not installed or command execution fails.",
            "total_tokens": 829,
            "latency": 4.436866,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "This Python script is a shellcheck linter that runs shellcheck on specified files and outputs the lint results in a JSON format. It defines classes for lint severity and lint message, runs shellcheck using subprocess and captures the output as JSON, then parses the JSON output to create lint message objects for each issue found in the files being checked. Additionally, it includes a command-line interface for specifying files to lint and checks if shellcheck is installed before running.",
            "total_tokens": 845,
            "latency": 1.013356,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"shellcheck_linter.py\" file is to run the ShellCheck linter on specified shell script files and report any issues found. It uses the ShellCheck command-line tool to analyze the files, parses the JSON output, and generates lint messages in a standardized format. The script can be run as a standalone tool, accepting file paths as arguments, and it checks for the presence of ShellCheck before execution.",
            "total_tokens": 1061,
            "latency": 2.70611,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `shellcheck_linter.py` file is a Python script that runs the ShellCheck linter on specified shell script files. It processes the output in JSON format, converts it into a list of `LintMessage` objects, and prints each lint message as a JSON object. The script also handles errors, such as when ShellCheck is not installed.",
            "total_tokens": 933,
            "latency": 1.008776,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The file implements a Python-based linter wrapper for the `shellcheck` tool, which analyzes shell script files and returns linting messages in JSON format. It defines severity levels, formats the linting results into structured data, and handles errors like missing `shellcheck` installation. The script can be executed to check specified files for issues, outputting the results in a consistent JSON structure.",
            "total_tokens": 837,
            "latency": 1.404488,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to run the shellcheck linter on a list of files and output the linting results in JSON format. The script checks for errors, warnings, and advice in the files and prints out the corresponding lint messages. If the shellcheck command is not installed, the script will output an error message and exit.",
            "total_tokens": 830,
            "latency": 2.83789,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file named \"shellcheck_linter.py\" is a tool that leverages the shellcheck utility to lint shell scripts. The file defines a command-line interface that accepts file paths as arguments to be linted by invoking the 'shellcheck' command with specific arguments and parsing its JSON-formatted output into structured lint messages. It encapsulates each linting outcome in a structured format with details about the file, line number, character position, severity of the issue, unique code, and description of the issue.",
            "total_tokens": 857,
            "latency": 3.348732,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `shellcheck_linter.py` file is to run the ShellCheck linter on a list of provided files, parsing the output and printing any linting errors or warnings in a JSON format. The script uses the `subprocess` module to execute the `shellcheck` command and handles errors, such as the command not being installed. The script also supports command-line arguments for specifying the files to lint and checks if the `shellcheck` command is installed before attempting to run it.",
            "total_tokens": 864,
            "latency": 1.490354,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file shellcheck_linter.py is a command-line tool that runs ShellCheck on given shell script files, capturing and formatting linting diagnostics into a standardized JSON output. It parses ShellCheck's JSON output, maps lint messages to a unified LintMessage structure, and reports errors or warnings for each file checked. If ShellCheck is not installed or fails to run, the tool emits a descriptive error message in the same structured format.",
            "total_tokens": 846,
            "latency": 1.29149,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/linter/adapters/shellcheck_linter.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script acts as a wrapper for the `shellcheck` command-line tool. It takes shell script filenames as input, executes `shellcheck` with JSON output formatting, and parses the results. The script then prints each detected linting issue as a JSON object, including checks for `shellcheck` installation.",
            "total_tokens": 1524,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.178118
        },
        "gemini-2.5": {
            "file_name": "/tools/linter/adapters/shellcheck_linter.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script serves as a wrapper for the `shellcheck` command-line tool to lint shell script files. It takes a list of filenames as input, executes `shellcheck` with JSON output enabled, and parses the results. Finally, it formats each lint finding into a structured `LintMessage` object and prints it as a JSON string to standard output.",
            "total_tokens": 1350,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 5.337724
        }
    },
    "/torch/distributed/optim/post_localSGD_optimizer.py": {
        "nova-lite-v1": {
            "file_summary": "The `post_localSGD_optimizer.py` file defines a `PostLocalSGDOptimizer` class that wraps an arbitrary PyTorch optimizer to implement the post-local SGD optimization algorithm. This optimizer runs a local optimizer at each step and periodically averages the model parameters after the local optimizer is applied, following a warm-up stage. The class integrates with PyTorch's distributed training capabilities, allowing for efficient model training across multiple devices or nodes.",
            "total_tokens": 1355,
            "latency": 0.980652,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"post_localSGD_optimizer.py\" is to define a class called \"PostLocalSGDOptimizer\" that wraps an arbitrary optimizer and runs post-local SGD algorithm. This optimizer runs the local optimizer at every step and averages parameters periodically after the local optimizer is applied. It also includes methods to save and load the optimizer's state, including the step counter for the model averager. The class is designed to be used with PyTorch's distributed data parallel (DDP) module for distributed training.",
            "total_tokens": 121,
            "latency": 3.004525,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python file implements a \"PostLocalSGDOptimizer\" class in PyTorch, wrapping around a standard optimizer to facilitate training with Post-local SGD, a distributed optimization algorithm. After an initial warm-up period, this optimizer periodically averages model parameters across nodes to achieve faster distributed training convergence. It supports state saving and loading for continuation of training from checkpoints.",
            "total_tokens": 1182,
            "latency": 3.550377,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the \"post_localSGD_optimizer.py\" file is to define a custom optimizer called \"PostLocalSGDOptimizer\" that wraps an existing optimizer and implements the post-local SGD algorithm. This optimizer runs a local optimizer at every step and averages parameters periodically after applying the local optimizer. It also includes methods for saving and loading the optimizer state, performing optimization steps, and managing parameter groups.",
            "total_tokens": 1202,
            "latency": 0.937667,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"post_localSGD_optimizer.py\" file is to implement a PostLocalSGDOptimizer class, which wraps an arbitrary PyTorch optimizer and applies the post-local SGD algorithm. This optimizer runs a local optimizer at every step and, after a warm-up stage, periodically averages parameters across distributed processes to improve model performance in distributed training scenarios. The file also includes methods for state management, parameter updates, and integration with PyTorch's distributed training ecosystem.",
            "total_tokens": 1528,
            "latency": 3.39965,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `post_localSGD_optimizer.py` file defines a `PostLocalSGDOptimizer` class that wraps an existing PyTorch optimizer to implement the post-local SGD algorithm. This optimizer applies the local optimizer at every step and periodically averages parameters after the warm-up stage. The class provides methods to save and load the optimizer's state, including the model averager's step.",
            "total_tokens": 1345,
            "latency": 1.146974,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `post_localSGD_optimizer.py` file defines a `PostLocalSGDOptimizer` class, a wrapper around the PyTorch optimizer, to implement the post-local SGD algorithm. It combines a local optimizer with periodic global parameter averaging using a model averager, enabling efficient distributed training during and after a warmup phase. The class also ensures proper state management for checkpointing to preserve and restore the optimizer and averager states.",
            "total_tokens": 1198,
            "latency": 1.932739,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to implement a custom PyTorch optimizer called `PostLocalSGDOptimizer` that wraps an arbitrary PyTorch optimizer and runs the post-local SGD algorithm. This algorithm applies local optimization at every step and periodically averages parameters after a warm-up stage. The optimizer is designed to work with distributed data parallelism (DDP) and can be used to train models in a distributed setting.",
            "total_tokens": 1211,
            "latency": 3.496911,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file defines a class `PostLocalSGDOptimizer` that extends `torch.optim.Optimizer` to implement the post-local Stochastic Gradient Descent (SGD) algorithm. This optimizer wraps any Torch optimizer and combines local optimization with periodic model averaging after a warm-up stage, based on the `post-local SGD` technique discussed in a published research paper. The class includes methods such as `step`, `zero_grad`, `add_param_group`, `state_dict`, and `load_state_dict` to manage the state and behavior of the optimizer, integrating the additional functionality to handle periodic averaging via an averager provided during initialization.",
            "total_tokens": 1249,
            "latency": 4.981761,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement a custom optimizer, `PostLocalSGDOptimizer`, which wraps an arbitrary PyTorch optimizer and applies the post-local SGD algorithm. This algorithm runs local optimization at every step and averages parameters periodically after a warm-up stage. The optimizer is designed to work with distributed data parallelism and can be used to train models in a distributed setting.",
            "total_tokens": 1207,
            "latency": 1.125943,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines PostLocalSGDOptimizer, a wrapper around any PyTorch optimizer that implements the post-local SGD algorithm, which performs local parameter updates and periodically averages parameters across workers after a warm-up phase, facilitating efficient distributed training. It manages optimizer state, handles saving/loading of both optimizer and model averager step, and delegates most optimizer methods to the wrapped optimizer while integrating parameter averaging logic. This enables seamless usage of post-local SGD in distributed settings, especially in conjunction with DistributedDataParallel and model averaging modules.",
            "total_tokens": 1214,
            "latency": 1.502515,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/optim/post_localSGD_optimizer.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines `PostLocalSGDOptimizer`, a wrapper around a standard PyTorch optimizer. It implements the post-local SGD algorithm by first applying the wrapped optimizer's step and then periodically averaging model parameters using a provided averager. The optimizer also handles saving and loading the averager's internal step count along with the optimizer's state.",
            "total_tokens": 1657,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.087074
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/optim/post_localSGD_optimizer.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `PostLocalSGDOptimizer` class, which wraps a standard PyTorch optimizer to implement the Post-Local SGD algorithm for distributed training. It executes the underlying local optimizer at each step. After an optional warm-up phase, it periodically averages the model parameters across distributed processes using a provided model averager.",
            "total_tokens": 1606,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 4.535956
        }
    },
    "/torchgen/operator_versions/gen_mobile_upgraders.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `gen_mobile_upgraders.py` file is to generate C++ source code for mobile bytecode upgraders based on the provided bytecode definitions. It reads bytecode information, constructs C++ templates for upgrader functions, and writes the final C++ source code to a specified directory within a PyTorch project. The script also handles version mappings and excludes certain operators during the generation process.",
            "total_tokens": 3629,
            "latency": 1.12652,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to generate a C++ file (upgrader\\_mobile.cpp) that contains a list of bytecode functions and operator strings for mobile devices. The file uses various code templates and constructs to generate the necessary code. It also sorts the upgrader list and excludes certain operators and upgraders from the final output. The generated C++ file is then written to the specified directory.",
            "total_tokens": 87,
            "latency": 2.572564,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python script generates C++ source code for managing operator version upgrades in PyTorch mobile, focusing on creating bytecode definitions that can upgrade model operators from older versions to newer ones on mobile devices. It parses upgrade definitions, constructs bytecode instructions, constants, operators, and type lists from the definitions, and then generates a C++ source file for Torch's mobile JIT runtime. The resulting C++ file helps PyTorch mobile applications maintain compatibility by upgrading serialized models' operator implementations dynamically.",
            "total_tokens": 2965,
            "latency": 5.498746,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided Python script generates bytecodes for upgrading mobile versions of operators in PyTorch. It includes functions to construct instructions, constants, operators, and type lists, as well as to write the generated C++ code into a file named 'upgrader_mobile.cpp' in the specified PyTorch directory. The script processes a list of upgrader dictionaries, sorts them, and generates bytecode functions based on the provided data.",
            "total_tokens": 3032,
            "latency": 1.419613,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"gen_mobile_upgraders.py\" file is to generate and write C++ code for mobile upgraders in PyTorch. It processes bytecode information for various upgraders, constructs version maps, and creates a C++ file containing the necessary data structures and functions for mobile upgraders. The script also handles the organization and sorting of upgrader information before writing it to the output file.",
            "total_tokens": 4018,
            "latency": 2.977219,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the file `gen_mobile_upgraders.py` is to generate C++ source code for mobile upgraders in PyTorch by constructing bytecode functions and version maps, and writing them to a specified path.",
            "total_tokens": 3590,
            "latency": 0.915312,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The script `gen_mobile_upgraders.py` focuses on generating C++ source code for mobile upgrader functionality in PyTorch. It processes data such as instructions, constants, and operator definitions to generate a C++ file (`upgrader_mobile.cpp`) that includes bytecode and version mappings for mobile operator upgrades. The generated file facilitates backward compatibility by defining how older model operators should be upgraded to ensure compatibility with newer runtime environments.",
            "total_tokens": 2953,
            "latency": 2.349103,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to generate C++ code for mobile upgraders in the PyTorch framework. It takes a list of upgrader dictionaries as input, constructs the necessary C++ code for each upgrader, and writes the generated code to a file named \"upgrader_mobile.cpp\". The file also defines various helper functions for constructing different parts of the C++ code, such as instructions, constants, operators, and types.",
            "total_tokens": 3045,
            "latency": 4.377408,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python script `gen_mobile_upgraders.py` primarily supports the generation of C++ code related to model upgraders for mobile platforms using PyTorch. The functions included in the file handle the construction and organization of upgrader specifications into structured formats such as bytecode instructions, constants, types, and operators which are later used to generate a C++ source file (`upgrader_mobile.cpp`). It also ensures filtering and sorting of operators and upgraders based on given constraints and criteria, supporting the maintenance and management of operator versioning for mobile deployment.",
            "total_tokens": 3059,
            "latency": 3.110574,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `gen_mobile_upgraders.py`, is to generate C++ code for mobile upgraders in the PyTorch framework. It takes a list of upgrader dictionaries as input, constructs the necessary C++ code for each upgrader, and writes the generated code to a file named `upgrader_mobile.cpp`. The file utilizes various templates and functions to construct the C++ code, including instruction lists, constant lists, operator strings, and version maps.",
            "total_tokens": 3056,
            "latency": 1.827399,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The gen_mobile_upgraders.py file generates C++ source code required for mobile operator upgraders in PyTorch by processing Python-level operator upgrader definitions and converting them into bytecode representations. It constructs C++ code for upgrader functions, their associated bytecode, and operator version maps, which are then output to upgrader_mobile.cpp for integration into PyTorch Mobile. The script is primarily used to bridge Python upgrader definitions and their deployment in the C++-based mobile runtime of PyTorch.",
            "total_tokens": 2969,
            "latency": 1.73474,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torchgen/operator_versions/gen_mobile_upgraders.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script generates bytecode representations for PyTorch mobile operator upgraders and retrieves operator versioning information. It processes this data and uses code templates to construct C++ source code defining the upgraders and their version map. Finally, it writes this generated C++ code to the file `upgrader_mobile.cpp` for use in the mobile runtime.",
            "total_tokens": 4416,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.141936
        },
        "gemini-2.5": {
            "file_name": "/torchgen/operator_versions/gen_mobile_upgraders.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script generates a C++ source file (`upgrader_mobile.cpp`) for the PyTorch mobile runtime. It retrieves operator versioning information and bytecode for model upgraders using internal PyTorch functions. The script then formats this data into C++ structures and maps using predefined code templates, enabling the mobile runtime to handle compatibility for older TorchScript models.",
            "total_tokens": 5112,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 13.395299
        }
    },
    "/torch/nn/functional.py": {
        "nova-lite-v1": {
            "file_summary": "This file defines various functional operations for tensor manipulation and neural network computations in PyTorch, including convolution, pooling, normalization, activation functions, loss functions, and attention mechanisms. It provides a comprehensive set of utility functions for building and training deep learning models. The functions are designed to handle different input shapes and configurations, and support various optimizations and backends for improved performance.",
            "total_tokens": 69606,
            "latency": 5.937781,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This file provides a collection of PyTorch's functional API implementations, particularly for convolution operations (`conv1d`, `conv2d`, etc.), pooling layers (`max_pool`, `avg_pool`, etc.), normalization layers (`batch_norm`, `layer_norm`, etc.), activation functions (like `relu`, `softmax`), losses (including cross-entropy and MSE), dropout methods, embedding lookups, and multi-head attention mechanisms (`scaled_dot_product_attention` and related functions). Each function contains comprehensive documentation for usage and argument details. Overall, it serves as PyTorch's backend implementations to support neural network modules and custom layers without requiring explicit class definitions.",
            "total_tokens": 61324,
            "latency": 38.340197,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "This Python file contains implementations of various neural network functions and operations, primarily focused on attention mechanisms and related components. Here's a summary of the main functionality:\n\n1. The file defines several attention-related functions, including:\n   - `scaled_dot_product_attention`\n   - `multi_head_attention_forward`\n   - Various utility functions for shape checking, mask handling, and projection operations\n\n2. It implements pooling operations like `max_pool1d`, `max_pool2d`, `max_pool3d`, and their adaptive variants.\n\n3. There are implementations of various loss functions, such as:\n   - `nll_loss`\n   - `cross_entropy`\n   - `binary_cross_entropy`\n   - `kl_div`\n   - `mse_loss`\n   - `l1_loss`\n\n4. The file includes activation functions like `relu`, `gelu`, `silu`, and `softmax`.\n\n5. There are normalization functions like `batch_norm`, `layer_norm`, and `group_norm`.\n\n6. It provides implementations for various dropout techniques, including `dropout`, `alpha_dropout`, and `feature_alpha_dropout`.\n\n7. The file includes utility functions for handling tensors, such as `unfold`, `fold`, and `interpolate`.\n\n8. There are implementations of distance and similarity measures like `cosine_similarity` and `pairwise_distance`.\n\nOverall, this file serves as a comprehensive collection of neural network building blocks, with a particular emphasis on attention mechanisms and related operations used in transformer architectures. It's likely part of a larger deep learning framework or library.",
            "total_tokens": 74334,
            "latency": 15.723343,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the file `functional.py` is to provide a collection of functional operations for neural networks in PyTorch, including convolutions, pooling, activation functions, normalization, loss functions, upsampling, padding, and attention mechanisms. These functions are designed to be used directly on tensors without the need for creating module instances, facilitating flexible and modular neural network construction.",
            "total_tokens": 69608,
            "latency": 12.662318,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The provided Python file defines various functionalities for deep learning operations, primarily focusing on the PyTorch framework. It includes methods for convolutional layers (e.g., `conv1d`, `conv2d`), pooling layers, and various activation functions (e.g., `relu`, `sigmoid`). Advanced functionalities such as multihead attention with `multi_head_attention_forward` and `scaled_dot_product_attention` are provided, along with utilities for handling special data transformations (e.g., `grid_sample`, `pixel_shuffle`, `interpolate`). Additionally, it encapsulates useful loss computations, such as `cross_entropy`, `triplet_margin_loss`, and `ctc_loss`, offering broad utility in building neural network models.",
            "total_tokens": 61336,
            "latency": 35.705794,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide various functions for neural network operations, including convolutional, pooling, activation, loss, and attention mechanisms. These functions are designed to be used in deep learning models and are implemented using PyTorch, a popular open-source machine learning library.",
            "total_tokens": 60447,
            "latency": 48.251515,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The code in the Python file mainly deals with defining multi-head attention mechanisms common in models like transformers. It consists of several functions that handle the calculations for attention mechanisms, including setting up the weight matrices (`in_projection`), conducting the scaled dot product attention (`scaled_dot_product_attention`), and handling multiple heads in attention layers (`multi_head_attention_forward`). These functions are essential components for building complex neural network architectures like Transformer models, which are widely used in natural language processing tasks. Overall, the functions work together to facilitate the attention mechanism, allowing a neural network to focus on different parts of the input data sequentially.",
            "total_tokens": 60507,
            "latency": 11.744846,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the given Python file is to provide various functions for neural network operations, including convolution, pooling, activation functions, loss functions, and attention mechanisms. These functions are part of the PyTorch library and are used to build and train neural networks. The file contains a large number of functions, each with its own specific purpose, such as `conv1d`, `conv2d`, `conv3d`, `max_pool1d`, `max_pool2d`, `max_pool3d`, `dropout`, `relu`, `sigmoid`, `tanh`, `softmax`, `cross_entropy`, `binary_cross_entropy`, `mean_squared_error`, and many others. The file also includes functions for attention mechanisms, such as `scaled_dot_product_attention` and `multi_head_attention_forward`. Overall, the file provides a wide range of functions for building and training neural networks using PyTorch.",
            "total_tokens": 60574,
            "latency": 15.786967,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "This file provides the functional interface for the PyTorch `torch.nn.functional` module, defining a wide range of stateless neural network operations such as convolutions, pooling, normalization, activations, loss functions, distance computations, and utility functions for tensor manipulation (e.g., upsampling and padding). It acts as a bridge to core PyTorch C++/CUDA kernels, adds comprehensive type annotations and docstrings, manages various input options (e.g., handling `__torch_function__` overrides and broadcasting), and includes additional logic for deprecated features, backward compatibility, and expert-level dispatch required for neural network layer implementations. Overall, it enables users and other PyTorch components to compose neural network computations as pure functions without binding them to stateful modules.",
            "total_tokens": 61342,
            "latency": 6.657828,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/functional.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides functional implementations of various neural network operations within the `torch.nn.functional` module. It includes functions for common tasks like convolutions, pooling, activation functions, normalization layers, and loss calculations. These stateless functions operate directly on tensors and serve as the computational backend for their corresponding `torch.nn.Module` counterparts.",
            "total_tokens": 70959,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.012917
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/functional.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a functional interface for many standard PyTorch neural network operations, acting as the stateless counterpart to layers found in `torch.nn`. It includes a wide range of functions covering convolutions, pooling, activation functions, normalization techniques, loss functions, embeddings, and attention mechanisms. These functions operate directly on tensors, providing building blocks for constructing neural networks without necessarily using the stateful `nn.Module` classes.",
            "total_tokens": 70999,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.004219
        }
    },
    "/benchmarks/dynamo/runner.py": {
        "nova-lite-v1": {
            "file_summary": "The `runner.py` script is a command-line tool designed to automate and manage benchmark tests for PyTorch-related projects. It generates shell commands for running benchmarks, executes these commands, and processes the results to produce summaries, visualizations, and reports. The script supports various benchmark configurations, including different compilers, data types, and benchmark suites, and it can output results in CSV format and generate graphs for performance metrics. Additionally, it integrates with a performance dashboard to update it with new benchmark results and detect regressions.",
            "total_tokens": 14487,
            "latency": 1.913131,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given python file is to generate and analyze benchmark results for different models, devices, and compilers. It supports both training and inference modes.\n\nThe script generates commands for running benchmarks, executes them, and then parses the results. It also generates summary statistics, plots graphs, and detects regressions.\n\nThe script uses the following libraries: argparse, dataclasses, datetime, glob, importlib, io, logging, matplotlib.pyplot, numpy, pandas, re, scipy.stats, subprocess, sys, tabulate, and torch.\n\nThe script has several functions, including:\n\n* `generate_commands`: generates bash commands for running benchmarks.\n* `generate_summary`: generates a build summary.\n* `archive_data`: archives the results.\n* `get_archive_name`: generates a unique archive name.\n* `build_summary`: generates a build summary.\n* `archive`: archives the results.\n* `Parser`: a class for parsing benchmark results.\n* `ParsePerformanceLogs`: a subclass of Parser for parsing performance logs.\n* `generate_executive_summary`: generates an executive summary.\n* `generate_warnings`: generates warnings.\n* `prepare_message`: prepares a message for a specific suite.\n* `gen_summary_files`: generates summary files.\n* `SummaryStatDiffer`: a class for comparing summary statistics.\n* `RegressionDetector`: a class for detecting regressions.\n* `RegressionTracker`: a class for tracking regressions.\n* `DashboardUpdater`: a class for updating the dashboard.\n\nThe script uses several environment variables, including:\n\n* `TORCH_CUDA_ARCH_LIST`\n* `CUDA_HOME`\n* `USE_LLVM`\n\nThe script also uses several command-line arguments, including:\n\n* `--devices`: specifies the devices to use.\n* `--dtypes`: specifies the data types to use.\n* `--suites`: specifies the benchmark suites to use.\n* `--compilers`: specifies the compilers to use.\n* `--flag-compilers",
            "total_tokens": 500,
            "latency": 12.909613,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python script, \"runner.py\", is designed to benchmark PyTorch models by automating the execution of training or inference tasks across various configurations, compilers, and data types. It generates benchmark commands, executes them, collects and parses resulting performance data (such as speedup, accuracy, compilation latency, and memory usage), and can also visualize this data through graphs and CSV outputs. Additionally, it includes functionality for reporting results and regressions systematically through customizable automation and dashboard integration.",
            "total_tokens": 12147,
            "latency": 9.322693,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file is a script that serves as a wrapper over a benchmark infrastructure. It allows for generating commands for various benchmark suites, parsing the results, and generating csv/graphs. It supports functionalities such as running specific benchmark suites, visualizing already finished logs, printing commands, and more. Additionally, it includes features to detect regressions, update the performance dashboard on GitHub with graphs and comments, and perform various analyses on the benchmark results such as checking speedup, compilation latency, compression ratio, accuracy, etc.",
            "total_tokens": 12071,
            "latency": 2.324384,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"runner.py\" file can be summarized in three sentences as follows:\n\n1. This script serves as a wrapper over the benchmark infrastructure to generate commonly used commands, parse results, and generate CSV files and graphs for performance benchmarking of PyTorch compilers and backends.\n\n2. It provides options to run benchmarks for different suites (e.g., torchbench, huggingface, timm_models), devices (CPU/GPU), data types (float32/float16), and compilers (e.g., eager, inductor, nvfuser) for both inference and training scenarios.\n\n3. The script can generate commands, execute benchmarks, parse logs, visualize results, detect regressions, and update a performance dashboard with the collected data and analysis.",
            "total_tokens": 15992,
            "latency": 7.246361,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `runner.py` file is to serve as a comprehensive wrapper for managing and executing benchmarks, parsing their results, and generating visualizations and reports. It supports generating run commands, visualizing logs, and updating a performance dashboard with detailed summaries and regression detection.",
            "total_tokens": 14436,
            "latency": 1.731109,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `runner.py` file is a benchmarking and profiling tool designed to test and analyze the performance, accuracy, memory footprint, and compilation latency of various PyTorch models and compilers for either inference or training. It allows users to generate benchmarking commands, execute them, parse the resulting logs, and create visualizations and detailed performance reports, including warnings and regressions. The script also integrates with GitHub to facilitate performance dashboards and comparisons over time for ongoing development improvements.",
            "total_tokens": 12143,
            "latency": 2.525488,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to generate commands for running benchmark tests, parse the results, and create visualizations and summaries of the performance metrics. The script can be run in different modes, including generating commands, running the commands and parsing the results, or just visualizing the already finished logs. It also supports updating a dashboard with the results.",
            "total_tokens": 12044,
            "latency": 47.985251,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `runner.py` defines an elaborate benchmark runner script that facilitates the execution and management of performance benchmarks for different backend compilers of PyTorch. The script allows generating benchmark command lines, running those commands, and visualizing the results in terms of logs and graphical outputs. It includes functionality to filter down tests, keep track of performance metrics across multiple runs, and quantify and flag any regressions or anomalous behaviors in the performance data. It utilizes a user-specified configuration table to set up various running conditions for inference and training tasks across different compilers and supports configurations like batch size, data type, and device specifics.",
            "total_tokens": 12092,
            "latency": 4.832093,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to generate and run benchmarking commands for PyTorch models, parse the results, and create visualizations and summaries of the performance metrics. The script can be run in different modes, including generating commands, running benchmarks, and visualizing logs, and it supports various options for customizing the benchmarking process, such as selecting specific models, devices, and data types.",
            "total_tokens": 12057,
            "latency": 3.329082,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file runner.py is a comprehensive automation and analysis tool for benchmarking PyTorch model performance and accuracy across a variety of suites, devices, data types, and compiler backends. It generates command-line scripts to run detailed benchmarks, parses and aggregates performance/accuracy results, visualizes key metrics (e.g., speedup, memory, compilation latency), and produces both CSV summaries and annotated graphs. Furthermore, it can detect regressions, compare recent runs, package results for dashboards (including uploading images and posting to GitHub), and provides extensive functionality for summarizing, flagging, and tracking benchmark outcomes over time.",
            "total_tokens": 12172,
            "latency": 2.501993,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/benchmarks/dynamo/runner.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script acts as a wrapper around a PyTorch benchmarking infrastructure. It generates, runs, and parses benchmark commands for various compilers, model suites, devices, and data types for both training and inference modes. The script produces detailed performance and accuracy reports, including executive summaries, warnings, and historical comparisons, with options to visualize results and update a performance dashboard.",
            "total_tokens": 15394,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.500387
        },
        "gemini-2.5": {
            "file_name": "/benchmarks/dynamo/runner.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script serves as a wrapper for running and analyzing PyTorch benchmarks across various configurations (suites, devices, dtypes, compilers). It can generate benchmark execution commands, run them, parse the resulting log files, and visualize performance metrics like speedup and compilation time as tables and graphs. The script also includes functionality to update a performance dashboard, track metrics over time, and detect regressions compared to previous runs.",
            "total_tokens": 15584,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.317932
        }
    },
    "/benchmarks/transformer/sdpa.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"sdpa.py\" file is to benchmark the performance of scaled dot-product attention (SDPA) operations using various configurations, measure the execution time and TFLOPS for both forward and backward passes, and then output the results in a tabular format and save them to a CSV file. It utilizes PyTorch for tensor operations and benchmarking, and the script is designed to run a series of experiments with different parameters to evaluate the performance of SDPA under various conditions.",
            "total_tokens": 2696,
            "latency": 1.18355,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"sdpa.py\" is to benchmark the performance of scaled dot-product attention in PyTorch. It uses the `benchmark_torch_function_in_microseconds` function to measure the execution time of the attention function and the `calculate_tflops` function to calculate the TFLOPS for both forward and backward passes. The file also includes functions to generate experiment configurations, get input tensors, and print and write the results to a CSV file. The benchmarking is performed for different batch sizes, sequence lengths, embedding dimensions, and backends.",
            "total_tokens": 127,
            "latency": 3.098488,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python script benchmarks the performance of the scaled dot product attention function in PyTorch using various configurations (such as batch size, sequence lengths, embedding dimensions, causality settings, data types, and backends). It measures the forward and backward execution times, calculates throughput in TFLOPS, and outputs the results into a readable table and CSV file. It includes functionality for run-time measurement, throughput calculation considering sparsity, and result aggregation.",
            "total_tokens": 2308,
            "latency": 4.879484,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file \"sdpa.py\" defines functions and data structures for benchmarking scaled dot product attention with PyTorch. It includes functions to calculate TFLOPS, run experiments with different configurations, and output the results in a readable format. The main functionality of the file is to generate various experiment configurations, run the experiments, display the results using tabulate, and write the results to a CSV file for further analysis.",
            "total_tokens": 2276,
            "latency": 1.297918,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "This Python file implements a benchmarking system for scaled dot product attention (SDPA) operations in PyTorch. It defines functions to measure the performance of SDPA in terms of execution time and TFLOPS for both forward and backward passes, considering various configurations such as batch size, sequence length, and data types. The script generates multiple experiment configurations, runs the benchmarks, and outputs the results in both tabular format and CSV files for further analysis.",
            "total_tokens": 2906,
            "latency": 3.299277,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `sdpa.py` file is designed to benchmark the performance of scaled dot-product attention (SDPA) in PyTorch, measuring both forward and backward pass times and calculating TFLOPS. It generates various experiment configurations, runs each experiment, and outputs the results in a formatted table and a CSV file.",
            "total_tokens": 2660,
            "latency": 1.045892,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `sdpa.py` file benchmarks the performance of scaled dot-product attention (SDPA) operations in PyTorch across various configurations, including batch size, sequence length, embedding dimensions, and more. It calculates execution times, throughput in TFLOPS, and runs experiments for both forward and backward passes with optional sparsity. Results are displayed in tabular format and optionally written to a CSV file for analysis.",
            "total_tokens": 2301,
            "latency": 1.584655,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to benchmark the performance of the scaled dot product attention (SDPA) kernel in PyTorch, specifically on CUDA devices. It generates various experiment configurations, runs each experiment, and measures the execution time and TFLOPS (tera floating-point operations per second) for both forward and backward passes. The results are then printed in a tabular format and written to a CSV file.",
            "total_tokens": 2280,
            "latency": 3.75565,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python script defined in 'sdpa.py' is centered around benchmarking scaled dot product attention (SDPA) computational functions, particularly emphasizing GPU execution and measuring performance in terms of throughput (TFLOPS). It supports different configurations for experiments, such as variations on batch size, sequence length, embedding dimensions, and backend processing (e.g., CUDA or inductor). Various functions to aid the process include gathering input data, running single experiments, calculating TFLOPS, and outputting results either as formatted tables or as CSV files, making it a comprehensive toolkit for performance evaluation of attention mechanisms in deep learning frameworks.",
            "total_tokens": 2313,
            "latency": 4.054575,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to benchmark the performance of the scaled dot product attention (SDPA) mechanism in PyTorch, specifically measuring the execution time and TFLOPS for forward and backward passes under various experiment configurations. The script generates a range of experiment configurations with different batch sizes, sequence lengths, and data types, and then runs each experiment to collect results, which are printed in a table and written to a CSV file. The benchmarking process utilizes PyTorch's built-in benchmarking tools and CUDA functionality to measure execution times and calculate TFLOPS.",
            "total_tokens": 2314,
            "latency": 1.951095,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The main functionality of sdpa.py is to benchmark the performance (execution time and TFLOPS) of PyTorch's scaled dot product attention (SDPA) operations across various input configurations, using CUDA on GPU. It systematically generates a range of input parameter settings, runs and times both forward and backward SDPA passes, calculates associated TFLOPS, and then prints and saves the results in a CSV file. The code supports consistent, automated performance evaluation of different SDPA backends and settings for large-scale transformer-like workloads.",
            "total_tokens": 2324,
            "latency": 1.59672,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/benchmarks/transformer/sdpa.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script is designed to benchmark the performance of PyTorch's `scaled_dot_product_attention` function. It runs experiments across various configurations, including different batch sizes, sequence lengths, data types, and causality settings. For each configuration, it measures the forward and backward pass execution times on CUDA, calculates TFLOPS, and reports the results in a table and saves them to a CSV file.",
            "total_tokens": 3341,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.236465
        },
        "gemini-2.5": {
            "file_name": "/benchmarks/transformer/sdpa.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script benchmarks the performance of PyTorch's `scaled_dot_product_attention` function under various configurations (like batch size, sequence length, dtype, causality). It measures the forward and backward pass execution times in microseconds and calculates the corresponding TFLOPS for each configuration using specified or default SDPA backends. Finally, the script prints the collected benchmark results in a formatted table and saves them to a timestamped CSV file.",
            "total_tokens": 3239,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.661991
        }
    },
    "/tools/code_coverage/package/oss/init.py": {
        "nova-lite-v1": {
            "file_summary": "The `init.py` file is designed to initialize and configure a testing environment for both C++ and Python tests. It includes functionalities to parse command-line arguments, create necessary directories, clean up previous test results, and compile a list of tests to run based on user input or default settings. The script also handles specific configurations for different compilers and ensures the environment is properly set up before running the tests.",
            "total_tokens": 1662,
            "latency": 0.702622,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to initialize the testing environment for PyTorch. It creates necessary folders, adds command-line arguments, and retrieves the list of tests to run. The file also handles the case when no tests are found and provides information about the testing environment, such as the PyTorch folder, test binaries folder, and compiler type. Additionally, it removes any leftover files from previous runs and exports the necessary environment variables for GCC.",
            "total_tokens": 95,
            "latency": 2.458945,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.5": {
            "file_summary": "This Python script initializes and prepares a testing environment for PyTorch by parsing command-line arguments, determining tests (C++ and Python) to run, handling blocked tests based on the compiler type (GCC or Clang), and setting up necessary folders and files. It supports selective test execution through specified options, cleans previous test data if required, and logs essential initialization information. It also manages specific compiler behaviors and restricted test lists to avoid issues with intermediate file processing.",
            "total_tokens": 1384,
            "latency": 4.883882,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "This Python file initializes test settings and options for running specific tests in a given directory. It defines functions for setting up test environment, parsing command-line arguments, selecting and organizing test cases, and displaying initialization information. Additionally, it includes logic to handle different test types (CPP and PY), compiler types (GCC and clang), and specific test exclusions based on compiler and test file names.",
            "total_tokens": 1352,
            "latency": 1.194149,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of this \"init.py\" file is to initialize and set up the environment for running tests in a Python project, likely related to PyTorch. It defines functions for parsing command-line arguments, creating necessary folders, generating test lists for C++ and Python tests, and handling compiler-specific configurations. The file also includes utility functions for cleaning up, detecting compiler types, and managing test execution options.",
            "total_tokens": 1792,
            "latency": 3.852016,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of this Python file is to initialize and configure the environment for running tests, including parsing command-line arguments, setting up necessary folders, and retrieving the list of tests to be executed based on specified criteria.",
            "total_tokens": 1623,
            "latency": 0.780838,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4o": {
            "file_summary": "The provided Python file, likely part of a larger framework or testing suite, handles initialization for running tests, including creating necessary folders, handling command-line arguments, and preparing test lists based on test types (e.g., Python or C++). It also provides utility functions for cleaning up previous logs and temporary files, managing blocked tests, and detecting compiler types (like GCC or Clang). Furthermore, it prints diagnostic or configuration information related to test and compiler settings.",
            "total_tokens": 1384,
            "latency": 1.142612,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to initialize the environment and settings for running tests, including creating necessary folders, parsing command-line arguments, and determining the list of tests to run based on the specified options. It also sets up the logging and removes any existing log files from previous runs. The file appears to be part of a larger testing framework for PyTorch, with support for both C++ and Python tests.",
            "total_tokens": 1365,
            "latency": 3.414356,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4": {
            "file_summary": "The `init.py` file is primarily focused on initializing and configuring the testing environment for a software project, presumably PyTorch, based on configurations like test type, specific tests to run, and compiler settings. It utilizes Python scripts and C++ binary test lists, handles the argument parsing setup for running particular tests, and cleans up log files from previous runs. Additionally, the script adjusts configurations dynamically based on the compiler type and manages directories for storing JSON and log outputs, ensuring that the test environment is prepared correctly before execution.",
            "total_tokens": 1380,
            "latency": 4.780678,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of this Python file, `init.py`, is to initialize the environment and parse command-line arguments for running tests, including creating necessary folders, cleaning up previous test data, and determining which tests to run based on user input. The file also sets up logging and detects the compiler type being used, such as GCC or Clang. The initialization process returns a tuple containing options, a list of tests to run, and a list of interested folders for reporting purposes.",
            "total_tokens": 1377,
            "latency": 1.676917,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.1": {
            "file_summary": "This file initializes and configures a testing environment for a project that involves both C++ and Python tests, with specialized handling depending on the compiler type (GCC/Clang) and test type. It sets up command-line argument parsing to select which tests to run, manages creation and cleanup of necessary directories and files, and builds a list of tests to run based on user inputs and defaults, excluding known problematic tests where necessary. The module serves as the entry point for managing test selection, cleanup, and logging for the overall testing workflow.",
            "total_tokens": 1400,
            "latency": 1.661608,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/code_coverage/package/oss/init.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file handles the initialization process for a testing framework, likely related to PyTorch, by parsing command-line arguments and setting up necessary directories. It identifies the specific C++ and Python tests to be executed based on user input and compiler type, including handling compiler-specific test exclusions. The script also performs initial cleanup tasks and prints relevant setup information before the tests are run.",
            "total_tokens": 2253,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.879911
        },
        "gemini-2.5": {
            "file_name": "/tools/code_coverage/package/oss/init.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script initializes a testing environment by parsing command-line arguments to configure options like which tests to run and whether to clean up previous results. It identifies C++ and Python test files within specified directories, potentially filtering them based on user input or compiler type (GCC/Clang). Finally, it prepares the environment by creating necessary folders, cleaning old artifacts, logging setup information, and returning the configuration options, the list of tests to execute, and folders of interest.",
            "total_tokens": 2029,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.181991
        }
    },
    "/functorch/notebooks/_src/plot_ensembling.py": {
        "nova-lite-v1": {
            "file_summary": "The file \"plot_ensembling.py\" demonstrates how to implement model ensembling using vectorization in PyTorch. It explains the concept of model ensembling, which combines predictions from multiple models, and shows how to use the `vmap` function to vectorize this process, thereby eliminating for-loops and speeding up computations. The example uses an ensemble of simple CNNs to illustrate the technique.",
            "total_tokens": 1594,
            "latency": 1.007763,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"plot_ensembling.py\" is to demonstrate how to use vmap for vectorizing model ensembling in PyTorch. The file defines a simple CNN model and generates dummy data. It then creates an ensemble of 10 models and uses vmap to speed up the process of generating predictions for different minibatches of data. The file also includes a note about the limitations of using vmap for vectorizing functions with side effects.",
            "total_tokens": 100,
            "latency": 2.430006,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python script demonstrates how to efficiently perform model ensembling for multiple neural networks using PyTorch and vectorization techniques provided by the `vmap` function from Functorch. It defines an ensemble of CNN models, combines their parameters and states into a unified batch representation, then vectorizes computations to simultaneously perform inference either with different data batches per model or a common batch. The main goal is eliminating traditional for-loops, thus speeding up computations through efficient tensor operations.",
            "total_tokens": 1386,
            "latency": 6.552079,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the 'plot_ensembling.py' file is to demonstrate how to perform model ensembling using vectorization techniques in PyTorch, specifically utilizing the 'vmap' function transform to map functions across dimensions of input tensors. The file includes the definition of a simple CNN model, generation of dummy data, creation and preparation of multiple models for ensembling, showcasing different options for generating predictions, and using 'vmap' to speed up the for-loop process by combining model states and parameters for efficient prediction computation.",
            "total_tokens": 1392,
            "latency": 1.127568,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "This Python file demonstrates the use of model ensembling and how to vectorize it using PyTorch's vmap function. It showcases two approaches for generating predictions with multiple models: using different minibatches for each model and using the same minibatch across all models. The file also illustrates how to use vmap to speed up the ensemble process by eliminating for-loops and leveraging vectorization.",
            "total_tokens": 1590,
            "latency": 2.409433,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `plot_ensembling.py` file is to demonstrate how to use PyTorch's `vmap` function to vectorize and speed up the process of model ensembling, specifically by combining the predictions from multiple simple CNNs. It showcases two options for generating predictions: using different minibatches for each model and using the same minibatch for all models.",
            "total_tokens": 1520,
            "latency": 1.115487,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The file `plot_ensembling.py` demonstrates how to use the PyTorch `vmap` function to vectorize model ensembling and eliminate for-loops for combining predictions across multiple models with the same architecture. It constructs a simple CNN, generates synthetic data, presents two possible approaches for obtaining predictions (using separate or shared minibatches), and showcases the efficiency and flexibility of `vmap` in handling stacked model parameters and buffers. Additionally, it highlights the limitations of `vmap` in processing non-pure functions.",
            "total_tokens": 1399,
            "latency": 1.747806,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to demonstrate how to use the `vmap` function from the `functorch` library to vectorize model ensembling, specifically for combining the predictions of multiple CNN models. The file shows how to prepare the models for use with `vmap` by stacking their parameters and buffers, and then uses `vmap` to speed up the process of generating predictions from the ensemble. The file also compares the results of using `vmap` with the traditional for-loop approach.",
            "total_tokens": 1396,
            "latency": 4.071495,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python script 'plot_ensembling.py' demonstrates how to perform model ensembling to combine predictions from multiple models, specifically utilizing the vectorization capabilities of `vmap` to expedite this process. The script includes a simple CNN model architecture and explores different approaches to feeding data batches for inference by considering both different and identical minibatches for each model. Additionally, it utilizes the `functorch` library to prepare and optimize the models for efficient ensembling through parameter stacking and vectorized computation using `vmap`.",
            "total_tokens": 1390,
            "latency": 6.66368,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `plot_ensembling.py`, is to demonstrate model ensembling using PyTorch's `vmap` function, which vectorizes the predictions from multiple models. The script defines a simple CNN model and generates dummy data to simulate an MNIST dataset, then uses `vmap` to combine the predictions from an ensemble of 10 models, showcasing two options: using different minibatches for each model and using the same minibatch for all models. The script verifies the results by comparing the predictions obtained with and without `vmap` for both options.",
            "total_tokens": 1413,
            "latency": 7.025408,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "This file demonstrates how to efficiently ensemble multiple neural network models with the same architecture by leveraging PyTorch's functorch.vmap for vectorization. It illustrates how to stack model parameters across an ensemble, prepare batched data, and use vmap to simultaneously obtain predictions from all models, contrasting this approach with the traditional for-loop method. The code uses a simple CNN and dummy data to show two ensembling scenarios: each model receives a different minibatch or all models share the same minibatch.",
            "total_tokens": 1392,
            "latency": 1.4932,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/functorch/notebooks/_src/plot_ensembling.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script demonstrates model ensembling in PyTorch using multiple instances of a simple CNN. It primarily showcases how to use `functorch.vmap` to vectorize the process of running the ensemble on data, contrasting it with a traditional loop-based approach. The example covers applying `vmap` both when each model in the ensemble processes a different data batch and when all models process the same batch.",
            "total_tokens": 1946,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 5.484953
        },
        "gemini-2.5-flash": {
            "file_name": "/functorch/notebooks/_src/plot_ensembling.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script demonstrates model ensembling using PyTorch. It shows traditional methods for combining predictions from multiple models, either with different or the same input minibatches. The core functionality is illustrating how to vectorize these ensembling operations efficiently using `functorch.vmap` and `functorch.combine_state_for_ensemble`.",
            "total_tokens": 1647,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 1.467103
        }
    },
    "/torch/_functorch/_activation_checkpointing/knapsack.py": {
        "nova-lite-v1": {
            "file_summary": "The `knapsack.py` file implements three different algorithms for solving the knapsack problem: a greedy approach, an integer linear programming (ILP) approach using `scipy`, and a dynamic programming (DP) approach using `torch`. Each function takes a list of memory usages, runtimes, and a maximum memory budget, and returns the total runtime, a list of items to save, and a list of items to allow recomputing. The greedy approach sorts items by their runtime-to-memory ratio and selects items until the memory budget is reached. The ILP approach uses `scipy` to solve the problem as an optimization problem. The DP approach quantizes the memory usages and runtimes and uses a DP table to find the optimal solution.",
            "total_tokens": 1467,
            "latency": 1.473946,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The Python file \"knapsack.py\" contains three functions: \"greedy_knapsack\", \"ilp_knapsack\", and \"dp_knapsack\". These functions are designed to solve the 0-1 Knapsack problem, which involves selecting a subset of items with the maximum total value while not exceeding a given weight limit. The \"greedy_knapsack\" function uses a greedy approach, the \"ilp_knapsack\" function employs an integer linear programming (ILP) method, and the \"dp_knapsack\" function implements a dynamic programming (DP) solution. The functions take in lists of memory and runtime values for each item, along with a maximum memory constraint, and return the maximum achievable runtime, the indices of the items to save, and the indices of the items to allow recomputing.",
            "total_tokens": 176,
            "latency": 3.830104,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements three different knapsack algorithm variants\u2014greedy, integer linear programming (ILP), and dynamic programming (DP)\u2014to optimally select a subset of items based on their memory usage and runtimes, such that the total runtime is maximized given a memory usage constraint. Each function returns the selected items to save in memory and those that need recomputing, along with the achieved maximum runtime. While the greedy method offers fast but approximate solutions, the ILP and DP methods produce optimal solutions at the cost of higher computational complexity.",
            "total_tokens": 1218,
            "latency": 5.049594,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided Python file defines three knapsack algorithms: greedy_knapsack, ilp_knapsack, and dp_knapsack. The greedy_knapsack function implements a greedy approach to solving the knapsack problem based on runtime to memory ratio. The ilp_knapsack function uses Integer Linear Programming (ILP) with scipy to solve the problem, and the dp_knapsack function implements a dynamic programming solution for the knapsack problem.",
            "total_tokens": 1207,
            "latency": 1.401182,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The file \"knapsack.py\" implements three different algorithms (greedy, integer linear programming, and dynamic programming) to solve the knapsack problem, specifically for optimizing memory usage and runtime in a computational context. Each function takes memory requirements, runtimes, and a maximum memory constraint as inputs, and returns the total runtime, items to save, and items to allow recomputing, providing different trade-offs between solution quality and computational complexity.",
            "total_tokens": 1489,
            "latency": 2.990381,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `knapsack.py` file implements three different algorithms to solve the knapsack problem with given memory constraints: a greedy approach, an integer linear programming (ILP) approach, and a dynamic programming (DP) approach. Each function returns the maximum achievable runtime and lists of items to save and allow recomputing.",
            "total_tokens": 1376,
            "latency": 0.99918,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `knapsack.py` file implements three different algorithms \u2014 greedy, integer linear programming (ILP), and dynamic programming (DP) \u2014 to solve a variation of the 0-1 Knapsack problem. Each function determines an optimal selection of items based on the runtime-to-memory trade-off, ensuring that the total memory used does not exceed a given maximum memory limit. The algorithms output the maximum achievable runtime, the indices of items chosen to save (keep in memory), and the indices of items marked for recomputation.",
            "total_tokens": 1211,
            "latency": 1.909762,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to provide three different algorithms (Greedy, Integer Linear Programming (ILP), and Dynamic Programming (DP)) to solve the 0-1 Knapsack problem, which is a classic problem in combinatorial optimization. The goal is to select a subset of items with maximum total runtime that can fit within a given memory constraint. The algorithms return the maximum runtime, the items to save, and the items to allow recomputing.",
            "total_tokens": 1209,
            "latency": 3.843599,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The file `knapsack.py` implements three different methods (`greedy_knapsack`, `ilp_knapsack`, and `dp_knapsack`) to solve the knapsack problem, specifically tailored towards optimizing the selection of computational tasks based on memory usage and runtime. The greedy approach sorts items by the ratio of runtime to memory and selects items until the memory limit is reached, ILP (Integer Linear Programming) approach uses SCIPY's MILP solver with memory constraints, and the DP (Dynamic Programming) approach utilizes a quantized pseudopolynomial DP algorithm to handle floating-point memory requirements. Each method returns the total runtime, list of tasks to save (execute), and list to allow recomputing (not execute) within a given memory constraint.",
            "total_tokens": 1265,
            "latency": 6.316073,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `knapsack.py`, is to solve the 0/1 knapsack problem using three different algorithms: a greedy approach, integer linear programming (ILP), and dynamic programming (DP). These algorithms aim to maximize the total runtime of items that can be saved within a given memory constraint. The file provides three functions, `greedy_knapsack`, `ilp_knapsack`, and `dp_knapsack`, each implementing one of these approaches to solve the problem.",
            "total_tokens": 1224,
            "latency": 1.56838,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "This file implements three approaches (greedy, integer linear programming, and dynamic programming) to solve a 0-1 knapsack problem where each item has an associated memory cost and runtime value. Each function selects a subset of items to maximize total runtime without exceeding a given memory budget, and returns both the achieved runtime and the indices of items chosen to save versus those to recompute. The code uses Torch for the dynamic programming implementation and optionally Scipy for the ILP method.",
            "total_tokens": 1203,
            "latency": 1.490205,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_functorch/_activation_checkpointing/knapsack.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides three different algorithms (greedy, Integer Linear Programming, and dynamic programming) to solve a 0-1 knapsack problem. The problem involves selecting items, each with a memory cost and an associated runtime benefit, to maximize the total runtime saved while staying within a maximum memory budget. The functions return the maximum achievable runtime and lists indicating which items should be saved and which should be recomputed.",
            "total_tokens": 1920,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.727352
        },
        "gemini-2.5": {
            "file_name": "/torch/_functorch/_activation_checkpointing/knapsack.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides three different algorithms (greedy, integer linear programming, and dynamic programming) for solving the 0/1 knapsack problem. Given lists representing memory costs and runtimes for a set of items, along with a maximum memory budget, the functions aim to select a subset of items that maximizes the total runtime without exceeding the memory limit. Each function returns the maximum achievable runtime, the indices of the selected items (\"saved\"), and the indices of the items not selected (\"recomputed\").",
            "total_tokens": 2017,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.274803
        }
    },
    "/torch/_functorch/pytree_hacks.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"pytree_hacks.py\" file is to provide deprecated utilities `tree_map_` and `treespec_pprint` from `torch.utils._pytree`, with a warning to users to switch to the new module. This file is intended to be removed once the migration of the pytree utility is complete.",
            "total_tokens": 352,
            "latency": 2.642112,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"pytree_hacks.py\" is to provide two functions, \"tree_map_\" and \"treespec_pprint\", which are imported from the \"torch.utils._pytree\" module. The file also includes a deprecation warning message, indicating that the \"torch._functorch.pytree_hacks\" module is deprecated and will be removed in a future release. Users are advised to use the \"torch.utils._pytree\" module instead. The file is licensed under the BSD-style license and is copyrighted by Facebook, Inc. and its affiliates.",
            "total_tokens": 136,
            "latency": 3.000304,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python file acts as a compatibility layer that imports two pytree utility functions\u2014`tree_map_` and `treespec_pprint`\u2014from `torch.utils._pytree`. It raises a deprecation warning message to inform the user that importing from `torch._functorch.pytree_hacks` is deprecated. The main functionality thus ensures backward compatibility, indicating ongoing migration to the new pytree utilities in `torch.utils._pytree`.",
            "total_tokens": 358,
            "latency": 3.449756,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the provided Python file \"pytree_hacks.py\" is to issue a deprecation warning regarding the use of \"`torch._functorch.pytree_hacks`\" and recommend the use of \"`torch.utils._pytree`\" instead. The file also imports and exposes two functions \"`tree_map_`\" and \"`treespec_pprint`\" from \"`torch.utils._pytree`\" as part of the public interface. Additionally, there is a placeholder comment to remove the file once the migration of the pytree utility is completed.",
            "total_tokens": 372,
            "latency": 1.38617,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The file \"pytree_hacks.py\" serves as a temporary bridge for the migration of pytree utility functions. It imports and re-exports two functions (tree_map_ and treespec_pprint) from torch.utils._pytree, while also issuing a deprecation warning to users, encouraging them to use the new module directly in the future.",
            "total_tokens": 380,
            "latency": 4.036235,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The file `pytree_hacks.py` serves as a transitional module that imports specific functions from `torch.utils._pytree` and issues a deprecation warning, signaling users to migrate to the new module before eventual removal.",
            "total_tokens": 327,
            "latency": 0.991994,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "This Python file acts as a temporary compatibility layer for `torch.utils._pytree`, re-exporting its `tree_map_` and `treespec_pprint` utilities while issuing a deprecation warning. It notifies users that the file is slated for removal in favor of directly using `torch.utils._pytree`. The functionality is specific to aiding migration during the transition phase for pytree utilities.",
            "total_tokens": 347,
            "latency": 1.357438,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide a deprecated interface to pytree utility functions, specifically `tree_map_` and `treespec_pprint`, while warning users to migrate to the new location `torch.utils._pytree` as this file will be removed in a future release.",
            "total_tokens": 323,
            "latency": 2.406513,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file \"pytree_hacks.py\" primarily serves as a deprecation warning mechanism. It imports specific functions from \"`torch.utils._pytree`\" such as `tree_map_` and `tree_spec_pprint`, which seems to be utilities for handling complex data structures in PyTorch. The file advises users that \"pytree_hacks.py\" is deprecated and instructs to use \"`torch.utils._pytree`\" directly instead, signaling a migration in utility usage within PyTorch's ecosystem.",
            "total_tokens": 363,
            "latency": 4.496783,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `pytree_hacks.py` file is to provide a deprecated interface to pytree utilities, specifically `tree_map_` and `treespec_pprint`, which are now recommended to be accessed through `torch.utils._pytree`. The file raises a deprecation warning, indicating that it will be removed in a future release. The file's content is primarily a placeholder for backwards compatibility, with a TODO comment indicating that it should be removed once the migration of the pytree utility is complete.",
            "total_tokens": 370,
            "latency": 2.721433,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file pytree_hacks.py serves as a transitional compatibility layer that re-exports the tree_map_ and treespec_pprint utilities from torch.utils._pytree. It issues a deprecation warning to inform users that this module will be removed in the future and advises using torch.utils._pytree directly. The file's main functionality is to provide temporary access to these pytree utilities during a migration period.",
            "total_tokens": 349,
            "latency": 1.359955,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_functorch/pytree_hacks.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This file re-exports the `tree_map_` and `treespec_pprint` functions, which are imported from `torch.utils._pytree`. It serves as a temporary file during a migration process. The file itself is deprecated, and users are warned to use `torch.utils._pytree` directly instead.",
            "total_tokens": 708,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.702606
        },
        "gemini-2.5": {
            "file_name": "/torch/_functorch/pytree_hacks.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file re-exports the `tree_map_` and `treespec_pprint` functions from PyTorch's internal `_pytree` utility module. Its primary function is to serve as a temporary alias while issuing a `DeprecationWarning` upon import. The warning advises users to switch to importing these functions directly from `torch.utils._pytree` as this compatibility layer will be removed in the future.",
            "total_tokens": 985,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.358605
        }
    },
    "/torch/distributed/elastic/agent/server/health_check_server.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"health_check_server.py\" file is to provide an interface for a health check monitoring server, which can be extended to start a TCP/HTTP server on a specified port. The `HealthCheckServer` class allows for the configuration of a callback function to track the last progress time of an agent, a port number, and a timeout value to determine if the agent is alive or dead. However, the provided implementation does not actually start any health check server, as indicated by the warning message in the `start` method.",
            "total_tokens": 631,
            "latency": 1.198627,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file \"health_check_server.py\" contains a class \"HealthCheckServer\" that serves as an interface for health check monitoring servers. This class has methods to start and stop the server, but the start method is currently unsupported in Pytorch. The file also includes a function \"create_healthcheck_server\" that creates an instance of the \"HealthCheckServer\" class. The class and function are part of a larger project licensed under the BSD-style license.",
            "total_tokens": 99,
            "latency": 2.447957,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines a class called `HealthCheckServer`, serving as an interface intended for health check monitoring of agents, primarily used within PyTorch's distributed elastic training environment. The class mainly provides methods to initiate and stop a health check server, although the actual server start functionality is currently not supported and simply logs a warning stating this. The file also includes a factory function `create_healthcheck_server` which instantiates and returns an instance of `HealthCheckServer`.",
            "total_tokens": 560,
            "latency": 3.884604,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the \"health_check_server.py\" file is to define a class `HealthCheckServer` and a function `create_healthcheck_server` for creating a health check monitor server. The `HealthCheckServer` class includes methods to start and stop the health check server, although the start method does not actually start any server. The `create_healthcheck_server` function is used to instantiate a `HealthCheckServer` object with the provided parameters: alive_callback, port, and timeout.",
            "total_tokens": 567,
            "latency": 1.153836,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"health_check_server.py\" file is to define a HealthCheckServer class that serves as an interface for health check monitoring. This class is designed to be extended for implementing actual health check servers, but in its current form, it doesn't start any real server and only provides a placeholder implementation. The file also includes a function to create instances of the HealthCheckServer class.",
            "total_tokens": 629,
            "latency": 2.977614,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `health_check_server.py` file defines an interface for a health check monitoring server, specifically the `HealthCheckServer` class, which is intended to be extended for starting a TCP/HTTP server on a specified port. It includes methods to start and stop the server, though the current implementation does not actually start any server and logs a warning instead. The file also provides a factory function `create_healthcheck_server` to instantiate the `HealthCheckServer` class.",
            "total_tokens": 618,
            "latency": 1.283182,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `health_check_server.py` file defines a `HealthCheckServer` class that serves as an interface for a health check monitoring server, with methods to start and stop the server, though the actual server functionality is not implemented and only logs warnings. It includes configurable parameters such as an `alive_callback`, `port`, and `timeout` to monitor agent health. Additionally, a helper function, `create_healthcheck_server`, is provided to instantiate the server object.",
            "total_tokens": 559,
            "latency": 1.406171,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a `HealthCheckServer` class that provides an interface for health check monitoring, allowing users to create a server that can be extended to start a TCP/HTTP server on a specified port. The class has methods to start and stop the health check server, although the start method is currently unsupported and does not start any server. The file also includes a function to create a `HealthCheckServer` object.",
            "total_tokens": 564,
            "latency": 3.924399,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The file defines a class `HealthCheckServer` and a function `create_healthcheck_server` for creating a monitoring server in a Python environment, specifically designed for checking the health of a server running PyTorch distributed applications. The `HealthCheckServer` class takes callbacks, port, and timeout parameters but lacks the implementation to effectively start or stop a real server \u2014 it only logs placeholder messages. Essentially, the server acts as a stub for a health checking mechanism potentially for development or testing purposes.",
            "total_tokens": 566,
            "latency": 2.424112,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `health_check_server.py` file is to define a `HealthCheckServer` class that provides an interface for health check monitoring, allowing for the creation of a server that can be extended to start a TCP/HTTP server on a specified port. The class has methods to start and stop the health check server, although the `start` method is currently unsupported and does not start any server. The file also includes a `create_healthcheck_server` function to create an instance of the `HealthCheckServer` class.",
            "total_tokens": 582,
            "latency": 2.158533,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines a basic HealthCheckServer class meant as an interface for a health monitoring server, providing methods to start and stop such a server, though actual start/stop functionality is not implemented for PyTorch and only logs warnings or info. The server is initialized with a callback, port, and timeout for potential health checks. Additionally, a factory function create_healthcheck_server is provided to instantiate the HealthCheckServer with the specified parameters.",
            "total_tokens": 553,
            "latency": 1.386944,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/elastic/agent/server/health_check_server.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `HealthCheckServer` class intended as an interface for health check monitoring, along with a factory function `create_healthcheck_server`. The current implementation of the `HealthCheckServer` is a placeholder (\"noop\") that does not actually start any server or perform health checks. It serves as a base or a non-functional stub for potential future extensions.",
            "total_tokens": 1054,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.313971
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/elastic/agent/server/health_check_server.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `HealthCheckServer` class intended as an interface for a health monitoring server, using a callback to check agent liveness based on a specified timeout. However, the provided implementation is a no-op, as the `start` method explicitly states it doesn't actually initiate any server process. A factory function `create_healthcheck_server` is also included to instantiate this placeholder server class.",
            "total_tokens": 1104,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.231409
        }
    },
    "/torch/distributed/algorithms/model_averaging/utils.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"utils.py\" file is to provide utility functions for averaging model parameters in a distributed training setup using PyTorch. The file includes three key functions: `average_parameters`, `get_params_to_average`, and `average_parameters_or_parameter_groups`. These functions facilitate the averaging of parameters across different processes in a distributed environment, ensuring efficient and synchronized updates during training.",
            "total_tokens": 977,
            "latency": 0.935674,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The Python file \"utils.py\" contains three functions:\n\n1. \"average_parameters\" - This function averages the given parameters across all processes in a specified process group. It first checks if the current process is part of the process group. If it is, it creates two iterators from the input parameters, flattens the parameters into a contiguous buffer, divides the buffer by the number of processes, and performs an all-reduce operation to average the parameters across all processes. Finally, it updates the original parameters with the averaged values.\n\n2. \"get_params_to_average\" - This function filters out the parameters that do not contain any gradients from the input parameters. It supports two types of input: a list of model parameters or a list of parameter groups from an optimizer. It returns a list of filtered parameters.\n\n3. \"average_parameters_or_parameter_groups\" - This function is a wrapper function that calls \"average_parameters\" with the filtered parameters obtained from \"get_params_to_average\".\n\nThe file also contains some import statements and type hints for the functions. It is designed to work with PyTorch and distributed computing.",
            "total_tokens": 255,
            "latency": 5.359052,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements utility functions to average torch parameters (weights of a neural network) across multiple distributed processes, typically for distributed training. It handles efficient all-reduce operations by flattening and unflattening model parameters and supports inputs both as raw parameters and optimizer parameter groups. It includes logic to filter out parameters that do not have gradients, and properly synchronizes data across the distributed process group.",
            "total_tokens": 843,
            "latency": 5.587718,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `utils.py` file is to provide functions for averaging parameters of a model or parameter groups of an optimizer using distributed training in PyTorch. The file includes functions for averaging parameters, getting parameters to average by filtering out those without gradients, and a high-level function that effectively averages parameters or parameter groups utilizing the previously defined functions. The file uses torch.distributed for communication and synchronization during the parameter averaging process.",
            "total_tokens": 846,
            "latency": 1.241477,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"utils.py\" file is to provide utility functions for averaging parameters in distributed machine learning environments. It includes functions to average parameters across multiple processes, filter parameters that need averaging, and handle both model parameters and optimizer parameter groups. The file is designed to work with PyTorch's distributed computing capabilities and supports efficient parameter averaging using allreduce operations.",
            "total_tokens": 1048,
            "latency": 2.473735,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `utils.py` file provides utility functions for averaging model parameters in a distributed PyTorch environment. It includes functions to average parameters across a process group, filter parameters that require gradients, and handle both model parameters and optimizer parameter groups. The main functionality revolves around facilitating parameter synchronization in distributed training setups.",
            "total_tokens": 954,
            "latency": 0.92463,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "This Python file, `utils.py`, primarily provides functionality to average neural network parameters across multiple processes in a distributed training setup using PyTorch's distributed utilities. It includes methods to filter (retrieve) parameters with gradients for averaging, flatten them into a contiguous buffer for efficient allreduce operations, and restore their original shapes and types after averaging. The file ensures compatibility with PyTorch's distributed group settings and handles both model parameters and optimizer parameter groups.",
            "total_tokens": 851,
            "latency": 1.703519,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to provide utility functions for averaging model parameters or parameter groups in a distributed training setting using PyTorch. The functions `average_parameters` and `average_parameters_or_parameter_groups` allow for the averaging of parameters across different processes in a distributed group, while `get_params_to_average` filters out parameters that do not contain any gradients.",
            "total_tokens": 839,
            "latency": 3.147915,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The file `utils.py` primarily deals with operations related to averaging parameters in PyTorch, tailored for distributed computing environments. It has functions to average parameters directly, fetch parameters which need averaging (while filtering out those without gradients), and handle averaging across different data types efficiently to accommodate data parallelism during model training. Additionally, it ensures synchronization and reduction of parameters across different processes in a group to maintain consistency and accurate averages in a distributed setup.",
            "total_tokens": 848,
            "latency": 2.832739,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `utils.py`, is to provide utility functions for averaging model parameters in a distributed training setting using PyTorch. The key functions include `average_parameters`, which averages a list of parameters across a process group, and `get_params_to_average`, which filters out parameters without gradients. These functions can be used to average model parameters or parameter groups of an optimizer in a distributed training environment.",
            "total_tokens": 852,
            "latency": 1.249249,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The provided utils.py file defines utility functions to average PyTorch model parameters across distributed processes using torch.distributed groups, typically for synchronizing parameters during distributed training. It includes functions to (1) aggregate parameters into a flat buffer for efficient all-reduce operations (average_parameters), (2) filter out parameters that require averaging based on gradient presence (get_params_to_average), and (3) provide a convenience wrapper to average either model or optimizer parameters (average_parameters_or_parameter_groups). Overall, the file streamlines the parameter averaging process in distributed machine learning workflows.",
            "total_tokens": 875,
            "latency": 1.500213,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/algorithms/model_averaging/utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides utility functions for averaging PyTorch model parameters or optimizer parameter groups in a distributed setting. It includes a function to filter parameters that have gradients and a core function that flattens, averages using `torch.distributed.all_reduce`, and unpacks the parameters. The main entry point `average_parameters_or_parameter_groups` orchestrates this process for various input types.",
            "total_tokens": 1541,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.717206
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/algorithms/model_averaging/utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides utility functions for PyTorch distributed training, specifically focusing on averaging parameters across multiple processes. It defines functions to select parameters (optionally filtering for those with gradients) and then performs an efficient `all_reduce` operation to compute and apply the average values to the parameters within a given process group. The main entry point handles both direct model parameters and optimizer parameter groups.",
            "total_tokens": 1626,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.828636
        }
    },
    "/torch/_inductor/autotune_process.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `autotune_process.py` file is to manage and execute benchmarking tasks for various computational kernels in parallel across multiple devices, primarily GPUs. It achieves this by launching multiple subprocesses, each dedicated to benchmarking a specific kernel, and distributing the workload among these subprocesses using a thread pool. The file includes classes for managing the subprocesses, handling benchmark requests, and performing the actual benchmarking in separate processes to avoid blocking the main program.",
            "total_tokens": 7854,
            "latency": 1.501945,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file `autotune_process.py` is a Python module that contains classes and functions for managing and interacting with a pool of subprocesses for benchmarking kernels in parallel across devices.\n\nThe main functionality of the file includes:\n\n1. `TuningProcess`: A class that represents a benchmarking subprocess. It has methods to start, check if it's alive, put work items, get results, shut down, and kill the subprocess.\n\n2. `TuningProcessPool`: A class that manages a pool of `TuningProcess` instances. It has methods to start the child processes, distribute work items to the subprocesses using a thread pool, and shut down the child processes.\n\n3. `BenchmarkRequest`: An abstract base class that represents a benchmarking request. It has methods to create a run function, clean up the run function, and do the benchmarking.\n\n4. `TritonBenchmarkRequest`: A class that extends `BenchmarkRequest` for benchmarking Triton kernels. It has additional attributes and methods to load the module, precompile the kernel, and create a run function for the kernel.\n\n5. `CUDABenchmarkRequest`: A class that extends `BenchmarkRequest` for benchmarking CUDA kernels. It has additional attributes and methods to compile the source code, load the DLL, and create a run function for the kernel.\n\n6. `CppBenchmarkRequest`: A class that extends `BenchmarkRequest` for benchmarking C++ kernels. It has additional attributes and methods to load the DLL and create a run function for the kernel.\n\n7. `benchmark_in_sub_process`: A function that uses the `TuningProcessPool` to benchmark a list of `TritonTemplateCaller` instances in separate subprocesses and returns the performance numbers (latency).\n\nOverall, the file provides a framework for parallel benchmarking of kernels across devices using a pool of subprocesses.",
            "total_tokens": 446,
            "latency": 10.401103,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The Python script implements a subprocess-based autotuning framework for benchmarking kernel performance in PyTorch, specifically targeting GPU and CPU kernels. It involves launching subprocesses to run isolated benchmarks across available devices (typically GPUs), measuring their execution times, and selecting optimal kernels based on these measurements. The file supports Triton, CUDA, and C++ kernel benchmarking, leveraging a combination of multiprocessing, subprocess management, and serialization (pickle) for robust and parallel performance evaluation.",
            "total_tokens": 6604,
            "latency": 9.285382,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the provided python file `autotune_process.py` is to implement a system for benchmarking and optimizing kernels in parallel across multiple devices. It includes classes like `TuningProcess` for launching subprocesses, `TuningProcessPool` for managing multiple subprocesses, and various `BenchmarkRequest` subclasses for defining benchmark tasks. The file handles subprocess communication, launching benchmarking tasks, and managing resources like CUDA devices, kernel code cache, and optimizing performance across different devices.",
            "total_tokens": 6573,
            "latency": 1.896022,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"autotune_process.py\" file is to implement a system for autotuning and benchmarking CUDA, Triton, and C++ kernels in separate subprocesses. It defines classes for managing tuning processes, handling benchmark requests, and executing benchmarks across multiple devices. The file also includes utilities for inter-process communication, tensor metadata handling, and code caching to optimize the autotuning process.",
            "total_tokens": 8794,
            "latency": 3.86819,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `autotune_process.py` file is to manage and execute benchmarking tasks for various kernels in separate subprocesses, utilizing a pool of `TuningProcess` instances to parallelize the work across available devices. It defines classes and methods for handling different types of benchmark requests, including Triton, CUDA, and CPU benchmarks, and provides mechanisms for serializing and executing these requests in isolated processes to ensure stability and performance.",
            "total_tokens": 7849,
            "latency": 1.58718,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The provided Python script, `autotune_process.py`, is a highly specialized module dedicated to managing and executing benchmark operations for optimizing computational kernels, particularly in GPU and CPU execution environments. It handles the creation and management of subprocesses (`TuningProcess` and `TuningProcessPool`) to execute benchmarks in isolation, ensuring resource allocation is streamlined and parallelism across devices is utilized. Additionally, it defines multiple benchmark request classes tailored to different programming frameworks (e.g., Triton, CUDA, C++), facilitating kernel performance evaluation within various execution contexts efficiently.",
            "total_tokens": 6626,
            "latency": 2.839386,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide a framework for benchmarking and autotuning of PyTorch kernels, particularly those generated by the Triton compiler. The file defines several classes and functions that enable the creation of benchmarking requests, the execution of these requests in separate processes, and the collection of results. The framework supports both CPU and GPU devices and allows for the benchmarking of different kernel configurations.",
            "total_tokens": 6563,
            "latency": 4.972151,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `autotune_process.py` handles automated performance tuning for Tensor Processing through benchmarking in subprocesses. It provides classes and methods to launch subprocesses specifically configured with distinct device environments, run benchmark tests on various Tensor processing operations using the Torch library, and maintain pool of tuning processes to manage multiple benchmark subprocesses. The file mainly concentrates on setting up the environment for subprocesses, executing benchmark tests, handling inter-process communication, and maintaining a process pool for tuning various kernel operations on different hardware devices.",
            "total_tokens": 6577,
            "latency": 3.384907,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `autotune_process.py`, is to manage a pool of subprocesses for benchmarking and autotuning of PyTorch kernels, particularly those using the Triton library. It creates a pool of subprocesses, each of which can execute benchmarking tasks independently, allowing for parallelization of the autotuning process across multiple devices (e.g., GPUs). The file defines various classes and functions to handle the creation, management, and communication with these subprocesses, as well as the execution of benchmarking tasks within them.",
            "total_tokens": 6596,
            "latency": 2.711676,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The main functionality of **autotune_process.py** is to orchestrate autotuning and benchmarking of kernel implementations (such as Triton, CUDA, or C++ kernels) in subprocesses, isolating each benchmark in its own process or across multiple devices for robustness and parallelism. It provides a mechanism to serialize requests and results, distribute them to worker subprocesses, and coordinate resource management, including workspace allocation and kernel precompilation. The file defines abstractions for specifying benchmark requests, handling device specifics (CPU/GPU), and ensuring safe and efficient benchmarking in a way that tolerates potential crashes or timeouts during kernel evaluation.",
            "total_tokens": 6641,
            "latency": 2.416959,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/autotune_process.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file implements a system for benchmarking kernel code (like Triton, CUDA, and C++) in isolated subprocesses. It manages a pool of these subprocesses, typically one per available device, to execute benchmarking tasks concurrently. Kernel details and tensor metadata are serialized and sent to the subprocesses, which run the kernels and return performance results to the parent process, providing isolation and parallel execution for autotuning.",
            "total_tokens": 8929,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 5.256646
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/autotune_process.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines infrastructure for benchmarking PyTorch kernels (Triton, CUDA, C++) by running them in isolated subprocesses to prevent crashes and ensure accurate timing. It manages a pool of these subprocesses, often one per available device, allowing for parallel benchmarking across multiple GPUs or CPU cores. The code handles serializing benchmark requests (including kernel details and tensor metadata), sending them to worker processes, and retrieving the timing results.",
            "total_tokens": 8824,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.370931
        }
    },
    "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py": {
        "nova-lite-v1": {
            "file_summary": "The file `rocm_benchmark_request.py` defines a class `ROCmBenchmarkRequest` that handles the precompilation and execution of ROCm (Radeon Open Compute) kernel benchmarks within the PyTorch framework. It manages the compilation of kernel source code into shared object files, initializes necessary workspaces, and creates callable functions to execute the compiled kernels with specified input and output tensors. The class integrates with PyTorch's internal mechanisms for device-specific code caching and execution.",
            "total_tokens": 1639,
            "latency": 1.068551,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file \"rocm_benchmark_request.py\" is a Python class that extends the \"GPUDeviceBenchmarkMixin\" and \"BenchmarkRequest\" classes. It is used to manage the compilation and execution of a kernel function on a GPU device using the ROCm platform. The class includes methods for precompiling the kernel code, generating a function to run the kernel, updating the workspace size, loading the DLL, and cleaning up after running the kernel. The class also includes methods for serialization and deserialization of the object across process boundaries.",
            "total_tokens": 119,
            "latency": 3.065521,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python file defines the `ROCmBenchmarkRequest` class, which is tailored for benchmarking ROCm GPU kernels in the context of PyTorch's inductor compiler backend. The class compiles and executes ROCm kernels from provided source code, dynamically manages workspace memory, and caches compiled source using `ROCmCodeCache`. It provides methods for creating executable functions (`make_run_fn`), estimating workspace requirements (`update_workspace_size`), and handling dynamic loading/unloading of compiled DLLs.",
            "total_tokens": 1358,
            "latency": 4.179292,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "This Python file, named 'rocm_benchmark_request.py', defines a class called ROCmBenchmarkRequest that extends GPUDeviceBenchmarkMixin and BenchmarkRequest classes. It handles benchmark requests for ROCm by precompiling source code, creating run functions, updating workspace size, loading dynamic link libraries (DLLs), and cleaning up after execution, in order to facilitate efficient GPU device benchmarking with TensorFlow code.",
            "total_tokens": 1333,
            "latency": 1.109838,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"rocm_benchmark_request.py\" file is to define a ROCmBenchmarkRequest class for benchmarking ROCm (Radeon Open Compute) kernels. This class handles the compilation, loading, and execution of ROCm kernels, including managing workspace memory and interfacing with the ROCm code cache. It provides methods for precompiling, creating run functions, updating workspace sizes, and cleaning up resources associated with the benchmark requests.",
            "total_tokens": 1788,
            "latency": 3.12977,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `rocm_benchmark_request.py` file defines a `ROCmBenchmarkRequest` class for handling benchmarking requests specific to ROCm (Radeon Open Compute) within the PyTorch framework. It manages the precompilation of ROCm kernel code, dynamic loading of compiled libraries, and execution of kernel functions with appropriate workspace management.",
            "total_tokens": 1606,
            "latency": 1.020834,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `rocm_benchmark_request.py` file defines a `ROCmBenchmarkRequest` class that facilitates benchmarking and running GPU kernels on AMD ROCm architecture using PyTorch's Inductor framework. It manages kernel metadata, code caching, and dynamic compilation, while also generating callable functions to execute compiled kernels with support for handling workspace memory allocation. Additionally, it ensures thread-safe precompilation and cleanup procedures to efficiently utilize GPU resources.",
            "total_tokens": 1347,
            "latency": 1.567887,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a class `ROCMBenchmarkRequest` that represents a request to benchmark a kernel on a ROCm (Radeon Open Compute) device. The class provides methods to precompile the kernel, create a run function for the kernel, update the workspace size, and ensure the DLL is loaded. The class is designed to be used in a multi-threaded environment and is responsible for managing the kernel's compilation, execution, and cleanup.",
            "total_tokens": 1354,
            "latency": 4.408685,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `rocm_benchmark_request.py` defines a class `ROCmBenchmarkRequest` which inherits behavior from `GPUDeviceBenchmarkMixin` and `BenchmarkRequest`. This class is utilized primarily for handling kernel benchmarking requests in a ROCm (Radeon Open Compute) environment. The class mainly focuses on managing the DAL\u00cd wrapping for compiled GPU kernels, updating and managing workspace sizes for tensor operations, and precompiling source code for ROCm execution, ensuring all instances are serializable and ready for cross-process utility.\n",
            "total_tokens": 1358,
            "latency": 3.774992,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `rocm_benchmark_request.py`, is to handle benchmarking requests for ROCm (Radeon Open Compute) kernels, which are used for GPU acceleration. The `ROCmBenchmarkRequest` class manages the compilation, execution, and cleanup of these kernels, utilizing the `torch` library and `ROCmCodeCache` for code caching and compilation. This class enables the creation of a run function for a given kernel, which can be executed with input tensors and an output tensor, allowing for efficient benchmarking and testing of ROCm kernels.",
            "total_tokens": 1377,
            "latency": 2.705526,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines ROCmBenchmarkRequest, a class for managing the compilation, caching, and execution of ROCm (AMD GPU) kernels for benchmarking purposes in PyTorch's Inductor backend. It handles kernel code compilation (and caching), workspace memory management, and dynamically creates callable run functions for executing the GPU kernels with specified input/output tensor metadata and arguments. The class ensures resources are properly managed and is designed to be serializable for use across process boundaries.",
            "total_tokens": 1352,
            "latency": 1.597793,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This file defines the `ROCmBenchmarkRequest` class, responsible for managing the benchmarking of compiled ROCm kernels within PyTorch's inductor framework. It handles the compilation of ROCm source code into a shared library using `ROCmCodeCache` and manages the loading of this library. The class prepares a callable function to execute the kernel with specified input/output tensors and arguments, including determining and allocating necessary workspace memory.",
            "total_tokens": 2263,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.850517
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `ROCmBenchmarkRequest` class, which extends `BenchmarkRequest` for ROCm (AMD GPU) devices. It encapsulates the source code and metadata for a specific kernel to be benchmarked. The class handles compiling the kernel source code, loading the compiled library, determining the required workspace size, and generating a callable function to execute the kernel with provided tensors.",
            "total_tokens": 2076,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.603211
        }
    },
    "/torch/_higher_order_ops/invoke_subgraph.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"invoke_subgraph.py\" file is to facilitate the invocation of a subgraph within PyTorch's autograd and functionalization mechanisms. It provides a way to trace and compile subgraphs, enabling efficient execution and gradient computation. The file defines a custom Higher Order Operator (HOP) called \"InvokeSubgraphHOP\" that allows the subgraph to be identified and compiled, and it includes utility functions for marking regions to be compiled, tracing joint graphs, and creating forward and backward graphs for the subgraph.",
            "total_tokens": 3639,
            "latency": 1.379098,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to define a class `InvokeSubgraphHOP` that extends `HigherOrderOperator` and provides a method `__call__` to handle the `invoke_subgraph` operation. It also defines a function `invoke_subgraph_placeholder` that serves as a placeholder for the `invoke_subgraph` operation when used with `torch.compile`. Additionally, the file includes functions for marking a region for compilation, getting the cache for `invoke_subgraph`, and creating forward and backward graphs for the `invoke_subgraph` operation. The file also includes a class `InvokeSubgraphAutogradOp` that extends `torch.autograd.Function` and provides methods for the forward and backward passes of the `invoke_subgraph` operation. The file also includes various helper functions and decorators to support the main functionality.",
            "total_tokens": 182,
            "latency": 4.642178,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements advanced internal mechanisms within PyTorch to manage, invoke, and cache computations of computational subgraphs, a component specifically used by torch.compile and Dynamo operations. It defines a HigherOrderOperator named \"invoke_subgraph,\" facilitating subgraph execution and integration with PyTorch's auto-differentiation (autograd) system, tracing routines, and optimization tooling. Primarily, it includes logic to handle both forward-backward graph execution, caching of traced subgraphs, and integrating cleanly with functionalization, autograd, tracing, and fake tensor proxy modes.",
            "total_tokens": 3067,
            "latency": 6.215747,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `invoke_subgraph.py` file is to define and implement a higher-order operator (`InvokeSubgraphHOP`) for invoking subgraphs in PyTorch. This file contains functions for invoking subgraphs, tracing forward and backward graphs, handling autograd operations, and managing cache for subgraph invocations. Additionally, it includes implementations for different dispatch keys like `CompositeExplicitAutograd` and `Autograd`, as well as proxy tensor dispatch modes.",
            "total_tokens": 3033,
            "latency": 1.604888,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"invoke_subgraph.py\" file is to implement a higher-order operator called \"invoke_subgraph\" that allows for the execution of subgraphs within a larger computational graph. It provides mechanisms for tracing, caching, and executing these subgraphs, including support for autograd and functionalization. The file also includes implementations for various dispatch keys and proxy modes, enabling integration with PyTorch's compilation and tracing systems.",
            "total_tokens": 4040,
            "latency": 3.5362,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `invoke_subgraph.py` file defines a mechanism for invoking subgraphs within PyTorch, utilizing higher-order operators (HOPs) to manage both forward and backward computations. It includes functionality for tracing and caching subgraphs, handling autograd operations, and integrating with TorchDynamo for compilation optimizations.",
            "total_tokens": 3597,
            "latency": 1.249014,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `invoke_subgraph.py` file implements functionality for executing, tracing, and functionalizing computational subgraphs within PyTorch's higher-order operations and automatic differentiation framework. It introduces a custom `InvokeSubgraphHOP` operator to handle the invocation and caching of subgraphs, which are either PyTorch `GraphModule` objects or functionalization context wrappers. The file also includes integration with PyTorch's `torch.compile`, caching mechanisms for dispatch modes, and autograd hooks for forward and backward graph tracing to optimize computation and support efficient automatic differentiation.",
            "total_tokens": 3061,
            "latency": 2.011862,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to implement the `invoke_subgraph` function, which is a higher-order operator (HOP) that allows for the invocation of a subgraph within a larger graph. The subgraph can be a PyTorch `GraphModule` or a `FunctionalizeCtxWrapper`, and the `invoke_subgraph` function takes care of tracing the forward and backward graphs, as well as caching the results to avoid redundant computations. The file also defines several helper functions and classes to support the `invoke_subgraph` function, including autograd functions, tracing functions, and caching mechanisms.",
            "total_tokens": 3066,
            "latency": 5.668458,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file \"invoke_subgraph.py\" defines and handles the execution of subgraphs in the PyTorch framework, particularly for purposes like autograd operations, TorchDynamo compilation, and handling tensors in different contexts (such as functionalization and proxy modes). Central to the file is `InvokeSubgraphHOP`, a higher order operator class for invoking subgraphs, supported by various utility functions and class methods to manage the invocation in various compute environments (e.g., autograd, functionalization). It also features methods to trace and optimize joint forward and backward computational graphs, integrate with TorchDynamo to ensure efficient execution, and handle specialized tensor operations suitable for dynamic code transformation and graph-level optimizations in PyTorch.",
            "total_tokens": 3082,
            "latency": 5.108322,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `invoke_subgraph.py`, is to implement a higher-order operator (HOP) called `InvokeSubgraph` that enables the invocation of subgraphs within PyTorch models, allowing for more efficient and flexible computation graphs. This operator is designed to work with PyTorch's dynamic computation graph and autograd system, enabling features like just-in-time (JIT) compilation and automatic differentiation. The file defines various implementations of the `InvokeSubgraph` operator for different dispatch keys, including autograd, composite explicit autograd, and proxy torch dispatch mode, to handle different scenarios and use cases.",
            "total_tokens": 3074,
            "latency": 5.238344,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines and implements the \"invoke_subgraph\" Higher Order Operator (HOP) in PyTorch, facilitating the encapsulation, invocation, and autodiff support for a compiled subgraph within larger computation graphs. It provides integration points for tracing, functionalization, autograd, and proxy dispatch backends, as well as caching logic for repeated subgraph execution, enabling efficient re-use and correct gradient computation. Additionally, the file exposes utilities and decorators (such as mark_compile_region) allowing users or the Torch compiler to demarcate subregions for isolated tracing, compilation, or special execution.",
            "total_tokens": 3071,
            "latency": 2.005836,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/_higher_order_ops/invoke_subgraph.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a PyTorch Higher Order Operator (HOP) called `invoke_subgraph` used primarily by `torch.compile`. Its main purpose is to encapsulate a region of code (often marked by the `@mark_compile_region` decorator) into a distinct subgraph within the larger computation graph. This allows `torch.compile` to compile the subgraph once and reuse the compiled artifact, preventing aggressive inlining and enabling modular compilation and differentiation through the subgraph via custom autograd handling.",
            "total_tokens": 4086,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.20629
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_higher_order_ops/invoke_subgraph.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a PyTorch `HigherOrderOperator` called `invoke_subgraph`. This operator is used by `torch.compile` to represent and manage compiled or traced subgraphs within a larger computation. It handles dispatching for various modes like Autograd, Functionalization, and Proxy tracing, including tracing joint forward/backward graphs and caching results.",
            "total_tokens": 3893,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.547777
        }
    },
    "/tools/code_coverage/package/oss/utils.py": {
        "nova-lite-v1": {
            "file_summary": "The `utils.py` file provides utility functions for managing and interacting with binary files, environment variables, and subprocesses, specifically tailored for a PyTorch-related project. It includes functions for locating binary files, detecting compiler types, and running Python tests. The file also handles environment-specific configurations and cleanup tasks, such as removing `.gcda` files.",
            "total_tokens": 1144,
            "latency": 0.945472,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "titan_premier": {
            "file_summary": "The 'utils.py' file contains various utility functions for managing files and directories, as well as interacting with the operating system. Specifically, it includes functions for:\n\n1. Retrieving the path to the PyTorch folder and its subdirectories, such as the binary folder and shared library.\n2. Detecting the compiler type based on user input or by automatically detecting it from the system.\n3. Cleaning up gcda files in the build directory.\n4. Running Python test scripts and handling any errors that may occur.\n\nThe file also includes functions for removing files and printing error messages, which are likely used by other functions in the file. Overall, the main functionality of the file is to provide a set of tools for managing and running tests in the PyTorch codebase.",
            "total_tokens": 173,
            "latency": 3.783641,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.5": {
            "file_summary": "This Python file contains utility functions related to environment setup, file paths, and compiler detection for a PyTorch project's code coverage tools. It handles finding test binaries, shared libraries, LLVM tools, cleaning generated coverage files (\".gcda\"), and executing OSS Python tests. Additionally, it includes functionality to auto-detect or validate the compiler type (clang or gcc) based on environment variables or the system settings.",
            "total_tokens": 965,
            "latency": 3.840876,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given `utils.py` file contains various utility functions related to managing binary files, shared libraries, compiler types, and executing Python tests in a PyTorch project. It includes functions to get paths for binary files and LLVM tools, detect compiler types, clean up generated files, and run Python tests. Additionally, there are functions to handle environmental variables, such as `PYTORCH_FOLDER` and `CXX`, and interactions with the system using subprocess and os modules.",
            "total_tokens": 970,
            "latency": 1.027177,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"utils.py\" file is to provide utility functions for managing and executing tests in a PyTorch development environment. It includes functions for locating binary files and folders, detecting compiler types, handling gcda files, and running Python tests. The file also contains helper functions for retrieving paths and cleaning up test-related files.",
            "total_tokens": 1207,
            "latency": 2.453213,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "nova-pro-v1": {
            "file_summary": "The `utils.py` file provides utility functions for locating and managing binaries, libraries, and configuration settings related to testing and compilation within a PyTorch development environment. It includes functions for determining paths to binaries and shared libraries, detecting the compiler type, and running Python tests. Additionally, it offers cleanup functionality for test-related files.",
            "total_tokens": 1137,
            "latency": 0.980909,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4o": {
            "file_summary": "The `utils.py` file provides utility functions to support testing and build processes for a codebase, including managing and retrieving paths for binaries, shared libraries, and test files related to PyTorch. It also includes functionalities for detecting the compiler type, cleaning up `.gcda` coverage files, and running Python test scripts. The file leverages environment variables and subprocess commands for dynamic setup and operations.",
            "total_tokens": 963,
            "latency": 1.405053,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to provide utility functions for working with the PyTorch project, including getting binary folders and files, detecting compiler types, cleaning up gcda files, and running OSS Python tests. The file appears to be part of a larger testing or build framework for PyTorch. It provides various helper functions to interact with the PyTorch project structure and tools.",
            "total_tokens": 960,
            "latency": 3.140232,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4": {
            "file_summary": "The Python file `utils.py` provides several utility functions related to managing paths and processes for a project centered around the use of the PyTorch library. Functions in the file are used to get directory paths, detect compiler types, handle Python and C++ test binaries, manage shared libraries, and clean up coverage files (`*.gcda`). Additionally, it accommodates different compilers (clang, gcc) and handles subprocesses for invoking shell commands, including detecting compilers or running Python scripts directly.",
            "total_tokens": 973,
            "latency": 4.575037,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `utils.py`, is to provide utility functions for working with the PyTorch project, including getting binary folders and files, detecting compiler types, and running tests. These functions appear to be designed to support testing and development workflows, such as running Python tests and cleaning up intermediate files. The file also includes functions for interacting with the file system, running subprocesses, and handling environment variables.",
            "total_tokens": 969,
            "latency": 1.502182,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.1": {
            "file_summary": "This file provides utility functions for managing and interacting with a PyTorch project build and test environment, particularly for code coverage and testing workflows. It includes functions to locate PyTorch build directories, binaries, and shared libraries, as well as to detect the system's compiler, clean up coverage-related files, and run Python-based test scripts. The utilities rely on environment variables, subprocesses, and file system operations to abstract away the complexities of handling different test types (C++ and Python) and build setups.",
            "total_tokens": 984,
            "latency": 2.309779,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/code_coverage/package/oss/utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file `utils.py` provides utility functions primarily for interacting with a PyTorch project's build and test environment. It includes functions to locate build directories, binaries, shared libraries, and external tools like LLVM. Additionally, it offers functionalities for detecting the C++ compiler type, managing code coverage `.gcda` files, and running Python tests.",
            "total_tokens": 1386,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.293384
        },
        "gemini-2.5": {
            "file_name": "/tools/code_coverage/package/oss/utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides utility functions primarily for supporting the PyTorch build and testing environment, particularly related to code coverage analysis. It includes functions to locate PyTorch source/build directories, test binaries (C++ and Python), shared libraries, and LLVM tools, as well as detecting the compiler type (Clang/GCC). Additionally, the file offers helpers for finding and cleaning up code coverage data files (`.gcda`) and executing Python tests using subprocess.",
            "total_tokens": 1595,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 6.609058
        }
    },
    "/benchmarks/inference/server.py": {
        "nova-lite-v1": {
            "file_summary": "This Python script implements a multi-process system for benchmarking the performance of a deep learning model, specifically a ResNet18 model, in a server-client architecture. The FrontendWorker process sends requests to the BackendWorker process, which performs computations on the input data and returns the results. The script measures various metrics such as latency, throughput, and GPU utilization during the process. The results are logged and can be optionally profiled using PyTorch's profiler.",
            "total_tokens": 3605,
            "latency": 1.192143,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to measure the throughput and latency of requests sent to a backend process, as well as the GPU utilization. It does this by creating two worker processes: a FrontendWorker and a BackendWorker. The FrontendWorker sends requests to the BackendWorker, measures the latency and throughput of the requests, and records the GPU utilization. The BackendWorker receives requests, performs some computation, and returns the results. The results are then recorded in a CSV file. The file also includes options to download a pre-trained model checkpoint and to profile the computation using PyTorch's profiler.",
            "total_tokens": 132,
            "latency": 3.513627,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements a performance benchmarking server using PyTorch which consists of two primary components: a FrontendWorker sending simulated inference requests and measuring metrics like latency, throughput, and GPU utilization, and a BackendWorker that processes these requests using a ResNet-18 model loaded on a GPU asynchronously. The script can profile computation, handle model compilation, and is capable of downloading the required model checkpoint automatically. It records the computed metrics to a CSV file for later analysis.",
            "total_tokens": 2824,
            "latency": 5.428997,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `server.py` file is to implement a system with a frontend worker and a backend worker that interact with each other to send requests, measure latency and throughput, and utilize the GPU for processing data using PyTorch. The frontend worker sends requests to the backend worker, measures response times, and records metrics like latency and throughput. The backend worker processes requests by running computations on a model (ResNet18), returns results, and records metrics related to model loading, compilation, and GPU utilization.",
            "total_tokens": 2819,
            "latency": 1.56274,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of this Python file is to implement a client-server architecture for benchmarking a ResNet18 model's performance. It consists of a FrontendWorker that sends requests and measures throughput, latency, and GPU utilization, and a BackendWorker that processes these requests using the ResNet18 model. The script also includes functionality for profiling, data handling, and metrics collection, with the results being saved to a CSV file.",
            "total_tokens": 3838,
            "latency": 2.983913,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `server.py` file implements a client-server model using multiprocessing and multithreading to measure the performance of a machine learning model. It sends requests to a backend process, measures throughput, latency, and GPU utilization, and saves the results to a CSV file.",
            "total_tokens": 3567,
            "latency": 0.991925,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The main functionality of the `server.py` file is to implement a performance benchmarking system that measures the latency, throughput, and GPU utilization for processing tensor data using a pretrained ResNet-18 model on a CUDA-enabled GPU. It uses two multiprocessing workers: a `FrontendWorker` to simulate sending requests and collect metrics, and a `BackendWorker` to load the model, perform computations, and process requests. The script integrates PyTorch, asynchronous processing, and profiling features, exporting gathered metrics in CSV format for analysis.",
            "total_tokens": 2834,
            "latency": 1.808945,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to simulate a client-server architecture where a frontend worker sends requests to a backend worker, which processes the requests using a ResNet18 model and returns the results. The frontend worker measures the throughput and latency of the requests, as well as the GPU utilization, and stores the metrics in a dictionary. The backend worker uses multiple threads to handle the requests concurrently.",
            "total_tokens": 2800,
            "latency": 3.85693,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python script `server.py` is designed to execute a distributed computing system involving front-end and back-end workers processing deep learning tasks. The front-end worker sends tensors to a backend worker which processes them using a ResNet model, benchmarking performance metrics such as latency and GPU utilization. The system leverages Python's multiprocessing capabilities, including queues and events for inter-process communication, as well as threading and asynchronous IO for concurrent operations, integrating numerous libraries like PyTorch and Pandas for data manipulation and model computations.",
            "total_tokens": 2818,
            "latency": 2.984128,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `server.py`, is to simulate a client-server architecture where a frontend worker sends requests to a backend worker, which processes the requests using a ResNet18 model and returns the responses. The script measures the throughput, latency, and GPU utilization of the backend worker and logs the results to a CSV file. The backend worker utilizes multi-threading and asynchronous execution to handle the requests concurrently, allowing for efficient processing of multiple requests in parallel.",
            "total_tokens": 2818,
            "latency": 1.858587,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "This file implements a benchmarking server for measuring the performance of a GPU-based PyTorch model, specifically ResNet18. It orchestrates frontend and backend worker processes: the frontend sends batches of fake image data to the backend, measures throughput, latency, and GPU utilization, while the backend loads the model, performs inference, and returns results. The script collects and saves detailed performance metrics (including optional profiling) to a CSV file after running the specified number of iterations and batch size.",
            "total_tokens": 2825,
            "latency": 1.972349,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/benchmarks/inference/server.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script simulates a client-server system to benchmark the performance of a deep learning inference task using a ResNet18 model. A `FrontendWorker` process sends requests and measures metrics like latency, throughput, and GPU utilization. The `BackendWorker` processes these requests using the model, managing data transfer and computation with threads and asyncio, and the collected performance data is saved to a CSV file.",
            "total_tokens": 4005,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.699115
        },
        "gemini-2.5": {
            "file_name": "/benchmarks/inference/server.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script implements a multi-process benchmarking system for a PyTorch ResNet model, separating tasks into a `FrontendWorker` and a `BackendWorker`. The `FrontendWorker` sends batches of inference requests, measures performance metrics like latency, throughput, and GPU utilization. The `BackendWorker` receives these requests, performs model inference asynchronously on a GPU using multiple threads and CUDA streams for efficiency, and sends results back.",
            "total_tokens": 4356,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.522589
        }
    },
    "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `cache_debug_microbenchmarks.py` file is to measure the performance of iterating over a large number of FX (Function for eXpressive) nodes in PyTorch. It creates a large symbolic trace graph, computes its hash details, and then benchmarks the time taken to debug and print the lines of this graph using the `FxGraphHashDetails` class. The script outputs the time taken for the operation and the rate of nodes processed per second.",
            "total_tokens": 392,
            "latency": 1.047966,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"cache_debug_microbenchmarks.py\" is to measure the performance of iterating over a large number of FX nodes. It creates a huge graph using the torch.fx.symbolic_trace function, and then uses the FxGraphHashDetails class to get the debug lines of the graph. The performance is measured by timing how long it takes to iterate over the debug lines of the graph a certain number of times. The results are then printed to the console.",
            "total_tokens": 105,
            "latency": 2.386932,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file benchmarks the performance of iterating over nodes in a large PyTorch FX computational graph. It constructs a huge graph consisting of repeated sine operations and uses `FxGraphHashDetails` to iterate through its nodes, measuring and printing how efficiently (in nodes per second) this iteration occurs. This tool is intended primarily for debugging or analyzing PyTorch's internal FX graph handling performance.",
            "total_tokens": 338,
            "latency": 4.202449,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The Python file \"cache_debug_microbenchmarks.py\" measures the performance of iterating over a large symbolic graph created using Torch FX. It defines a function to create a huge graph, traces it symbolically, calculates hash details, and then measures the time taken to iterate over the graph nodes. The script outputs the time taken to iterate over the nodes and the nodes processed per second.",
            "total_tokens": 337,
            "latency": 0.786206,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the file \"cache_debug_microbenchmarks.py\" is to perform a microbenchmark on the performance of iterating over a large number of FX nodes in a PyTorch graph. It creates a huge graph with many repeated operations, measures the time taken to generate debug lines for this graph using FxGraphHashDetails, and reports the performance in terms of nodes processed per second.",
            "total_tokens": 395,
            "latency": 2.21454,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The file `cache_debug_microbenchmarks.py` is designed to benchmark the performance of iterating over a large number of FX nodes generated by PyTorch's symbolic tracing, specifically measuring the time taken to process these nodes and printing the results.",
            "total_tokens": 343,
            "latency": 0.762317,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The file `cache_debug_microbenchmarks.py` evaluates the performance of iterating over a large number of FX graph nodes created using PyTorch's `torch.fx` symbolic tracing mechanism. It defines a function generating a computational graph with repetitive sine operations, measures the processing speed using the `FxGraphHashDetails.debug_lines()` method, and reports the time and nodes-per-second rate for benchmarking purposes. This is primarily aimed at profiling and debugging PyTorch's FX graph handling in terms of performance.",
            "total_tokens": 358,
            "latency": 1.401668,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to measure the performance of iterating over a large PyTorch FX graph, specifically the time it takes to iterate over a graph with 10,000 nodes, repeated 100 times. The file uses the `timeit` module to benchmark the execution time of the `debug_lines` method of the `FxGraphHashDetails` class, which is responsible for iterating over the graph nodes. The results are printed to the console, showing the total time taken and the number of nodes iterated per second.",
            "total_tokens": 376,
            "latency": 4.267466,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python script \"cache_debug_microbenchmarks.py\" benchmarks the performance related to iterating and processing nodes in a large FX-graph (built using PyTorch FX) by employing the `FxGraphHashDetails.debug_lines()` method. It creates a huge computation graph where a trigonometric function (sine) is applied repeatedly (N times) to a tensor, symbolically traces it into an FX graph, and then measures the time it takes to process debug information from the graph K times, reporting the results in seconds and nodes per second. This helps in evaluating the efficiency of debug-logging mechanisms in handling large FX graphs.",
            "total_tokens": 387,
            "latency": 11.337669,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `cache_debug_microbenchmarks.py` file is to measure the performance of iterating over a large number of FX nodes in a PyTorch graph. The script creates a huge graph with 10,000 nodes, and then uses the `timeit` module to benchmark the time it takes to iterate over these nodes 100 times. The results are printed to the console, showing the total time taken and the number of nodes processed per second.",
            "total_tokens": 362,
            "latency": 1.274868,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file cache_debug_microbenchmarks.py defines a microbenchmark that constructs a large torch.fx computational graph by repeatedly applying the sine operation, then measures the performance of the debug_lines() method from the FxGraphHashDetails class over many iterations. It uses timeit to record how long it takes to process a large number (N * K) of FX nodes, printing the total time taken and throughput. The primary purpose is to benchmark and debug the performance of code related to torch._inductor's handling of FX graph details.",
            "total_tokens": 365,
            "latency": 2.359589,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script defines a microbenchmark to measure the performance of iterating over nodes in a large `torch.fx.Graph`. It creates a graph with many operations and uses `timeit` to time the execution of generating debug lines for this graph via `FxGraphHashDetails.debug_lines()`. The script then prints the total number of nodes processed, the time taken, and the processing rate in nodes per second.",
            "total_tokens": 857,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.403948
        },
        "gemini-2.5": {
            "file_name": "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script benchmarks the performance of generating debug information for a large PyTorch FX graph. It specifically measures the time taken by the `debug_lines()` method of the `FxGraphHashDetails` class, which is part of PyTorch Inductor's code caching system. The script uses `timeit` to repeatedly execute this method on a synthetically generated large graph and reports the execution speed in nodes per second.",
            "total_tokens": 994,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.038948
        }
    },
    "/torch/distributions/independent.py": {
        "nova-lite-v1": {
            "file_summary": "The `independent.py` file defines a custom `Independent` class that inherits from `torch.distributions.distribution.Distribution`. This class allows for reinterpreting some of the batch dimensions of a base distribution as event dimensions, which can be useful for reshaping the output of methods like `log_prob`. The primary purpose of this class is to facilitate the transformation of distributions, such as converting a diagonal Normal distribution to have the same shape as a Multivariate Normal distribution.",
            "total_tokens": 1461,
            "latency": 1.046534,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file \"independent.py\" is a Python file that defines a class called \"Independent\" which is a subclass of the \"Distribution\" class from the \"torch.distributions\" module. The main functionality of the \"Independent\" class is to reinterpret some of the batch dimensions of a distribution as event dimensions, which is useful for changing the shape of the result of the \"log_prob\" method. The class takes two arguments: \"base_distribution\", which is the base distribution to reinterpret, and \"reinterpreted_batch_ndims\", which is the number of batch dimensions to reinterpret as event dimensions. The class also overrides several methods from the \"Distribution\" class, such as \"sample\", \"rsample\", \"log_prob\", \"entropy\", and \"enumerate_support\", to apply the reinterpretation of batch dimensions as event dimensions.",
            "total_tokens": 180,
            "latency": 3.97886,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python file defines the \"Independent\" class, a wrapper around PyTorch distributions that reinterprets some of the batch dimensions of a given distribution as event dimensions. It adjusts shape semantics (especially for methods like `log_prob`, `entropy`) to allow interchangeability with multivariate distributions. Essentially, it helps manage the dimensions and behavior of probability distributions in PyTorch for flexible distribution reshaping.",
            "total_tokens": 1214,
            "latency": 5.243362,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The file defines a class called \"Independent\" that extends the \"Distribution\" class in PyTorch. It allows reinterpreting some batch dimensions of a distribution as event dimensions, which is useful for changing the shape of the result of log probabilities. The class provides methods for sampling, calculating log probabilities, and handling the distribution's properties like mean, mode, and variance.",
            "total_tokens": 1185,
            "latency": 2.218926,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The \"independent.py\" file defines an Independent class that extends PyTorch's Distribution class. This class reinterprets some of the batch dimensions of a base distribution as event dimensions, primarily to modify the shape of the log probability output. It allows for creating distributions with different batch and event shapes while maintaining the underlying distribution's properties, which is useful for making certain distributions interchangeable in probabilistic models.",
            "total_tokens": 1566,
            "latency": 2.824686,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `independent.py` file defines a class `Independent` that inherits from `Distribution` in PyTorch. It reinterprets some batch dimensions of a base distribution as event dimensions, primarily to change the shape of the result of `log_prob`. The class includes methods to handle sampling, log probability calculation, entropy, and other distribution properties.",
            "total_tokens": 1437,
            "latency": 1.11533,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The Python file defines a custom `Independent` class that extends PyTorch's `Distribution` class. Its main functionality is to reinterpret certain batch dimensions of a base distribution as event dimensions, enabling operations like `log_prob` or `entropy` to sum over selected dimensions while maintaining compatibility with PyTorch's probabilistic modeling framework. This is particularly useful when working with distributions that require reshaping or reinterpreting batch-event relationships for advanced modeling scenarios.",
            "total_tokens": 1224,
            "latency": 1.40402,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a custom PyTorch distribution class called `Independent`, which reinterprets some of the batch dimensions of a base distribution as event dimensions. This class allows for changing the shape of the result of the `log_prob` method, making it useful for creating diagonal distributions with the same shape as multivariate distributions. The class inherits from PyTorch's `Distribution` class and overrides several methods to accommodate the reinterpreted batch dimensions.",
            "total_tokens": 1212,
            "latency": 3.822538,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `independent.py` defines a class named `Independent`, which extends the functionality of a given PyTorch distribution by allowing the reinterpretation of certain batch dimensions as event dimensions, primarily to modify the output shape of the `log_prob` method. The class includes methods for calculating properties like mean, mode, variance, and entropy by leveraging analogous methods from the base distribution, while modifying returns such as log probabilities to consider new event dimensions. Essential redefinitions and validations are contained within the constructor, and methods for sampling and expansion support such adaptations to navigate between batch and event dimensions seamlessly.",
            "total_tokens": 1230,
            "latency": 2.936114,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to define a class `Independent` that reinterprets some of the batch dimensions of a distribution as event dimensions, allowing for changes in the shape of the result of `log_prob`. This class is a subclass of `torch.distributions.distribution.Distribution` and is designed to work with PyTorch distributions. The `Independent` class takes a base distribution and the number of batch dimensions to reinterpret as event dimensions, and provides various methods for working with the reinterpreted distribution, including `log_prob`, `sample`, and `entropy`.",
            "total_tokens": 1234,
            "latency": 10.707086,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines the Independent distribution class, which is a PyTorch distribution wrapper that reinterprets a specified number of batch dimensions of a base distribution as event dimensions. This is primarily useful for adjusting how methods like log_prob sum across dimensions, making independent distributions behave structurally like multivariate ones. The class properly handles sampling, shape management, and probability computation by delegating to the base distribution and summing or reshaping results as needed.",
            "total_tokens": 1223,
            "latency": 1.392579,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributions/independent.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `Independent` distribution class from PyTorch's distributions module. It acts as a wrapper around a base distribution, reinterpreting a specified number of the base distribution's batch dimensions as event dimensions. This modification primarily affects the shape of the output from methods like `log_prob` and `entropy` by summing over the newly designated event dimensions.",
            "total_tokens": 1735,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.882877
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributions/independent.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `Independent` distribution class within the `torch.distributions` library. The `Independent` distribution wraps a base distribution and reinterprets a specified number of its rightmost batch dimensions as event dimensions. This transformation primarily impacts methods like `log_prob` and `entropy`, which then sum over these reinterpreted dimensions.",
            "total_tokens": 1696,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.391357
        }
    },
    "/torch/_export/db/examples/specialized_attribute.py": {
        "nova-lite-v1": {
            "file_summary": "The given Python file defines an enumeration `Animal` and a PyTorch module `SpecializedAttribute`. The `SpecializedAttribute` class has a `forward` method that squares its input and adds a constant if the attribute `a` matches the value of `Animal.COW`; otherwise, it raises a `ValueError`. This file is primarily focused on creating a specialized neural network module with conditional behavior based on an enumerated attribute.",
            "total_tokens": 327,
            "latency": 0.926584,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The file \"specialized_attribute.py\" is a Python file that defines an Enum class called \"Animal\" and a class called \"SpecializedAttribute\" that inherits from \"torch.nn.Module\". The \"SpecializedAttribute\" class has two attributes, \"a\" and \"b\", and a method called \"forward\" that takes an input \"x\" and performs a calculation based on the value of \"a\". The file also includes an example usage of the \"SpecializedAttribute\" class with a random input.\n\nThe main functionality of the file is to define a specialized attribute model using the PyTorch library and perform calculations based on the value of a specific attribute. The file also includes an example usage of the model with a random input.",
            "total_tokens": 157,
            "latency": 3.394791,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The Python file defines a PyTorch neural network module named \"SpecializedAttribute\" that includes specialized attributes (\"a\" with value \"moo\" and \"b\" with value 4). Its forward pass squares its input tensor and adds the attribute \"b\" when attribute \"a\" matches the value of \"Animal.COW\". Otherwise, it raises a ValueError.",
            "total_tokens": 298,
            "latency": 3.404345,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The python file named specialized_attribute.py defines a class SpecializedAttribute which is a subclass of torch.nn.Module. This class has an __init__ method initializing attributes 'a' and 'b', and a forward method that performs a specific calculation based on the value of 'a'. An example of creating an instance of SpecializedAttribute and providing it with input data is also included in the file.",
            "total_tokens": 306,
            "latency": 1.273287,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"specialized_attribute.py\" file is to define a custom PyTorch module called SpecializedAttribute. This module performs a conditional operation in its forward method, multiplying the input by itself and adding a constant if a specific attribute matches a predefined value from an Animal enum. The file also includes an example of creating an instance of the module and preparing example input arguments for demonstration purposes.",
            "total_tokens": 362,
            "latency": 2.583491,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `specialized_attribute.py` file defines a PyTorch neural network module called `SpecializedAttribute` with specialized attributes. The module performs a specific forward computation based on the value of its attribute `a`, which is compared against an enumeration value from the `Animal` enum. An example usage of the module is provided at the end of the file.",
            "total_tokens": 313,
            "latency": 0.931597,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The file defines a PyTorch `nn.Module` named `SpecializedAttribute` that performs a simple computation in its `forward` method, squaring the input tensor and adding a constant if a class attribute matches the value of a specific `Animal` enum. The file also includes an example input tensor and instantiates the model as `model`. It demonstrates the use of enums and conditionally specialized behaviors in a neural network module.",
            "total_tokens": 311,
            "latency": 1.873342,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a custom PyTorch module called `SpecializedAttribute` that inherits from `torch.nn.Module`. This module has a specialized attribute `a` that is compared to an enum value from the `Animal` enum, and if they match, it performs a specific operation on the input `x` in the `forward` method. The file also creates an instance of this module and passes a random tensor to its `forward` method.",
            "total_tokens": 330,
            "latency": 3.588364,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file \"specialized_attribute.py\" defines a simple PyTorch model named `SpecializedAttribute` which inherits from `torch.nn.Module`. The model checks if an internal attribute `a` matches the value assigned to an animal (in this case, \"COW\" with value \"moo\") in an enumeration; if the check passes, it computes and returns `(x*x) + b` during the forward pass, otherwise, it raises a `ValueError`. The primary purpose of this file seems to be demonstrating a PyTorch model that processes input differently based on a predefined enum condition.",
            "total_tokens": 349,
            "latency": 3.280786,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `specialized_attribute.py` file is to define a custom PyTorch module called `SpecializedAttribute` that inherits from `torch.nn.Module`. This module has a specialized attribute `a` that is compared to an enum value from the `Animal` class, and based on this comparison, it either performs a mathematical operation on the input `x` or raises a `ValueError`. The module is then instantiated and an example input is prepared using PyTorch's `torch.randn` function.",
            "total_tokens": 339,
            "latency": 4.226153,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines a PyTorch neural network module named SpecializedAttribute, which includes custom attributes and logic based on an Animal enum. The module's forward method performs mathematical operations on input tensors, using the 'a' attribute and the Animal enum for conditional behavior. Additionally, the file includes sample input data and instantiates the model for potential use in testing or demonstration.",
            "total_tokens": 297,
            "latency": 1.032716,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_export/db/examples/specialized_attribute.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a PyTorch module `SpecializedAttribute`. The module's `forward` method checks if a string attribute `a` matches the value of the `Animal.COW` enum member. Based on this check, it either performs a specific tensor operation on the input or raises a `ValueError`.",
            "total_tokens": 615,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.111158
        },
        "gemini-2.5": {
            "file_name": "/torch/_export/db/examples/specialized_attribute.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a simple PyTorch module `SpecializedAttribute` with two attributes, `a` (string) and `b` (integer). The module's `forward` method performs a calculation (`x*x + b`) only if the attribute `a` matches the value of an `Animal` enum member (`COW`), otherwise raising an error. An example instantiation is also provided.",
            "total_tokens": 658,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.595114
        }
    },
    "/torch/fx/passes/backends/cudagraphs.py": {
        "nova-lite-v1": {
            "file_summary": "The `cudagraphs.py` file provides functionality to partition a PyTorch FX graph into sub-GraphModules that can be executed within CUDA graphs, ensuring that all operations within a subgraph involve CUDA tensors only. It defines a `CudaGraphsSupport` class to determine if a node in the graph is supported for CUDA graph execution and a `partition_cudagraphs` function to partition the graph accordingly. This is useful for optimizing the execution of PyTorch models on CUDA-enabled hardware by leveraging CUDA graph capture and replay.",
            "total_tokens": 771,
            "latency": 1.39439,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"cudagraphs.py\" is to provide support for partitioning an FX graph into sub-GraphModules that can be validly run under CUDA graphs. This is achieved through the \"partition_cudagraphs\" function, which takes a GraphModule and a list of inputs as arguments. The function first propagates fake tensors through the inputs, then creates a CudaGraphsSupport object to determine if a node is supported for CUDA graphs. It then uses a CapabilityBasedPartitioner to propose partitions based on the supported operations and fuses the partitions into a single fused graph. The file also includes imports from various torch modules and defines a CudaGraphsSupport class that extends the OperatorSupport class.",
            "total_tokens": 156,
            "latency": 3.509733,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python code defines functionality for partitioning PyTorch FX computational graphs into subgraphs that can be efficiently executed as CUDA graphs. It checks node compatibility by verifying the presence of CUDA-specific operations and ensures all operands and results involve CUDA tensors. The `partition_cudagraphs` function performs partitioning based on identified CUDA-compatible nodes using the FX graph partitioning utilities provided by PyTorch.",
            "total_tokens": 644,
            "latency": 3.961588,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided Python file named 'cudagraphs.py' defines a class 'CudaGraphsSupport' that checks if certain nodes in a PyTorch FX graph support CUDA operations based on predefined conditions. Additionally, it includes a function 'partition_cudagraphs' that partitions an FX graph into sub-GraphModules that can be run under CUDA graphs only if all operations involve CUDA tensors. The code aims to optimize the execution of PyTorch graphs for CUDA compatibility by utilizing capabilities of GPU acceleration.",
            "total_tokens": 667,
            "latency": 1.166227,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"cudagraphs.py\" file is to partition an FX graph into sub-GraphModules that can be run under CUDA graphs. It defines a CudaGraphsSupport class to determine which nodes are supported for CUDA graph execution, and provides a partition_cudagraphs function that uses this support class to propose and fuse partitions of the graph that are compatible with CUDA graph execution.",
            "total_tokens": 824,
            "latency": 3.168677,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "nova-pro-v1": {
            "file_summary": "The `cudagraphs.py` file defines a class `CudaGraphsSupport` to check if a given node in a Torch FX graph is compatible with CUDA graphs, and a function `partition_cudagraphs` to partition an FX graph into sub-GraphModules that can be run under CUDA graphs, ensuring all operations involve CUDA tensors only.",
            "total_tokens": 733,
            "latency": 1.253375,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4o": {
            "file_summary": "The Python file `cudagraphs.py` defines functionality for partitioning PyTorch FX graphs into subgraphs that are compatible with CUDA graph execution. It does this by validating nodes for CUDA compatibility and ensuring that all operations within a subgraph involve CUDA tensors. The file provides the class `CudaGraphsSupport` for defining compatibility checks and the function `partition_cudagraphs` to apply these checks and generate fused graph modules.",
            "total_tokens": 652,
            "latency": 1.607722,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to support the creation of CUDA graphs in PyTorch by partitioning a given FX graph into subgraphs that can be executed on a CUDA device. The `CudaGraphsSupport` class checks if a node in the graph is supported for CUDA execution, and the `partition_cudagraphs` function uses this class to partition the graph into valid CUDA subgraphs. The partitioned subgraphs can then be executed on a CUDA device, potentially improving performance.",
            "total_tokens": 671,
            "latency": 4.891373,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4": {
            "file_summary": "The Python file 'cudagraphs.py' defines functionality to manipulate CUDA-compatible graphs within PyTorch, specifically focusing on determining support for CUDA graph execution and partitioning FX graphs based on CUDA tensor operations. The class `CudaGraphsSupport` provides methods to check if nodes in a graph are supported based on the operations involved and whether they use CUDA tensors. The function `partition_cudagraphs` then utilizes this class to partition a given PyTorch FX graph into subgraphs that are exclusively composed of operations involving CUDA tensors, preparing them for optimized CUDA graph execution.",
            "total_tokens": 682,
            "latency": 3.760483,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `cudagraphs.py` file is to partition a PyTorch FX graph into sub-graphs that can be executed on a CUDA device, ensuring that all operations within each sub-graph only involve CUDA tensors. The `CudaGraphsSupport` class checks if a node in the graph is supported for CUDA execution, and the `partition_cudagraphs` function uses this class to partition the graph into valid CUDA sub-graphs. The partitioning process involves propagating fake tensor information, identifying supported operations, and fusing the resulting partitions into a single graph.",
            "total_tokens": 692,
            "latency": 2.76638,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.1": {
            "file_summary": "The file defines functionality to partition a PyTorch FX computation graph into subgraphs that are safe to execute under CUDA graphs, ensuring that all operations within each subgraph operate only on CUDA tensors. It introduces a custom OperatorSupport class (`CudaGraphsSupport`) to determine CUDA compatibility of nodes, and provides a `partition_cudagraphs` function that uses this support logic to partition and fuse FX GraphModules accordingly. This enables safe and efficient execution of PyTorch graphs using CUDA's graph capture and replay mechanism.",
            "total_tokens": 667,
            "latency": 1.494062,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/fx/passes/backends/cudagraphs.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines utilities for partitioning PyTorch FX graphs to identify subgraphs suitable for execution under CUDA graphs. It includes a `CudaGraphsSupport` class to determine if an operation and its associated tensors are exclusively on the CUDA device. The `partition_cudagraphs` function uses this support logic to split an input graph into submodules containing only CUDA-compatible operations.",
            "total_tokens": 1280,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.034867
        },
        "gemini-2.5": {
            "file_name": "/torch/fx/passes/backends/cudagraphs.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines logic to partition a PyTorch FX graph based on CUDA compatibility. It introduces a `CudaGraphsSupport` class to check if individual graph nodes operate exclusively on CUDA tensors. The `partition_cudagraphs` function then uses this check with FX's `CapabilityBasedPartitioner` to group consecutive CUDA-compatible operations into subgraphs, preparing the model for potential acceleration with CUDA graphs.",
            "total_tokens": 1523,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 10.302643
        }
    },
    "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `handlers.py` file is to provide a function `get_subprocess_handler` that creates an instance of `SubprocessHandler` from the `torch.distributed.elastic.multiprocessing` module. This function is used to configure and initialize a subprocess handler for distributed training, allowing for the specification of the entry point, arguments, environment variables, and standard output/error paths.",
            "total_tokens": 363,
            "latency": 0.863668,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the \"handlers.py\" file is to provide a function called \"get_subprocess_handler\" that returns an instance of the \"SubprocessHandler\" class. This function takes in parameters such as the entrypoint, arguments, environment variables, stdout, stderr, and local rank id, and uses them to initialize a \"SubprocessHandler\" object. The \"SubprocessHandler\" class is imported from the \"torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler\" module.",
            "total_tokens": 113,
            "latency": 2.64282,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines a function `get_subprocess_handler` that returns a configured instance of the `SubprocessHandler` class imported from `torch.distributed.elastic`. This handler facilitates managing subprocesses by specifying an entry point, arguments, environment variables, standard output/error redirections, and a local rank ID. Overall, the primary purpose of this file is to abstract and simplify subprocess creation and management specifically tailored for distributed multiprocessing tasks.",
            "total_tokens": 351,
            "latency": 3.37534,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The `handlers.py` file defines a function `get_subprocess_handler` that creates an instance of `SubprocessHandler` from the `torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler` module. The function takes parameters like entrypoint, arguments, environment variables, stdout and stderr paths, and local rank id, and returns an initialized SubprocessHandler. The main functionality of this file is to provide a wrapper function for creating and configuring subprocess handlers for distributed processing using PyTorch.",
            "total_tokens": 366,
            "latency": 1.279778,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"handlers.py\" file is to provide a function called `get_subprocess_handler` that creates and returns a `SubprocessHandler` object. This function serves as a wrapper or factory method for initializing a `SubprocessHandler` with specified parameters, likely used for managing subprocesses in a distributed computing environment.",
            "total_tokens": 385,
            "latency": 2.206299,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `handlers.py` file provides a function to instantiate a `SubprocessHandler` object from the `torch.distributed.elastic.multiprocessing.subprocess_handler` module, which is likely used for managing subprocesses in a distributed computing environment. The main functionality is encapsulated in the `get_subprocess_handler` function, which takes several parameters to configure the subprocess handler.",
            "total_tokens": 359,
            "latency": 1.126139,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The file `handlers.py` defines a utility function `get_subprocess_handler` that initializes and returns an instance of `SubprocessHandler` from PyTorch's distributed elastic module. This handler is designed to manage subprocess execution with specified entrypoints, arguments, environmental variables, and I/O configurations. Its purpose is likely to support distributed or parallel computation in machine learning workflows.",
            "total_tokens": 338,
            "latency": 1.297442,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file, `handlers.py`, is to provide a function `get_subprocess_handler` that returns an instance of `SubprocessHandler` from the `torch.distributed.elastic.multiprocessing.subprocess_handler` module. This function takes several parameters, including the entry point, arguments, environment variables, and output streams, to configure the subprocess handler. The returned `SubprocessHandler` instance is likely used to manage a subprocess in a distributed computing environment.",
            "total_tokens": 367,
            "latency": 3.762742,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The 'handlers.py' file defines a function `get_subprocess_handler` used to set up and return a `SubprocessHandler` instance, which is responsible for handling subprocesses in a parallel computing environment. This function specifies several parameters such as the entrypoint, arguments, environment variables, outputs for `stdout` and `stderr`, and a `local_rank_id`, which are then utilized to initialize the `SubprocessHandler` object. The main purpose of this module is to facilitate managing subprocesses specifically tailored for distributed computing tasks, likely within the context of using the PyTorch framework.",
            "total_tokens": 383,
            "latency": 2.70314,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `handlers.py` file is to provide a function `get_subprocess_handler` that returns a `SubprocessHandler` object, which is used to manage subprocesses in a distributed computing environment. This function takes in several parameters, including the entrypoint, arguments, environment variables, and output streams, to configure the subprocess handler. The file appears to be part of a larger distributed computing framework, likely PyTorch, and is designed to facilitate the creation and management of subprocesses in a distributed setting.",
            "total_tokens": 377,
            "latency": 1.445281,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines a utility function get_subprocess_handler that creates and returns an instance of SubprocessHandler, given parameters such as the entrypoint, arguments, environment variables, output/error streams, and a local rank ID. It serves as a thin wrapper over the SubprocessHandler class imported from torch.distributed.elastic, likely to standardize or simplify subprocess handler instantiation. The file is part of a distributed or multiprocessing framework, potentially used for launching and managing distributed worker processes.",
            "total_tokens": 359,
            "latency": 1.49869,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a factory function `get_subprocess_handler` which simplifies the creation of `SubprocessHandler` objects. It takes configuration parameters like entrypoint, arguments, environment variables, and standard stream redirection paths as input. The primary purpose is to instantiate and return a `SubprocessHandler` configured for managing a worker subprocess within the PyTorch Elastic distributed training framework.",
            "total_tokens": 881,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.497721
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a factory function `get_subprocess_handler`. This function creates and returns an instance of the `SubprocessHandler` class. It initializes the handler with parameters specifying the subprocess to manage, such as its entrypoint, arguments, environment, and standard I/O paths.",
            "total_tokens": 912,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.509651
        }
    },
    "/torch/_inductor/autoheuristic/autoheuristic.py": {
        "nova-lite-v1": {
            "file_summary": "The `autoheuristic.py` file defines a framework for collecting data, learning heuristics, and generating code based on heuristics for optimizing kernel choices in PyTorch. It includes classes for handling feedback, managing metadata, and implementing heuristic selection algorithms. The main functionality is to facilitate the collection of performance data for different kernel choices and use this data to inform and improve heuristic decision-making for selecting the most efficient kernel operations.",
            "total_tokens": 3071,
            "latency": 1.091094,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"autoheuristic.py\" is to provide a framework for collecting data, learning a heuristic, and generating the heuristic to code. The framework is implemented through the AutoHeuristic class, which allows one to collect data for a choice and provides methods to get the chosen option based on the value of autoheuristic_use. The class also includes methods to get the top k choices, get collected feedback, and save data to a log file. Additionally, the file includes a subclass of AutoHeuristic called AutoHeuristicSelectAlgorithm, which is specifically designed for kernel choice selection.",
            "total_tokens": 128,
            "latency": 3.189935,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The given Python file implements an \"AutoHeuristic\" framework integrated into PyTorch, designed to collect data for heuristics, learn decision models (such as regression trees), and select optimal choices for certain operations, particularly GPU-related computations. Its main goal is to dynamically select optimal kernel and operation choices by learning from collected performance feedback data and contexts, creating heuristics to improve PyTorch's inductor performance. This functionality is encapsulated primarily in the \"AutoHeuristic\" and \"AutoHeuristicSelectAlgorithm\" classes, capable of decision-making and logging feedback for heuristic optimization.",
            "total_tokens": 2620,
            "latency": 6.00396,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the \"autoheuristic.py\" file is to implement classes and methods for the AutoHeuristic framework in PyTorch. The file provides classes such as \"LocalFeedback\", \"InconsistentMetadata\", \"AutoHeuristic\", and \"AutoHeuristicSelectAlgorithm\" that allow for collecting data, learning heuristics, selecting algorithms, and providing feedback based on choices. It also includes methods for saving and logging data, handling preconditions, and making decisions based on learned heuristics within the AutoHeuristic framework.",
            "total_tokens": 2613,
            "latency": 1.23924,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"autoheuristic.py\" file is to implement an AutoHeuristic framework for collecting data, learning heuristics, and making decisions in PyTorch's inductor module. It provides classes and methods for managing choices, feedback, and context in auto-tuning scenarios, allowing for data collection, heuristic learning, and decision-making based on learned models. The file also includes specialized classes for algorithm selection and supports both local and global feedback mechanisms.",
            "total_tokens": 3421,
            "latency": 4.375328,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `autoheuristic.py` file defines a framework for collecting data, learning heuristics, and generating heuristic code. It includes classes for providing local feedback, handling metadata inconsistencies, and implementing the core AutoHeuristic functionality, which allows for data collection and heuristic decision-making. Additionally, it provides a specialized subclass for selecting algorithms based on collected data.",
            "total_tokens": 3058,
            "latency": 1.119958,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `autoheuristic.py` file implements a framework for AutoHeuristic, which is used to collect data, train heuristics (such as regression trees), and make decisions in scenarios with multiple choices. It provides mechanisms for logging feedback on choices, managing metadata consistency, and applying learned heuristics to select optimal decisions or kernel algorithms, particularly within a GPU-based environment in PyTorch's inductor module. Additionally, it includes custom functionality for algorithm selection and feedback registration for measuring kernel performance.",
            "total_tokens": 2603,
            "latency": 1.614293,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the file is to implement the AutoHeuristic class, which is a framework for collecting data, learning a heuristic, and generating the heuristic to code. The class allows for collecting data, learning a heuristic, and making decisions based on the learned heuristic. It also provides methods for getting the chosen option, getting the top k choices, and saving data. Additionally, the file defines a subclass AutoHeuristicSelectAlgorithm, which is specifically designed for kernel choice selection.",
            "total_tokens": 2607,
            "latency": 4.456353,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file defines a module named 'autoheuristic' which facilitates the collection of data, learning, and deployment of heuristics for decision-making in computational tasks, likely in a machine learning or neural network context using PyTorch. It includes classes for handling local feedback, handling inconsistent metadata exceptions, and the primary AutoHeuristic class which allows data collection for heuristic training. Additionally, there is an AutoHeuristicSelectAlgorithm class, a subclass of AutoHeuristic, tailored for kernel choice selection in computational tasks, supporting functionality to dynamically adjust decisions based on feedback and predefined conditions.",
            "total_tokens": 2622,
            "latency": 4.525094,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement an AutoHeuristic framework, which allows for the collection of data, learning of a heuristic, and generation of the heuristic to code. The framework is designed to work with PyTorch and utilizes a learned heuristic controller to make decisions based on collected data. The file includes classes such as `AutoHeuristic` and `AutoHeuristicSelectAlgorithm` that provide methods for collecting feedback, making choices, and saving data to a log file.",
            "total_tokens": 2611,
            "latency": 1.662358,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file autoheuristic.py implements a framework for collecting runtime feedback, training, and applying learned heuristics\u2014such as regression trees\u2014for choice selection in PyTorch's Inductor backend. It provides mechanisms to record and serialize data about algorithmic choices and their associated feedback (e.g., runtime or performance), and then allows learned models to guide future choices, with facilities for caching, device-specific logging, and context augmentation. Additionally, a specialized subclass supports gathering feedback specifically for kernel or algorithm selection scenarios, integrating with autotuning infrastructure.",
            "total_tokens": 2611,
            "latency": 1.699277,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/autoheuristic/autoheuristic.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `AutoHeuristic` class, a framework designed to collect data for training and using learned heuristics within PyTorch Inductor. It captures context, available choices, and performance feedback for specific operations, logging this data to a file per device. A specialized subclass, `AutoHeuristicSelectAlgorithm`, applies this framework specifically to the task of selecting optimal kernel algorithms.",
            "total_tokens": 3670,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.626552
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/autoheuristic/autoheuristic.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This file defines the `AutoHeuristic` framework within PyTorch Inductor, designed to automate decision-making processes like kernel selection. It enables the collection of performance data associated with different computational choices and their context. This collected data can then be used to learn and apply heuristics, allowing the system to make optimized, data-driven choices instead of relying solely on default fallbacks.",
            "total_tokens": 3527,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.232708
        }
    },
    "/torch/nn/modules/instancenorm.py": {
        "nova-lite-v1": {
            "file_summary": "The `instancenorm.py` file defines various instance normalization layers for different dimensions (1D, 2D, and 3D) using PyTorch. These layers include standard instance normalization and lazy instance normalization, which defers the initialization of certain parameters until the first forward pass. The normalization process involves computing the mean and variance for each instance separately and normalizing the input data accordingly.",
            "total_tokens": 5634,
            "latency": 1.285546,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to define the InstanceNorm1d, InstanceNorm2d, and InstanceNorm3d classes, which are used to apply instance normalization to input data. The classes take in various parameters such as num_features, eps, momentum, affine, and track_running_stats, and use these parameters to normalize the input data. The classes also include methods for checking input dimensions and handling input with no batch dimension. Additionally, the file defines LazyInstanceNorm1d, LazyInstanceNorm2d, and LazyInstanceNorm3d classes, which are similar to the InstanceNorm classes but with lazy initialization of the num_features argument.",
            "total_tokens": 150,
            "latency": 4.026034,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python code implements Instance Normalization modules (InstanceNorm1d, InstanceNorm2d, InstanceNorm3d, and their lazy initialization variants) for PyTorch. These normalization modules standardize inputs across each instance individually, often used in neural networks for style transfer and visual tasks. It includes options for learnable affine transformations and tracking running statistics for evaluation.",
            "total_tokens": 5154,
            "latency": 5.951022,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file, 'instancenorm.py', defines classes for instance normalization operations in PyTorch. It includes classes for InstanceNorm1d, InstanceNorm2d, InstanceNorm3d, LazyInstanceNorm1d, LazyInstanceNorm2d, and LazyInstanceNorm3d. These classes implement instance normalization calculations and handle input validation and lazy initialization of parameters. Instance normalization is applied on different-dimensional inputs to normalize the mean and standard deviation per dimension separately, providing functionality similar to batch normalization but operating on individual samples or channels.",
            "total_tokens": 5170,
            "latency": 1.594334,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"instancenorm.py\" file is to implement Instance Normalization layers for 1D, 2D, and 3D inputs in PyTorch. It defines classes for InstanceNorm1d, InstanceNorm2d, InstanceNorm3d, and their lazy counterparts, which normalize input features across spatial dimensions independently for each channel and each sample in a batch. The file also includes functionality for handling running statistics, affine transformations, and various input dimensions.",
            "total_tokens": 6029,
            "latency": 4.269072,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `instancenorm.py` file is to implement instance normalization layers for 1D, 2D, and 3D inputs in PyTorch, including both standard and lazy initialization versions. These layers normalize the input data per instance and per dimension, optionally with learnable affine parameters.",
            "total_tokens": 5619,
            "latency": 1.204012,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `instancenorm.py` file implements various types of Instance Normalization layers in PyTorch, including 1D, 2D, and 3D, as well as their lazy initialization counterparts. These layers normalize inputs per instance and per feature dimension, allowing for improved convergence and stylization in neural networks. They support options like learnable affine parameters, tracking running statistics, and handling unbatchable inputs for flexibility in training and inference.",
            "total_tokens": 5171,
            "latency": 1.917756,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to implement instance normalization layers in PyTorch, specifically for 1D, 2D, and 3D inputs. Instance normalization is a technique used in deep learning to normalize the input data for each instance (i.e., each sample in a batch) separately, rather than normalizing across the entire batch. The file defines several classes, including `InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d`, and their lazy initialization counterparts, which inherit from a base class `_InstanceNorm`. These classes implement the instance normalization operation and provide options for learnable affine parameters, tracking running statistics, and more.",
            "total_tokens": 5202,
            "latency": 6.514301,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The provided Python file, \"instancenorm.py\", defines several classes that implement different versions of Instance Normalization (InstanceNorm1d, InstanceNorm2d, InstanceNorm3d) and their respective lazy initialization counterparts in a neural network using the PyTorch library. These layers are used to apply normalization on a per-instance basis (different from Batch Normalization which normalizes across the entire batch), suitable for both training and evaluation modes, with optional affine (learned parameters) and tracking of running statistics. The classes are built upon abstract base classes which ensure implementation of necessary methods and use PyTorch functionalities to adjust the data distribution of inputs for stabilizing and speeding up the training of neural networks.",
            "total_tokens": 5203,
            "latency": 6.72573,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of this Python file is to implement instance normalization layers for PyTorch, including `InstanceNorm1d`, `InstanceNorm2d`, and `InstanceNorm3d`, which apply instance normalization to 2D, 3D, and 5D inputs, respectively. These layers can be used for tasks such as image and video processing, and they provide options for learnable affine parameters and tracking running statistics. Additionally, the file also defines lazy initialization versions of these layers, `LazyInstanceNorm1d`, `LazyInstanceNorm2d`, and `LazyInstanceNorm3d`, which infer the `num_features` argument from the input size.",
            "total_tokens": 5203,
            "latency": 2.246698,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "This file implements PyTorch's instance normalization layers for 1D, 2D, and 3D inputs, including both standard and \"lazy\" variants that infer the number of features at runtime. Instance normalization normalizes input data across each instance and channel, with options for learnable affine parameters and tracking running statistics. The file defines the core logic, input shape validation, and module inheritance structure necessary for these layers to work within the PyTorch framework.",
            "total_tokens": 5173,
            "latency": 2.926177,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/modules/instancenorm.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines PyTorch modules for Instance Normalization, a technique that normalizes input data channel-wise for each sample independently. It provides implementations for 1D, 2D, and 3D inputs, including standard and \"lazy\" versions that infer the number of features upon the first forward pass. These modules are commonly used in deep learning, particularly for tasks like image style transfer.",
            "total_tokens": 5987,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.480252
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/modules/instancenorm.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines PyTorch modules for applying Instance Normalization to 1D, 2D, and 3D input tensors. It includes a base class `_InstanceNorm` containing the core normalization logic and specific implementations (`InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d`) for different input dimensions. Additionally, it provides \"lazy\" versions (`LazyInstanceNorm1d`, `LazyInstanceNorm2d`, `LazyInstanceNorm3d`) that automatically infer the number of features from the input data during the first forward pass.",
            "total_tokens": 5977,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.308857
        }
    },
    "/tools/linter/adapters/flake8_linter.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"flake8_linter.py\" file is to serve as a wrapper around the Flake8 linter tool, providing additional features such as retrying failed commands, mapping specific lint codes to custom severity levels, and outputting results in JSON format. It allows users to specify custom severity levels for certain lint codes, retry failed commands, and provides verbose logging options. The script takes one or more file paths as input and outputs the linting results in JSON format.",
            "total_tokens": 3814,
            "latency": 1.241572,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to serve as a wrapper for the Flake8 linter. It allows users to specify the severity level of certain linting errors, retry timed out Flake8 commands, and set the path to additional Flake8 plugins. The script takes a list of filenames as input, runs Flake8 on them, and outputs the linting results in JSON format. It also provides the documentation URL for each linting error. The script uses the argparse module for command-line argument parsing and the subprocess module to run Flake8. It also includes a regular expression pattern to parse the output of Flake8 and extract the relevant information. The script is designed to be used as a standalone command-line tool.",
            "total_tokens": 160,
            "latency": 4.052166,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file acts as a wrapper around the \"flake8\" linter tool, executing Python linting commands on specified files, parsing linting results, categorizing them by severity (error, warning, advice, or disabled), and associating error codes with their respective documentation URLs. It supports additional functionalities such as configurable retry logic, overriding default severity for specific error codes, and enhancing logging output for troubleshooting purposes. It reports linting issues in a structured JSON format.",
            "total_tokens": 3166,
            "latency": 5.117224,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file \"flake8_linter.py\" is a script that serves as a wrapper for the Flake8 linter. It defines classes like `LintSeverity` and `LintMessage` for handling linting results, provides functions for running Flake8 commands, determining severity of lint issues, and checking files for linting errors based on defined severity levels. The main functionality of the script is to parse user input to customize linting severity levels, run Flake8 on specified files, and output linting results in JSON format.",
            "total_tokens": 3157,
            "latency": 1.50193,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"flake8_linter.py\" file is to serve as a wrapper for the Flake8 linter, providing a custom interface to run Flake8 on Python files and process its output. It parses Flake8 results, categorizes issues by severity, and outputs lint messages in a structured JSON format. The script also includes features like retrying on timeouts and mapping custom severities to specific Flake8 codes.",
            "total_tokens": 3818,
            "latency": 3.841842,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `flake8_linter.py` script is a wrapper for the Flake8 linter that processes Python files, captures linting results, and outputs them in a structured JSON format. It allows customization of severity levels for specific lint codes and retries failed linting commands.",
            "total_tokens": 3772,
            "latency": 0.963202,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `flake8_linter.py` script is a Flake8 wrapper utility designed to lint Python files by identifying potential code issues and returning detailed messages including file paths, line numbers, error codes, and descriptions. It supports configurable retry mechanisms for timed out linting processes, maps lint error codes to severities (error, warning, or advice), and provides links to relevant documentation for specific issues. Additionally, it handles custom configurations for Flake8 plugins and logs outputs in a structured JSON format for further processing.",
            "total_tokens": 3174,
            "latency": 4.97613,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to serve as a wrapper for the Flake8 linter, which checks Python code for style and syntax errors. The script takes in command-line arguments, including file paths to lint, and outputs linting results in JSON format. It also allows for customization of linting severity and retries for timed-out Flake8 commands.",
            "total_tokens": 3128,
            "latency": 3.70374,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The provided Python file, named \"flake8_linter.py,\" serves as a custom wrapper for the flake8 linting tool. It enhances the linting process by allowing retries on failures, custom severity levels for linting issues, and providing additional documentation links for specific linting error codes. The main functionality includes parsing command-line arguments to configure linting options, executing the flake8 command on specified files, and handling and formatting lint-related output based on severity and retries configuration.",
            "total_tokens": 3148,
            "latency": 2.938107,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of this Python file is to serve as a wrapper for the flake8 linter, allowing for customizable severity levels and retries for timed-out commands. It takes in a list of filenames to lint, checks them using flake8, and outputs lint messages in JSON format, including information such as the file path, line number, and severity of the issue. The script also supports additional plugins and allows for verbose logging and customizable severity mappings.",
            "total_tokens": 3149,
            "latency": 1.834428,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file flake8_linter.py is a command-line Python script that wraps the flake8 static code analysis tool, allowing users to lint Python files while customizing plugin paths, error severities, and retries for timeouts. It parses and formats flake8 output into structured messages with additional metadata (such as documentation URLs and severity levels) and outputs the results as JSON objects. The script is designed to be robust, with error handling, logging, and support for environment-based plugin configuration.",
            "total_tokens": 3170,
            "latency": 1.615534,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/linter/adapters/flake8_linter.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script is a wrapper around the `flake8` linter, designed to execute it on specified files. It parses the standard output of the `flake8` command using a regular expression, mapping the detected issues to structured `LintMessage` objects. The script assigns custom severities based on issue codes and includes documentation links before outputting the results as JSON.",
            "total_tokens": 4538,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.034994
        },
        "gemini-2.5": {
            "file_name": "/tools/linter/adapters/flake8_linter.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script serves as a wrapper around the `flake8` linter, executing it on specified Python files with support for custom plugin paths and retries on timeouts. It parses the raw output from `flake8` using regular expressions, classifying each reported issue into severity levels like error, warning, or advice based on the issue code. Finally, the script formats each lint message into a structured JSON object and prints it to standard output.",
            "total_tokens": 4370,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.425343
        }
    },
    "/torch/_functorch/benchmark_utils.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"benchmark_utils.py\" file is to provide utilities for benchmarking GPU utilization and the percentage of time spent on specific operations (like matrix multiplication and convolution) in PyTorch models. It includes functions to profile the execution of a given function, dump the profiling results into a Chrome trace file, and compute the GPU utilization metrics from the trace data. The file also includes helper functions to synchronize GPU operations and filter specific events from the trace.",
            "total_tokens": 1956,
            "latency": 1.282652,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"benchmark_utils.py\" is to provide a set of functions for benchmarking the GPU utilization and the percentage of time spent on matmul and convolution operations. The file includes functions for synchronizing the GPU, dumping the chrome trace, getting the chrome trace events, sorting the GPU events, getting the duration of the GPU events, getting the sorted GPU mm conv events, computing the GPU utilization, and benchmarking the GPU utilization. The file also includes a function for creating a directory if it does not exist.",
            "total_tokens": 113,
            "latency": 2.75112,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file contains utilities to benchmark GPU utilization when running PyTorch functions, specifically measuring total GPU usage and the percentage of time spent on matrix multiplication (matmul) and convolution operations. It leverages the PyTorch profiler to collect performance metrics as Chrome trace files and computes GPU utilization statistics from these trace outputs. The main functionality is encapsulated in the `benchmark_utilization()` method, which automates profiling, data extraction, and utilization analysis.",
            "total_tokens": 1633,
            "latency": 4.035805,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the 'benchmark_utils.py' file is to provide functions for benchmarking GPU utilization and the percentage of time spent on matrix multiplication and convolution operations using PyTorch profiling tools. The file includes functions to dump Chrome trace data of running a given function, analyze the trace data to compute GPU utilization metrics, and benchmark GPU utilization and operation percentages based on the trace data. Additionally, it offers a function 'benchmark_utilization' that combines these features to benchmark GPU utilization and operation percentages of a specified function with given inputs.",
            "total_tokens": 1645,
            "latency": 1.397907,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"benchmark_utils.py\" file is to provide utilities for benchmarking and profiling Python functions, particularly those using PyTorch and CUDA. It includes functions for generating Chrome trace files, analyzing GPU events, calculating GPU utilization, and measuring the time spent on matrix multiplication and convolution operations. The file also offers a high-level benchmarking function that combines these utilities to assess the performance of given functions with GPU operations.",
            "total_tokens": 2095,
            "latency": 5.199457,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `benchmark_utils.py` file provides utilities for benchmarking GPU utilization and the percentage of time spent on matrix multiplication and convolution operations using PyTorch's profiler. It generates Chrome trace files and computes relevant metrics based on the recorded events.",
            "total_tokens": 1914,
            "latency": 1.144369,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "This Python file provides utilities for benchmarking GPU utilization and analyzing the performance of PyTorch functions based on Chrome trace outputs generated by the PyTorch profiler. It allows users to measure total GPU usage, particularly focusing on operations like matrix multiplication (matmul) and convolution, and saves detailed traces of function executions for further analysis. The main functions include `dump_chrome_trace` for creating trace files, `compute_utilization` for calculating GPU utilization from traces, and `benchmark_utilization`, which integrates these functionalities for end-to-end benchmarking.",
            "total_tokens": 1649,
            "latency": 2.774241,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to benchmark the GPU utilization and time spent on matrix multiplication and convolution operations of a given function `f` with input `input` using the PyTorch profiler. The file provides functions to dump Chrome trace files, compute GPU utilization, and benchmark utilization. The `benchmark_utilization` function is the main entry point, which runs the function `f` with input `input` and returns the GPU utilization and time spent on matrix multiplication and convolution operations.",
            "total_tokens": 1643,
            "latency": 5.376667,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `benchmark_utils.py` provides functionality for profiling and measuring the performance of PyTorch functions on GPUs. It includes utilities to generate and analyze Chrome trace files that detail GPU utilization and percentages of time spent on matrix multiplication (matmul) and convolution operations. Key functions in the file include `dump_chrome_trace` to record the trace, `compute_utilization` to compute overall GPU utilization and time spent on specific operations from the trace, and `benchmark_utilization` which combines these functionalities to evaluate and report performance metrics for provided function calls.",
            "total_tokens": 1650,
            "latency": 3.018811,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `benchmark_utils.py`, is to benchmark and analyze the GPU utilization and performance of PyTorch functions, specifically measuring the time spent on matrix multiplication and convolution operations. The file provides functions to dump Chrome trace files for profiling, compute GPU utilization, and benchmark the utilization of given functions. The `benchmark_utilization` function is the primary entry point, which runs a given function with specified inputs, generates a Chrome trace, and computes the GPU utilization and percentage of time spent on key operations.",
            "total_tokens": 1652,
            "latency": 1.894796,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file benchmark_utils.py provides utility functions for benchmarking PyTorch functions, specifically measuring and analyzing GPU utilization and the percentage of time spent on matrix multiplication (matmul) and convolution operations using PyTorch's profiler and Chrome trace outputs. It automates running a function under profiling with warmups, exports traces for visualization, parses these traces, and computes statistics on GPU and operation-specific utilization. The main interface is benchmark_utilization, which orchestrates profiling, trace file management, and utilization computation for a given function and input.",
            "total_tokens": 1647,
            "latency": 1.501664,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_functorch/benchmark_utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides utilities for benchmarking PyTorch functions, specifically focusing on GPU performance analysis. It uses the PyTorch profiler to generate Chrome trace files while executing a target function. The code then parses these trace files to calculate overall GPU utilization and the percentage of time spent on matrix multiplication and convolution operations.",
            "total_tokens": 2613,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.232077
        },
        "gemini-2.5": {
            "file_name": "/torch/_functorch/benchmark_utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file provides utilities for benchmarking PyTorch functions, focusing on GPU performance analysis. It uses the PyTorch profiler to generate Chrome trace files detailing the execution timeline of operations. The core functionality involves running a given function, capturing its performance trace, and then analyzing this trace to calculate metrics like overall GPU utilization and the percentage of time spent specifically on matrix multiplication and convolution kernels.",
            "total_tokens": 2236,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.107243
        }
    },
    "/torch/xpu/_utils.py": {
        "nova-lite-v1": {
            "file_summary": "The `_utils.py` file defines a function `_get_device_index` that retrieves the index of a device from a given input, which can be a `torch.device` object, a Python integer, or `None`. The function supports optional and CPU device inputs, and it defaults to using `torch._utils._get_device_index` if the input device is not a recognized XPU device. This utility function is intended to help manage device indices in PyTorch, particularly for XPU devices.",
            "total_tokens": 653,
            "latency": 1.234972,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the _utils.py file is to provide a function called _get_device_index, which takes a device object, an optional boolean flag, and an allow_cpu boolean flag as input. This function returns the device index from the given device, which can be a torch.device object, a Python integer, or None. The function checks the type of the device and returns the device index accordingly. If the device is a torch.device object, it checks if it is a XPU device and returns the device index if it is. If the device is a Python integer, it returns the integer as is. If the device is None, it returns the current default XPU device if the optional flag is True. If the device is not a XPU device and the allow_cpu flag is False, it raises a ValueError. If the device is a XPU device without a specified index, it returns the current default XPU device if the optional flag is True. If the device is a XPU device with a specified index, it returns the index. If the device is a CPU device and the allow_cpu flag is True, it returns -1. If the device is a CPU device and the allow_cpu flag is False, it raises a ValueError. If the device is not a XPU or CPU device, it raises a ValueError. The function also checks if the scripting mode is enabled and returns the device index accordingly. If the scripting mode is enabled and the device is a XPU device, it returns the device index. If the scripting mode is enabled and the device is not a XPU device, it raises a ValueError. If the scripting mode is not enabled and the device is a XPU device, it calls the _get_device_index function from torch._utils module to get the device index. If the scripting mode is not enabled and the device is not a XPU device, it raises a ValueError.",
            "total_tokens": 406,
            "latency": 8.348855,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "This Python script defines a helper function \"_get_device_index\", which retrieves the device index (integer identifier) from various forms of device information (integer, string or torch.device) specifically tailored for \"xpu\" or optionally \"cpu\" devices. It includes additional validation to ensure only allowed device types (\"xpu\"/\"cpu\") are processed. The function calls an existing PyTorch utility \"_torch_get_device_index\" for the primary logic, extending its handling by adding specific checks for XPU device types.",
            "total_tokens": 591,
            "latency": 4.948428,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `_utils.py` file is to define a function `_get_device_index` that retrieves the device index based on the input `device`, which can be a Torch device object, a Python integer, or `None`. The function handles different cases like checking if the device is an XPU device, handling CPU devices, and returning the device index accordingly. If the input device is not an integer or None, the function delegates the task to `_torch_get_device_index` function imported from `torch._utils`.",
            "total_tokens": 592,
            "latency": 1.401319,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"_utils.py\" file is to provide a utility function called \"_get_device_index\" that retrieves the device index from various input types (torch.device object, Python integer, or None). This function is designed to handle XPU (presumably a custom or extended processing unit) and CPU devices, with options to allow CPU devices and handle optional device specifications. The file also imports and utilizes a similar function from torch._utils for compatibility and extended functionality.",
            "total_tokens": 664,
            "latency": 2.907892,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `_utils.py` file contains a single function, `_get_device_index`, which determines the device index for a given PyTorch device. This function accepts a device specified as a `torch.device` object, a Python integer, or `None`, and returns the corresponding device index. The function also handles optional parameters to allow for CPU devices and default device selection.",
            "total_tokens": 622,
            "latency": 1.074379,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `_utils.py` file defines a function `_get_device_index` that retrieves the device index based on the input, which can be a `torch.device` object, a Python integer, or `None`. It includes checks for compatibility with XPU devices, optionally allowing CPU devices and handling default XPU devices when the device index is unspecified. The function leverages PyTorch utilities to streamline device management in a Torch-based computational framework.",
            "total_tokens": 577,
            "latency": 2.113496,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to provide a utility function `_get_device_index` that retrieves the device index from a given device, which can be a PyTorch device object, a Python integer, or `None`. The function handles different types of devices, including XPU and CPU, and raises errors if the device is not valid. The function is designed to be used in a PyTorch environment and provides flexibility in handling different device types and indices.",
            "total_tokens": 586,
            "latency": 3.637648,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `_utils.py` provides a utility function `_get_device_index()`, which determines the index of a device specified by the `device` parameter that could either be a torch.device object, Python integer, or None. The function supports conditions where the device can optionally be None or a CPU device, returning the device index or `-1` (in the case of CPU when allowed), based on the passed parameters. Any incorrect device type raises a ValueError, while for valid XPUs and CPUs, it appropriately handles device indices, default behaviors, or directly returns the passed integer index.",
            "total_tokens": 604,
            "latency": 2.486237,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `_utils.py` file is to provide a utility function `_get_device_index` that retrieves the device index from a given device, which can be a torch.device object, a Python integer, or None. This function is designed to handle XPU (Xilinx Programmable Unit) devices and optionally allows CPU devices, returning the device index or a default value based on the input and optional parameters. The function includes error checking to ensure the device type is valid and raises a ValueError if an invalid device is provided.",
            "total_tokens": 600,
            "latency": 1.383291,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file defines a utility function, _get_device_index, which standardizes the extraction of a device index from various types of device representations (integer, string, torch.device) specifically for XPU (and optionally CPU) devices in PyTorch. It also includes compatibility logic for scripting and delegates to PyTorch's internal device index handling when necessary. This function helps ensure robust and consistent device management for code targeting XPU backends.",
            "total_tokens": 576,
            "latency": 1.300469,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/xpu/_utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a function `_get_device_index` that determines the integer index of a specified device. It handles various input types for the device, including integers, strings, `torch.device` objects, and `None`. The function primarily targets 'xpu' devices but can optionally accept 'cpu', ultimately delegating the core logic to `torch._utils._get_device_index`.",
            "total_tokens": 836,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.082826
        },
        "gemini-2.5": {
            "file_name": "/torch/xpu/_utils.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a utility function `_get_device_index` designed to extract the numerical device index from various input types like integers, strings, or `torch.device` objects. It specifically handles logic for XPU devices, optionally allows CPU devices (returning -1), and falls back to an internal PyTorch function for general device index retrieval. The function ensures the input represents a valid device and returns its corresponding integer index.",
            "total_tokens": 909,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.8692
        }
    },
    "/functorch/examples/ensembling/parallel_train.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"parallel_train.py\" file is to demonstrate how to parallelize the training of multiple neural network models on a single GPU using PyTorch's eager-mode `vmap` function. It showcases the implementation of a two-layer MLP classifier, a training step function, and the use of `vmap` to train multiple models in parallel. The file also highlights the need for functional APIs, functional optimizers, and a functional gradient API to achieve similar results as JAX.",
            "total_tokens": 1636,
            "latency": 1.161126,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"parallel_train.py\" is to demonstrate the parallel training of multiple models using eager-mode vmap in PyTorch. The script generates spiral data, defines a two-layer MLP classifier, and implements a training step function. It then trains a single model and verifies that the loss decreases. Next, it initializes multiple models and trains them in parallel using vmap, showing that the loss continues to decrease. The script also highlights the need for random data subsets to prevent overfitting in the ensemble models. The file serves as an example of achieving similar functionality as JAX in PyTorch using additional features such as the NN module functional API, functional optimizers, and a functional grad API.",
            "total_tokens": 152,
            "latency": 3.402441,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file demonstrates how to parallelize training of multiple neural network models simultaneously using PyTorch's functional APIs and eager-mode vectorization (`vmap`). It creates a synthetic spiral dataset, implements a simple multi-layer perceptron (MLP) classifier with teacher-forced gradient calculations, manually optimizes parameters via a functional implementation of SGD, and showcases training multiple models concurrently on the same data points. The method is inspired by a similar functionality originally shown using JAX, aiming to facilitate parallel model ensembling and rapid experimentation on a single hardware device.",
            "total_tokens": 1466,
            "latency": 5.254515,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the \"parallel_train.py\" file is to demonstrate the parallel training of multiple models using PyTorch and the eager-mode vmap functionality. The file includes the creation of spirals for the dataset, definition of a two-layer MLP model and a training function, and showcases the training process for multiple models in parallel while highlighting the importance of avoiding overfitting by using different subsets of data for each model. The file also mentions the adaptations made from a tutorial on Model Ensembling with JAX by Will Whitney and highlights the additional functionalities used in PyTorch compared to JAX for achieving similar results.",
            "total_tokens": 1479,
            "latency": 1.252011,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"parallel_train.py\" file is to demonstrate parallel training of multiple neural network models using PyTorch's vmap functionality. It implements a two-layer MLP classifier for a spiral dataset and showcases how to train multiple models simultaneously using vectorized operations. The file also discusses potential improvements and limitations of the approach, such as the need for data subsampling to prevent overfitting across the ensemble.",
            "total_tokens": 1736,
            "latency": 2.909472,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "nova-pro-v1": {
            "file_summary": "The `parallel_train.py` script demonstrates how to use PyTorch's `vmap` function to parallelize the training of multiple neural network models on a single device. It creates synthetic spiral data, defines a two-layer MLP, and shows how to train multiple models in parallel while highlighting the need for functional APIs and optimizers.",
            "total_tokens": 1602,
            "latency": 0.991466,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4o": {
            "file_summary": "The `parallel_train.py` script demonstrates how to use PyTorch's functional APIs, such as `vmap` and `functional_call`, to parallelize the training of multiple neural network models on the same dataset, inspired by an analogous technique in JAX. It includes functionalities to generate spiral data, define an MLP classifier, and implement gradient-based training (manually mimicking a stochastic gradient descent optimizer), along with showcasing both sequential and parallelized training processes. The script concludes by highlighting PyTorch's current limitations, such as the lack of native functional optimizers, while providing a foundation for further experiments with ensemble models trained on diverse subsets of data.",
            "total_tokens": 1486,
            "latency": 2.635299,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to demonstrate the use of PyTorch's `vmap` function to parallelize the training of multiple neural network models on a single GPU. The file defines a two-layer MLP classifier and a loss function, and then uses `vmap` to train multiple instances of the model in parallel on a dataset of spirals. The file also explores the use of `vmap` to train multiple models on different subsets of the data to prevent overfitting.",
            "total_tokens": 1461,
            "latency": 4.053443,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4": {
            "file_summary": "The Python file `parallel_train.py` is designed to demonstrate parallel training of neural network models on a single GPU using PyTorch, inspired by similar techniques in JAX. It specifically uses a two-layer MLP (Multi-Layer Perceptron) classifier to train on a synthetic dataset (spiral data) and implements various functionalities including model initialization, ensembling, and parallelism via `vmap`. The file includes a step-by-step guide illustrating the training process for individual models and extends this to parallel training of multiple models, highlighting how different models can be trained simultaneously by vectorizing the training loop.",
            "total_tokens": 1476,
            "latency": 3.294626,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `parallel_train.py`, is to demonstrate the use of PyTorch's `vmap` function to parallelize the training of multiple neural network models on a single GPU. The script defines a simple two-layer MLP classifier and trains it on a spiral dataset, then extends this to train multiple models in parallel using `vmap`. The goal is to show that PyTorch can achieve similar parallelization capabilities as JAX, a library known for its parallelization features.",
            "total_tokens": 1465,
            "latency": 1.4033,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": ""
        },
        "gpt-4.1": {
            "file_summary": "The file parallel_train.py demonstrates how to use PyTorch's functional and vectorized mapping APIs (functorch's vmap and related utilities) to parallelize training of multiple neural network models on synthetic spiral data, inspired by an analogous JAX tutorial. It defines a simple two-layer MLP and shows both single-model and multi-model (ensembled) training, using functional_call and vmap to perform parallel SGD updates across the ensemble. The script concludes by noting that, while the example trains all models on the same dataset for simplicity, further steps would be required to avoid overfitting by introducing data diversity\u2014a topic left as an exercise.",
            "total_tokens": 1484,
            "latency": 1.80284,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/functorch/examples/ensembling/parallel_train.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script demonstrates how to train multiple neural network models in parallel on a single device using PyTorch and its `torch.func` (functorch) library. It defines an MLP model and a training step, then uses `torch.vmap` to apply this step simultaneously to an ensemble of models represented by batched parameters. The goal is to showcase the capability of eager-mode `vmap` for parallelizing training over models, similar to techniques used in JAX.",
            "total_tokens": 1777,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 1.570158
        },
        "gemini-2.5": {
            "file_name": "/functorch/examples/ensembling/parallel_train.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script demonstrates how to train an ensemble of MLP classifiers in parallel on a single device using PyTorch's functional programming features. It utilizes `vmap` to vectorize the training step across multiple model instances, applying the same logic simultaneously to different sets of model parameters. The example trains the models on a synthetic spiral dataset and showcases how PyTorch can achieve parallel model training similar to frameworks like JAX.",
            "total_tokens": 2034,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.187229
        }
    },
    "/tools/gen_vulkan_spv.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"gen_vulkan_spv.py\" file is to generate SPIR-V shader binaries from GLSL source files and create corresponding C++ files for shader registration and dispatch in a Vulkan API. It takes GLSL source files, processes them with given parameters, compiles them into SPIR-V bytecode, and generates C++ header and source files that register these shaders for use in a Vulkan application.",
            "total_tokens": 7105,
            "latency": 1.572992,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to generate SPIR-V binary files and C++ source files from GLSL source files. The file takes in a list of paths to look for GLSL source files, a path to the GLSLC compiler, a temporary directory path, and an output directory path. It also allows for setting a number of key-value pairs for environment variables.\n\nThe file first initializes a SPVGenerator object with the given paths, environment variables, and GLSLC path. It then adds the GLSL source files and template YAML files to the generator object. The generator object then constructs an output map of shader names to their corresponding source GLSL files and shader parameters.\n\nThe file then generates the SPIR-V binary files for each shader in the output map. It does this by preprocessing the GLSL source files with the given shader parameters, and then compiling the preprocessed GLSL files with the GLSLC compiler. The resulting SPIR-V binary files are saved to the temporary directory.\n\nFinally, the file generates the C++ source files. It does this by determining the shader information for each SPIR-V binary file, and then generating the necessary C++ code to register the shader information and dispatch the shaders. The resulting C++ source files are saved to the output directory.\n\nOverall, the file provides a convenient way to generate SPIR-V binary files and C++ source files from GLSL source files, with the ability to customize the environment variables and shader parameters.",
            "total_tokens": 326,
            "latency": 7.80263,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.5": {
            "file_summary": "This Python script preprocesses GLSL shader source files using parameters defined in YAML templates, generates GLSL variant combinations, and compiles them into SPIR-V binaries using an external compiler (glslc). It then packages these binaries into a C++ source file containing embedded SPIR-V shader data arrays. Additionally, the script generates relevant shader metadata and registers these shaders for use with a Vulkan-based pipeline.",
            "total_tokens": 5833,
            "latency": 6.683722,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file is a tool for SPIR-V code generation. It includes functions for creating SPIR-V binaries from GLSL source files, generating shader parameters, and registering shaders for Vulkan. The file also contains functions to parse GLSL source files for shader information such as tile size, layouts, storage types, and registry keys. Additionally, it includes a function to generate C++ files with SPIR-V binary arrays, shader information, and shader registry handling. The main functionality of the file is to process GLSL source files, generate SPIR-V binaries, and create C++ files for shader registration in Vulkan.",
            "total_tokens": 5828,
            "latency": 1.803373,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"gen_vulkan_spv.py\" file is to generate SPIR-V shader code and corresponding C++ files for Vulkan shaders. It processes GLSL source files and YAML configuration files, generates shader variants based on specified parameters, compiles the GLSL shaders to SPIR-V using glslc, and creates C++ files containing shader information and registration code for use in a Vulkan-based application.",
            "total_tokens": 7806,
            "latency": 6.719789,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `gen_vulkan_spv.py` file is to generate SPIR-V binary files from GLSL shader source files and to create corresponding C++ files for shader registration in a Vulkan environment. It processes GLSL files, applies templating and parameterization, compiles them using `glslc`, and generates C++ code to register these shaders.",
            "total_tokens": 7093,
            "latency": 1.669738,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4o": {
            "file_summary": "The script `gen_vulkan_spv.py` is a tool for generating SPIR-V (Shader Portable Intermediate Representation) binary files and accompanying C++ shader registration code from GLSL shader files using template parameters and optional YAML configurations. It facilitates shader variant generation and preprocessing with dynamic substitutions, integrates SPIR-V compilation via an external GLSL compiler, and organizes shader information for Vulkan environments. Additionally, it creates a C++ source file to register the generated shaders and their metadata into a Vulkan shader registry.",
            "total_tokens": 5852,
            "latency": 2.211433,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to generate Vulkan SPIR-V shaders from GLSL source files, and then use these shaders to generate C++ files for registering the shaders in a Vulkan shader registry. The script takes several command-line arguments, including the paths to the GLSL source files, the path to the GLSL compiler, and the output directory for the generated C++ files.",
            "total_tokens": 5788,
            "latency": 5.090415,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4": {
            "file_summary": "The Python script \"gen_vulkan_spv.py\" is designed for generating Vulkan SPIR-V shaders from GLSL (OpenGL Shading Language) source files. It reads shader configurations from YAML files and provides functionalities to preprocess and compile GLSL shaders into SPIR-V format using a specified GLSL compiler. Additionally, the script supports generating C++ source files that integrate with the shader binaries, registering the shaders for specific operations or dispatch keys using a Vulkan API shader registry mechanism. This setup aims to facilitate the use of custom shaders in Vulkan-based applications.",
            "total_tokens": 5814,
            "latency": 4.561784,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of this Python file is to generate Vulkan SPIR-V shaders from GLSL source files and create corresponding C++ files for shader registration. The script takes in command-line arguments for GLSL source file paths, a temporary directory path, an output path, and optional environment variables, and uses these to generate the shaders and C++ files. The generated C++ files include shader binary arrays and registration code for the shaders.",
            "total_tokens": 5797,
            "latency": 3.596583,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS"
        },
        "gpt-4.1": {
            "file_summary": "The file gen_vulkan_spv.py is a code generation tool that automates the process of preprocessing GLSL shader source files (including handling templates and variants defined in YAML), compiling them into SPIR-V binaries using glslc, and generating corresponding C++ files to register the shaders for use in a Vulkan-based pipeline. It parses input directories for shader sources and YAML template definitions, manages environment variables for code generation, invokes compilation, and extracts associated metadata to be included in C++ registries. This script streamlines shader preparation and integration for Vulkan projects with build automation and template variant management.",
            "total_tokens": 5871,
            "latency": 2.111897,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/gen_vulkan_spv.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script reads GLSL shader source files and associated YAML configuration files to generate shader variants. It preprocesses the GLSL using a custom templating engine and compiles the resulting code into SPIR-V binaries using an external GLSL compiler. Finally, it generates C++ source code that embeds these SPIR-V binaries and registers them along with extracted metadata into a C++ shader registry.",
            "total_tokens": 7724,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.414766
        },
        "gemini-2.5": {
            "file_name": "/tools/gen_vulkan_spv.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script processes GLSL shader templates and YAML configuration files to generate multiple Vulkan SPIR-V (SPV) shader variants based on defined parameters. It compiles the resulting GLSL code into SPV binaries using an external compiler (`glslc`) and extracts metadata like descriptor layouts and tile sizes from comments in the source files. Finally, it generates C++ header and source files that embed these SPV binaries as byte arrays and register them along with their metadata for runtime access within a C++ application.",
            "total_tokens": 7925,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 7.26646
        }
    },
    "/torch/_lazy/device_context.py": {
        "Llama3.3": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the `device_context.py` file is to manage device contexts for PyTorch, a popular deep learning framework, using a thread-safe approach. The `DeviceContext` class represents a device context, and the `get_device_context` function retrieves or creates a device context for a given device, with a default device type used if none is specified. The device contexts are stored in a dictionary, `_CONTEXTS`, which is protected by a lock to ensure thread safety.",
            "total_tokens": 363,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.365493
        },
        "titan_premier": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file \"device_context.py\" is a Python file that defines a class called \"DeviceContext\" and a function called \"get_device_context\". The \"DeviceContext\" class is used to manage device contexts, and the \"get_device_context\" function is used to retrieve the device context for a given device or the default device. The file also imports the necessary modules for threading and typing, as well as the \"torch._C._lazy\" module for accessing device-related functionality.\n\nHere are the key takeaways from the file:\n\n* The \"DeviceContext\" class is used to manage device contexts, with a dictionary to store the contexts and a lock to ensure thread safety.\n* The \"get_device_context\" function retrieves the device context for a given device or the default device, creating a new context if necessary.\n* The file imports the necessary modules for threading, typing, and device-related functionality.\n\nOverall, the main functionality of the file is to provide a way to manage device contexts in a thread-safe manner.",
            "total_tokens": 229,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 6.145046
        },
        "nova-pro-v1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The `device_context.py` file defines a `DeviceContext` class and a `get_device_context` function to manage and retrieve device contexts for PyTorch operations, ensuring thread-safe access and reuse of context instances.",
            "total_tokens": 329,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 0.811631
        },
        "gpt-4": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file `device_context.py` defines a class `DeviceContext` and a function `get_device_context`. This function retrieves or creates a `DeviceContext` instance for a specified device, storing each instance in a dictionary with threading locks to handle concurrency. The `DeviceContext` class manages a context related to a specific device (for example, hardware or simulation devices), facilitating the management of device-specific states or configurations.",
            "total_tokens": 343,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 2.127384
        },
        "Claude3.5": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"device_context.py\" file is to manage device contexts in a multi-threaded environment. It provides a DeviceContext class and a get_device_context function that creates or retrieves a DeviceContext object for a given device, using a thread-safe mechanism to ensure proper handling of shared resources across different threads.",
            "total_tokens": 382,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 2.221844
        },
        "gpt-4o": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file \"device_context.py\" provides functionality to manage and retrieve a thread-safe singleton instance of `DeviceContext`, which represents the context associated with a specific device (e.g., CPU, GPU). It ensures that a single context instance is created per device using a lock and maintains these instances in a shared dictionary. Additionally, it leverages a default device type from PyTorch\u2019s lazy module when no device is explicitly specified.",
            "total_tokens": 349,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.810665
        },
        "Llama3.1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the `device_context.py` file is to manage a thread-safe cache of device contexts, where each context is associated with a specific device (e.g. a GPU or CPU). The `get_device_context` function retrieves or creates a device context for a given device, using a default device if none is specified. The device context is stored in a dictionary, which is protected by a lock to ensure thread safety.",
            "total_tokens": 351,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 3.526815
        },
        "gpt-4.1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file defines a thread-safe mechanism for managing and retrieving singleton device context objects associated with computation devices (such as CPU or GPU), using Python threading locks. It provides a DeviceContext class to represent a computational device and a get_device_context() function to retrieve or create (if needed) a DeviceContext for a specified or default device. The implementation ensures only one context per device exists throughout the application's lifetime.",
            "total_tokens": 345,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 1.807766
        },
        "nova-lite-v1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"device_context.py\" file is to manage and provide access to device-specific contexts in a thread-safe manner. It utilizes a dictionary to store instances of `DeviceContext` associated with different devices, ensuring that each device context is created only once. The `get_device_context` function retrieves or creates a `DeviceContext` instance for a specified device, defaulting to the default device type if no device is provided.",
            "total_tokens": 376,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 0.904497
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"device_context.py\" file is to define a class DeviceContext that manages the context of different devices. It provides a method get_device_context() to retrieve or create a DeviceContext object based on the specified device type. The class uses a dictionary to store and reuse DeviceContext objects for each device type, ensuring a single context per device.",
            "total_tokens": 331,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 0.984957
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `DeviceContext` class to represent a specific device type. It provides a thread-safe function, `get_device_context`, which acts as a factory to retrieve a unique `DeviceContext` instance for a given device string. The function caches instances in a dictionary, ensuring that only one `DeviceContext` object exists per device type and returning the existing one if available.",
            "total_tokens": 794,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.41509
        },
        "gpt-4.5": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `DeviceContext` class, which manages and caches device contexts in a thread-safe manner. It provides a global function `get_device_context()` that retrieves or creates a context instance for a specific device (e.g., CPU, GPU), storing it in a shared dictionary to avoid redundant instantiations. The default device type is obtained from `torch._C._lazy` if not explicitly specified.",
            "total_tokens": 348,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 9.055873
        },
        "gemini-2.5": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a thread-safe mechanism for managing and retrieving device-specific context objects (`DeviceContext`). It uses a global dictionary protected by a lock to ensure that only one `DeviceContext` instance is created per unique device string. The `get_device_context` function acts as a factory, returning the existing context for a device or creating a new one, defaulting to the PyTorch lazy tensor default device if none is specified.",
            "total_tokens": 836,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.895201
        }
    },
    "/torch/utils/benchmark/op_fuzzers/spectral.py": {
        "gpt-3.5-turbo": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The given 'spectral.py' file contains a class called 'SpectralOpFuzzer' that extends the 'benchmark.Fuzzer' class. This class is used to generate benchmark parameters for spectral operations, specifically for testing Fast Fourier Transform (FFT) implementations in PyTorch. The parameters include dimensionality of data, shapes for input tensors (including regular sizes for FFT testing), steps for strided memory access, and other tensor properties like data type and memory allocation.",
            "total_tokens": 1008,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.213898
        },
        "gpt-4.1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of spectral.py is to define a fuzz testing utility, SpectralOpFuzzer, for benchmarking spectral (e.g., FFT) operations in PyTorch. It generates varied tensor shapes\u2014including those optimized for fast FFT performance\u2014using parameterized fuzzing, allowing systematic exploration of dimensionality, individual axis sizes, strides, and memory layouts. The generated tensors and parameters are particularly tailored to test and evaluate the efficiency and robustness of PyTorch's spectral operators, emphasizing sizes (\"regular numbers\") that are typically performant for FFT algorithms.",
            "total_tokens": 1023,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 2.596718
        },
        "Llama3.1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of this Python file is to define a class `SpectralOpFuzzer` that inherits from `torch.utils.benchmark.Fuzzer`. This class is designed to generate random inputs for benchmarking spectral operations, such as Fast Fourier Transform (FFT), in PyTorch. The fuzzer generates random tensor shapes, sizes, and strides, with a focus on regular sizes that are optimized for FFT implementations.",
            "total_tokens": 1003,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 3.236405
        },
        "nova-pro-v1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The `spectral.py` file defines a `SpectralOpFuzzer` class for fuzz testing spectral operations (like FFT) in PyTorch. It generates various input tensor shapes and strides to benchmark the performance, with a focus on \"regular\" sizes that are multiples of 2, 3, and 5. The fuzzer utilizes PyTorch's benchmarking utilities to create and test these configurations.",
            "total_tokens": 1083,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 1.158452
        },
        "nova-lite-v1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The `spectral.py` file defines a class `SpectralOpFuzzer` for fuzz testing spectral operations in PyTorch, focusing on generating tensors with specific shapes and strides to test the efficiency and robustness of FFT implementations. It uses a combination of regular and random sizes for tensor dimensions, and includes parameters for controlling the distribution of these sizes, the data type, and whether to use CUDA. The file also includes utility functions for generating power ranges and regular sizes.",
            "total_tokens": 1095,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 0.880591
        },
        "gpt-4o": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `SpectralOpFuzzer`, a utility for creating and benchmarking fuzzed tensors and parameters targeted specifically for spectral operations such as FFT (Fast Fourier Transform). It generates test cases with configurable dimensions, tensor shapes, strides, and probabilities emphasizing \"regular sizes\" that are computationally efficient for FFT. The fuzzer leverages PyTorch's benchmarking utilities to evaluate performance under diverse conditions, focusing on varied tensor configurations and memory access patterns.",
            "total_tokens": 1007,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.609978
        },
        "titan_premier": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file \"spectral.py\" contains a class named \"SpectralOpFuzzer\" that inherits from the \"Fuzzer\" class in the \"torch.utils\" module. This class generates random inputs for benchmarking spectral operations in PyTorch. The class initializes various parameters and tensors, including the dimensionality, shape, and steps of the input tensor, as well as the data type and device (CPU or GPU) for the tensor. The class also includes a list of regular sizes that are commonly used in FFT implementations and are generated using prime factors 2, 3, and 5. The probability of generating a regular size can be controlled by the \"probability_regular\" parameter. Overall, the main functionality of the file is to provide a framework for fuzz testing of spectral operations in PyTorch.",
            "total_tokens": 175,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 4.042059
        },
        "gpt-4": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file `spectral.py` defines a class `SpectralOpFuzzer` for fuzz testing spectral operations (like FFT). It utilizes `torch` and `benchmark` from PyTorch to set up a fuzzing environment where tensor dimensions and shapes can be randomly generated according to specific parameters, including both regular and irregular distribution sizes optimized for FFT performance. Different data types, memory strides, and contingencies for tensor creation on both CPU and GPU are supported by configuring detailed fuzzer parameters.",
            "total_tokens": 1013,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 2.831055
        },
        "Llama3.3": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the provided Python file, `spectral.py`, is to define a class `SpectralOpFuzzer` that inherits from `torch.utils.benchmark.Fuzzer` and is designed to generate random inputs for benchmarking spectral operations, particularly those involving Fast Fourier Transforms (FFTs). The fuzzer generates random tensor shapes, sizes, and strides, with a focus on \"regular\" sizes that are optimized for FFT performance. The class allows for customization of parameters such as data type, CUDA usage, and probability of selecting regular sizes, making it a versatile tool for benchmarking and testing spectral operations in PyTorch.",
            "total_tokens": 1049,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.880654
        },
        "Claude3.5": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"spectral.py\" file is to define a SpectralOpFuzzer class, which is a custom fuzzer for spectral operations in PyTorch. This fuzzer generates random input parameters and tensors for testing spectral operations, with a focus on creating various shapes and sizes, including regular sizes that are optimal for FFT implementations. The file also includes utility functions and constants to support the fuzzer's operation, particularly in generating and handling dimension sizes for the input tensors.",
            "total_tokens": 1261,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 4.148508
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `SpectralOpFuzzer` class, inheriting from `torch.utils.benchmark.Fuzzer`, designed to generate fuzzed inputs for benchmarking spectral operations like FFT. It pre-calculates a list of \"regular\" sizes (factorizable by 2, 3, and 5) known to be efficient for FFT implementations. The fuzzer generates tensors with varying dimensions, shapes (with a preference for these regular sizes), and strides to test performance across different input configurations.",
            "total_tokens": 1414,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.294311
        },
        "gpt-4.5": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The provided Python file defines a \"SpectralOpFuzzer,\" a benchmarking utility built using PyTorch's benchmarking framework to fuzz-test spectral operations (such as FFTs). This fuzzer generates randomized tensor shapes and strides targeting particularly fast (regular) sizes that factorize into powers of primes 2, 3, and 5, as well as various memory layouts to test strided access effectively. Its primary purpose is to systematically stress-test and benchmark the spectral operations' performance in various realistic tensor configurations.",
            "total_tokens": 1017,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 4.359109
        },
        "gemini-2.5": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `SpectralOpFuzzer` class used for benchmarking PyTorch spectral operations like FFT. It generates randomized input tensors with varying dimensions, sizes, and memory layouts (strides) to test performance under diverse conditions. The fuzzer specifically prioritizes generating tensor sizes known as \"regular numbers\" (factors of 2, 3, 5), which are often optimized in FFT implementations.",
            "total_tokens": 1387,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 4.369911
        }
    },
    "/torch/distributed/tensor/_sharding_prop.py": {
        "gpt-4": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The provided Python file \"_sharding_prop.py\" implements and manages sharding strategies and propagation rules for distributed tensor operations in PyTorch. It includes classes and methods for caching strategies locally, registering sharding rules for different tensor operations, propagating tensor metadata dynamically based on operations' sharding strategies, and managing output specifications in a distributed environment. The automation within the file aims to enhance computation by effectively distributing data and computations in a multi-node setup, adapting strategies for tensor operation, and minimizing data redistribution overhead during execution.",
            "total_tokens": 4395,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 3.403851
        },
        "nova-pro-v1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the `_sharding_prop.py` file is to define and manage the propagation of sharding strategies for distributed tensor operations in PyTorch, utilizing caching mechanisms and strategy selection based on cost.",
            "total_tokens": 5352,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 1.345853
        },
        "Llama3.1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of this file is to define a class `ShardingPropagator` that is responsible for propagating sharding information through PyTorch operators. It uses a set of predefined rules and strategies to determine the output sharding of an operator based on its input sharding. The class also provides methods for registering custom sharding propagation rules and strategies for specific operators.",
            "total_tokens": 4373,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 6.225286
        },
        "Llama3.3": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the given Python file `_sharding_prop.py` is to propagate sharding for PyTorch operators, which involves determining how to split and distribute tensors across multiple devices. The `ShardingPropagator` class is responsible for this task, using a combination of predefined rules and strategies to determine the optimal sharding for a given operator. The propagator takes into account factors such as tensor shapes, strides, and device placements to make its decisions.",
            "total_tokens": 4392,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 2.689875
        },
        "gpt-4o": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The `_sharding_prop.py` file implements functionality for propagating and managing tensor sharding in distributed systems using PyTorch's distributed tensor framework. It provides mechanisms for registering sharding rules and strategies for various operators, optimizing tensor reshaping and redistribution, and maintaining metadata such as tensor shape and stride during sharding operations. The main component, `ShardingPropagator`, handles sharding propagation by dynamically adjusting tensor placements and metadata based on defined rules, caching mechanisms, and efficient strategy selection.",
            "total_tokens": 4430,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.456099
        },
        "titan_premier": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the file is to propagate the sharding for an operator given the op schema. It also has methods to register sharding propagation rule for an operator and register a sharding strategy generator for an operator. It also has methods to propagate the tensor metadata and wrap the output spec with the tensor metadata.",
            "total_tokens": 63,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 2.47016
        },
        "nova-lite-v1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a `ShardingPropagator` class that manages the propagation of tensor sharding strategies in distributed tensor operations. It registers rules and strategies for specific operations to determine how tensors should be sharded across different devices in a distributed computing environment. The class provides methods to propagate sharding information for given operation schemas and associate the resulting sharding with tensor metadata.",
            "total_tokens": 5383,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 1.55044
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the provided Python file \"_sharding_prop.py\" is to define classes and methods related to sharding propagation for operators in PyTorch. It includes functionality to register sharding rules and strategies for operators, propagate sharding information for operators based on rules/strategies, adjust shape and stride arguments for certain operations, and handle tensor metadata propagation. The file also contains classes such as `ShardingPropagator` for managing sharding information, `LocalLRUCache` for caching user functions locally, and methods for wrapping output specifications with tensor metadata.",
            "total_tokens": 4406,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.711209
        },
        "Claude3.5": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"_sharding_prop.py\" file is to implement a sharding propagation system for distributed tensor operations in PyTorch. It defines a ShardingPropagator class that handles the registration and execution of sharding propagation rules and strategies for various PyTorch operators, allowing for efficient distribution of tensor computations across multiple devices or machines.",
            "total_tokens": 5859,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 5.596799
        },
        "gpt-4.1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The primary functionality of this file is to define the ShardingPropagator class and its associated logic, which is responsible for propagating sharding strategies and tensor metadata for distributed tensor operations in PyTorch. It provides mechanisms to register sharding propagation rules and operator strategies, automatically select optimal sharding placements, and handle redistributions for a wide variety of tensor operations. The file ensures seamless and efficient manipulation, suggestion, and validation of sharded tensor layouts during distributed execution by supporting both rule-based and strategy-based propagation.",
            "total_tokens": 4434,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 3.204338
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `ShardingPropagator` class, which is responsible for determining the output sharding of PyTorch operations based on the sharding of their inputs. It uses registered rules or strategy functions for specific operators to compute the output sharding and can suggest input redistributions if necessary. The class also handles propagating tensor metadata and adjusting shape/stride arguments for certain operations.",
            "total_tokens": 5754,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.292712
        },
        "gpt-4.5": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file implements a system for sharding propagation in PyTorch distributed tensors, primarily through the class `ShardingPropagator` responsible for registering and applying sharding rules and strategies to different torch operators (`OpOverload` instances). It provides methods for handling tensor metadata propagation (`TensorMeta`), managing caching for efficiency, and automatically adjusting tensor shapes and strides when performing distributed operations. Overall, this module enables efficient, strategy-driven automatic sharding for distributed tensor operations in PyTorch.",
            "total_tokens": 4429,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 10.363015
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines the `ShardingPropagator` class, responsible for determining how sharding (distribution across devices) should be applied to the outputs of PyTorch operations involving distributed tensors (DTensors). It uses registered rules or strategies specific to each operator to calculate the output sharding based on the input tensors' distributed layout and the operation itself. The propagator also handles tensor metadata propagation and can suggest input tensor redistributions if the current sharding is incompatible with the chosen execution strategy.",
            "total_tokens": 6055,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 7.437133
        }
    },
    "/torch/nn/quantizable/modules/activation.py": {
        "nova-pro-v1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The `activation.py` file serves as a compatibility layer during the migration of quantizable modules to the `torch/ao/nn/quantizable` package. It imports the `MultiheadAttention` module from the new location to ensure continued functionality during the transition.",
            "total_tokens": 257,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 0.899444
        },
        "Llama3.1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the file \"activation.py\" is to provide a module for quantizable activation functions, specifically importing the MultiheadAttention class from another module. The file is currently being migrated to a new location and is being kept for compatibility purposes. It does not contain any original functionality, but rather serves as a redirect to the new location.",
            "total_tokens": 271,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 2.843743
        },
        "Llama3.3": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the `activation.py` file is to provide a compatibility layer for quantizable modules, specifically importing the `MultiheadAttention` module from `torch.ao.nn.quantizable.modules.activation`. This file is currently being migrated to `torch/ao/nn/quantizable` and is being kept for backwards compatibility. The file does not contain any new functionality and instead serves as a redirect to the new location of the quantizable modules.",
            "total_tokens": 293,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.36056
        },
        "nova-lite-v1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"activation.py\" file is to provide compatibility for quantizable modules during the migration process to a new location in the PyTorch library. It imports the `MultiheadAttention` class from the new module location, `torch.ao.nn.quantizable.modules.activation`, and serves as a transitional file until the migration is complete. New functionalities should be added to the new module location instead of this file.",
            "total_tokens": 292,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 0.691123
        },
        "gpt-4o": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file `activation.py` serves as a compatibility layer during the migration of quantizable modules to the `torch/ao/nn/quantizable` namespace. It re-exports the `MultiheadAttention` module from the new location to ensure backward compatibility. The file also advises developers to add new functionality directly to the updated namespace while maintaining import statements here.",
            "total_tokens": 269,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.09162
        },
        "gpt-4.1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file activation.py serves as a transitional compatibility module for quantizable neural network components, specifically re-exporting the MultiheadAttention class from torch.ao.nn.quantizable.modules.activation. It includes documentation stating that its contents are being migrated to a new location and advises contributors to add new features to the updated directory while importing them here for backward compatibility. Thus, its main function is to maintain compatibility during the refactoring of quantizable PyTorch modules.",
            "total_tokens": 286,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 3.457431
        },
        "Claude3.5": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file \"activation.py\" serves as a compatibility layer during the migration process of quantizable modules from their current location to \"torch/ao/nn/quantizable\". It imports the MultiheadAttention module from the new location while maintaining backward compatibility for existing code. The file also includes a note advising developers to add new functionalities directly to the appropriate files under the new directory structure.",
            "total_tokens": 301,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 3.042503
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"activation.py\" file is to provide information about Quantizable Modules being migrated to a specific directory in the torch library. It serves as a placeholder for compatibility during the migration process and instructs users to add new entries or functionalities to the appropriate file under the designated directory while ensuring import statements are added to this file. Specifically, it imports the MultiheadAttention module from torch.ao.nn.quantizable.modules.activation.",
            "total_tokens": 283,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.086942
        },
        "titan_premier": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file \"activation.py\" is a Python file that contains the import statement for the MultiheadAttention class from the torch.ao.nn.quantizable.modules.activation module. This file is currently in the process of being migrated to the torch/ao/nn/quantizable directory and is kept here for compatibility purposes. The file also contains a docstring that provides information about the ongoing migration process and instructions for adding new entries or functionalities.",
            "total_tokens": 92,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 3.846073
        },
        "gpt-4": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file \"activation.py\" is involved in the migration of quantizable modules to the new directory `torch/ao/nn/quantizable`. It currently primarily handles legacy support for compatibility during this migration by importing the `MultiheadAttention` module from the new location `torch.ao.nn.quantizable.modules.activation`. Thus, this file acts as a transitional linker to the new modular structure while maintaining backward compatibility.",
            "total_tokens": 279,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 1.909914
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file is part of a migration process for quantizable modules in PyTorch, serving as a compatibility layer. It re-exports modules from their new location to maintain backward compatibility during the transition. Currently, it specifically imports and makes available the `MultiheadAttention` module from `torch.ao.nn.quantizable.modules.activation`.",
            "total_tokens": 555,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.396711
        },
        "gpt-4.5": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The provided Python file acts as a backward compatibility layer for quantizable modules in PyTorch, specifically for activation modules undergoing migration to a new location (`torch/ao/nn/quantizable`). It imports the `MultiheadAttention` module from its new location, enabling a seamless transition for codebases relying on the previous structure. The file also includes instructions for developers to contribute new functionality directly to the target module location.",
            "total_tokens": 280,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 3.207523
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file acts as a compatibility shim within the PyTorch library, specifically for quantizable modules. It facilitates the migration of modules like `MultiheadAttention` to the newer `torch.ao.nn.quantizable` namespace. Its main function is to re-export these migrated modules to maintain backward compatibility for users during the transition period.",
            "total_tokens": 881,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 8.095404
        }
    },
    "/torch/_export/db/examples/class_method.py": {
        "Llama3.1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of this Python file is to define a PyTorch neural network module `ClassMethod` that contains a class method `method` and an instance method `forward`. The `forward` method applies a linear transformation to the input and then calls the class method `method` in three different ways to perform some computation. The file also creates an instance of the `ClassMethod` model and defines some example input arguments.",
            "total_tokens": 318,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 3.773656
        },
        "nova-pro-v1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file `class_method.py` defines a custom PyTorch neural network module named `ClassMethod` that includes a class method `method` which increments its input by 1. The `forward` method of the module applies a linear transformation to the input, then multiplies the result by three versions of the class method applied to the input. Finally, the file creates an instance of the `ClassMethod` module and prepares example arguments for its execution.",
            "total_tokens": 341,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 1.491621
        },
        "Claude3.5": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"class_method.py\" file is to define a PyTorch neural network module called ClassMethod. This module demonstrates the use of a class method within a neural network, showing how it can be inlined during tracing. The forward method of the module applies a linear transformation to the input and then uses the class method in three different ways to process the result.",
            "total_tokens": 354,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 3.834355
        },
        "gpt-4.1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file defines a PyTorch neural network module called `ClassMethod` that demonstrates the use of class methods within a model's computation. In the `forward` method, the module applies a linear transformation to the input and then calls a class method (`method`) in three different ways, multiplying the results. This file serves as an example of how class methods can be invoked and used in the forward pass of a PyTorch model, particularly in contexts where tracing or scripting might be involved.",
            "total_tokens": 324,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 2.152192
        },
        "gpt-4": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file `class_method.py` defines a Python class `ClassMethod` that inherits from `torch.nn.Module`. The class encapsulates neural network operations and includes a class method `method`, which simply increments its input by one. The `forward` function processes an input tensor through a linear transformation and then multiplies the result by three instances of the output from the `method` (applied different ways to demonstrate class method access). This hints at potential experimentation with class-level method invocations within a PyTorch model computation graph.",
            "total_tokens": 333,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 2.86058
        },
        "Llama3.3": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the given Python file is to define a PyTorch neural network module `ClassMethod` that inherits from `torch.nn.Module`. This module contains a class method `method` that increments the input by 1, and an instance method `forward` that applies a linear transformation to the input and then applies the class method in three different ways. The file also creates an instance of the `ClassMethod` model and defines example input arguments for it using PyTorch's `torch.randn` function.",
            "total_tokens": 337,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.989565
        },
        "gpt-4o": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file defines a neural network class `ClassMethod` using PyTorch's `torch.nn.Module`, which includes a custom forward method that applies a linear transformation, followed by computations involving a class method `method`. The `method` function increments the input tensor by 1 and is statically referenced multiple times during the forward pass. Overall, the file integrates PyTorch's tracing capabilities to demonstrate the inline usage of class methods within a model's computation graph.",
            "total_tokens": 319,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.169305
        },
        "titan_premier": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the file \"class_method.py\" is to define a class called \"ClassMethod\" that inherits from the \"torch.nn.Module\" class. This class has a class method called \"method\" that takes an input \"x\" and returns \"x + 1\". The class also has an instance method called \"forward\" that takes an input \"x\", applies a linear transformation using the \"linear\" attribute, and then applies the \"method\" class method to the transformed input. The file also includes an example usage of the class, where an instance of the class is created and passed a random input tensor.",
            "total_tokens": 132,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 3.229346
        },
        "nova-lite-v1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the file is to define a PyTorch neural network class `ClassMethod` that includes a class method `method`, an initialization method, and a forward pass method. The forward method utilizes the class method to perform operations on the input tensor, combining it with the class method in various ways. The file also includes an example of instantiating the class and preparing sample input data.",
            "total_tokens": 328,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 1.938332
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The given 'class_method.py' file defines a class 'ClassMethod' that inherits from torch.nn.Module. It includes a class method 'method' that performs a simple operation on the input, an initialization method '__init__' that sets up a linear layer, and a 'forward' method that uses the linear layer and calls the class method 'method'. An example argument is provided, and an instance of the 'ClassMethod' class is created.",
            "total_tokens": 317,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 0.979658
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a PyTorch `nn.Module` subclass named `ClassMethod`. It includes a `classmethod` called `method` that adds 1 to its input. The `forward` method applies a linear transformation and then calls the `method` class method in three different ways (`self.method`, `self.__class__.method`, and `type(self).method`) to process the output.",
            "total_tokens": 539,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.08725
        },
        "gpt-4.5": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file defines a PyTorch neural network module named \"ClassMethod,\" which contains a linear layer and a class method \"method\" performing a simple addition operation. In its forward method, the module processes input through the linear transformation, then applies the class method multiple times before returning the combined outcome. Additionally, a model instance and example input tensors are created to demonstrate usage of the defined module.",
            "total_tokens": 306,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 3.563017
        },
        "gemini-2.5": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a PyTorch neural network module named `ClassMethod`. The module includes a `@classmethod` called `method` that performs a simple addition. The main functionality demonstrated in the `forward` method is applying a linear layer to the input and then calling this class method multiple times using different access patterns (`self.method`, `self.__class__.method`, `type(self).method`).",
            "total_tokens": 961,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 7.812061
        }
    },
    "/scripts/export/update_schema.py": {
        "gpt-4o": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file `update_schema.py` is a script designed to update and validate schema files used in PyTorch while ensuring version compatibility for the schema and treespec definitions. It provides functionality to generate updated schema and associated files, with options for dry-run and forced unsafe updates. The script checks for compatibility, generates headers, and writes YAML, C++ header, and Thrift schema files to specified paths.",
            "total_tokens": 994,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 3.083895
        },
        "gpt-4.1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of update_schema.py is to update, validate, and optionally write new versions of PyTorch export schema files (in YAML, C++, and Thrift formats), ensuring that schema and treespec versions do not regress and that changes maintain compatibility unless explicitly overridden. The script uses command-line arguments to control its behavior (e.g., dry-run, force-unsafe), performs version checks, and integrates with a schema_check module to coordinate the schema update process. If not in dry-run mode, it writes the resulting schema files with appropriate headers and checksums to the specified directory.",
            "total_tokens": 1030,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 5.209708
        },
        "Llama3.3": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the `update_schema.py` file is to update the schema of a PyTorch project by checking and validating the schema version and treespec version, and then writing the updated schema to YAML, C++ header, and Thrift schema files. The script takes several command-line arguments, including the root directory of the PyTorch project, a dry-run option, and a force-unsafe option to override schema validation. The script ensures that the schema version and treespec version are not downgraded and that the schema is properly formatted and written to the correct files.",
            "total_tokens": 1039,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.722528
        },
        "gpt-4": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python script `update_schema.py` acts as a tool to update serialization schema for the PyTorch project using command-line arguments provided by the user. It ensures schema version compatibility by checking against predetermined base versions and allows users to conditionally force updates even when schema validations fail through an explicit command-line flag. If not being run in dry-run mode (where updates are just printed), the script writes the updated schema in YAML format, as well as the associated header file for C++ integration and a Thrift schema to their respective files within a specified directory tree in the PyTorch repository.",
            "total_tokens": 1034,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 2.622999
        },
        "titan_premier": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"update_schema.py\" file is to update the schema version and generate the corresponding YAML, C++ header, and Thrift schema files. It takes command-line arguments for the root directory of the PyTorch project, whether to perform a dry run, and whether to force an update even if schema validation fails. The script checks the current schema version, compares it with the base version, and updates the schema version if necessary. It then generates the required files with the updated schema version and checksum. If a dry run is specified, the script prints the updated schema instead of writing it to files.",
            "total_tokens": 130,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 3.580286
        },
        "nova-lite-v1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The script `update_schema.py` is designed to update and validate the schema for a PyTorch project. It takes a root directory as input and checks the schema version, ensuring it does not downgrade. If the schema is valid, it updates the schema files and writes them to the specified directory, or it can print the schema without writing if the `--dry-run` option is used. The script also allows forcing the update even if the schema validation fails, using the `--force-unsafe` option.",
            "total_tokens": 1167,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 0.882524
        },
        "Claude3.5": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"update_schema.py\" file is to update and validate schema files for a PyTorch project. It checks for version compatibility, generates new schema content in YAML, C++, and Thrift formats, and either prints the updated schema (in dry-run mode) or writes the new content to the respective files. The script also includes safety checks and options for forcing updates, making it a tool for managing schema versioning and compatibility in the PyTorch codebase.",
            "total_tokens": 1264,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 4.367885
        },
        "nova-pro-v1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The `update_schema.py` script updates the schema files for a PyTorch project by generating YAML, C++, and Thrift schema files. It validates the schema versions and writes the updated schema to the specified directory unless a dry-run is requested. Various checks and assertions ensure the integrity and versioning of the schema.",
            "total_tokens": 1131,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 1.038294
        },
        "gpt-3.5-turbo": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The given \"update_schema.py\" file is a script that updates the schema in a PyTorch directory based on certain conditions and user-provided arguments. It checks for schema versions, validates schema changes, generates YAML and C++ headers, and writes them to their respective files in the PyTorch directory. The script handles scenarios where the schema version is not updated correctly and provides options to resolve the issue, including using a force-unsafe flag and detailed error messages.",
            "total_tokens": 1009,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.045443
        },
        "Llama3.1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of this Python file, `update_schema.py`, is to update the schema of a PyTorch project by generating new schema files (YAML, C++, and Thrift) based on the current project state. It checks for schema version compatibility and validates the schema before writing the new files, with options for dry-run and force-unsafe modes. The script is designed to be run from the root of the PyTorch directory, and it uses the `torch._export.serde.schema_check` module for schema checking and generation.",
            "total_tokens": 1031,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 4.075416
        },
        "gemini-2.5-flash": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python script updates serialization schema files (YAML, C++, and Thrift) used by PyTorch's export functionality. It generates new schema content based on the current state defined in the codebase and performs version checks against existing files. The script then writes the updated schema files to a specified directory prefix within the PyTorch source tree, with options for a dry run or forcing the update.",
            "total_tokens": 1397,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.235741
        },
        "gpt-4.5": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The provided Python script updates and validates schema-related files within a PyTorch repository by generating YAML, C++, and Thrift schema files based on the results from schema checking. It ensures compatibility by validating versions and checksums, providing options like `--dry-run` (to preview without applying changes) and `--force-unsafe` (to force updates despite potential incompatibilities). If conditions are not met, such as non-existent paths or unexpected schema version downgrades, the script raises appropriate errors.",
            "total_tokens": 1013,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 4.498155
        },
        "gemini-2.5": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This script updates the serialization schema definitions used within the PyTorch project. It generates new schema versions based on the current codebase, performs validation checks for compatibility and version increments, and then writes the updated schema to YAML, C++, and Thrift files. The script allows for dry runs to preview changes and an option to force updates even if validation checks fail.",
            "total_tokens": 1371,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 3.960782
        }
    },
    "/torch/profiler/_pattern_matcher.py": {
        "Llama3.1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of this Python file is to define a set of patterns for identifying performance anti-patterns in PyTorch code, and to provide a function `report_all_anti_patterns` that can be used to report all matched anti-patterns in a given PyTorch profiler. The patterns include things like extra CUDA copies, for loop indexing, FP32 matrix multiplication, and more. The function can also benchmark the matched events and provide a summary of the results.",
            "total_tokens": 5714,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 5.090148
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the `_pattern_matcher.py` file is to define a set of patterns implemented as classes to identify anti-patterns in PyTorch code related to performance optimization. These patterns include identifying issues such as unnecessary operations, inefficient data loading, incorrect settings in optimizers, and more. The file uses event profiling data to match against these patterns and generate reports with recommendations on how to resolve the identified anti-patterns.",
            "total_tokens": 5700,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.487047
        },
        "gpt-4": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file implements a system designed to identify and report various anti-patterns in PyTorch profiling events, which could potentially hinder performance. These patterns include unnecessary copying of data to the GPU, redundant bias settings in combination with BatchNorm2d layers, improper optimizer configurations, synchronized DataLoader operations, and the use of suboptimal tensor operations like element-wise setting in loops. This process involves defining different classes for each pattern where each class contains methods to match specific events, report problems, and suggest improvements or benchmarks. The system allows overall compilation and output of these reports and even writes to JSON if necessary.",
            "total_tokens": 5735,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 4.984866
        },
        "Llama3.3": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the provided Python file is to identify and report anti-patterns in PyTorch code, specifically performance-related issues such as unnecessary CUDA copies, inefficient data loading, and suboptimal tensor operations. The file defines a set of patterns to match these anti-patterns and provides a reporting mechanism to output the findings, including source code locations and recommendations for improvement. The report can also be saved to a JSON file for further analysis.",
            "total_tokens": 5710,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 2.24138
        },
        "nova-pro-v1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the `_pattern_matcher.py` file is to define a base `Pattern` class and several subclasses that identify specific anti-patterns in PyTorch code, providing detailed reports and optional performance benchmarks for matched events. The file also includes a function to generate and print a comprehensive report of all detected anti-patterns.",
            "total_tokens": 6653,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 2.113673
        },
        "nova-lite-v1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the `_pattern_matcher.py` file is to define and implement various patterns for detecting anti-patterns in PyTorch code, providing detailed reports and recommendations for optimization. The file includes a base `Pattern` class and several subclasses that identify specific anti-patterns, such as inefficient tensor operations, suboptimal data loading, and improper optimizer configurations. It also includes a function to generate a comprehensive report of detected issues, which can be printed or saved as a JSON file.",
            "total_tokens": 6683,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 1.303635
        },
        "Claude3.5": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The main functionality of the \"_pattern_matcher.py\" file is to define and implement various anti-patterns in PyTorch code. It includes a base Pattern class and several specific pattern classes that inherit from it, such as ExtraCUDACopyPattern, FP32MatMulPattern, and OptimizerSingleTensorPattern. The file also provides a function called \"report_all_anti_patterns\" that analyzes a PyTorch profile, identifies matching anti-patterns, and generates a report with suggestions for code optimization.",
            "total_tokens": 7230,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 4.574387
        },
        "titan_premier": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The Python file named \"_pattern_matcher.py\" is a part of the TorchTidy tool, which is used to analyze PyTorch profiling results and identify anti-patterns. The file contains several classes that inherit from the base class \"Pattern\". Each class represents a specific anti-pattern and implements methods to match the pattern and generate a report.\n\nThe main functionality of the file is to define various anti-patterns and provide methods to match these patterns in the profiling results. The anti-patterns include:\n\n1. Extra CUDA Copy Pattern: This pattern identifies if a constant tensor is created on the CPU and immediately moved to the GPU.\n2. For Loop Indexing Pattern: This pattern identifies if a for loop is used to index a tensor that can be vectorized.\n3. FP32 MatMul Pattern: This pattern identifies if the FP32 version of the MatMul operation is used instead of the TF32 version on GPUs that support TF32.\n4. Optimizer Single Tensor Pattern: This pattern identifies if the single-tensor version of an optimizer is used instead of the multi-tensor version.\n5. Synchronized DataLoader Pattern: This pattern identifies if the DataLoader is running with a synchronized implementation instead of an asynchronous implementation.\n6. Grad Not Set To None Pattern: This pattern identifies if the gradient is set to zero instead of None in the zero_grad method.\n7. Conv2d Bias Followed By BatchNorm2d Pattern: This pattern identifies if bias is enabled in Conv2d that is followed by BatchNorm2d.\n8. MatMul Dim In FP16 Pattern: This pattern identifies if the dimension of the input tensors for the matmul operation is not aligned when using FP16.\n\nThe file also includes helper functions to extract information from the profiling results, such as the source code location and input shapes and types.\n\nThe main function \"report\\_all\\_anti\\_patterns\" takes a profiling object, a boolean flag to enable benchmarking, a boolean flag to enable printing the report, and an optional directory to save the report in JSON format. It iterates through all the anti-patterns, matches them in the profiling results, generates a report for each pattern, and",
            "total_tokens": 500,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 11.372683
        },
        "gpt-4.1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The file \"_pattern_matcher.py\" defines a set of pattern-matching classes that analyze PyTorch profiler traces to identify inefficient or suboptimal code patterns (anti-patterns) in deep learning workflows. Each pattern targets specific inefficiencies\u2014such as unnecessary CPU-to-GPU tensor copies, lack of vectorization, suboptimal optimizer usage, inefficient data loading, or misaligned matrix multiplication\u2014and provides recommendations and optional benchmark-based speedup estimates for remedying these issues. The main functionality is to scan profiler data, detect these anti-patterns, report findings with actionable advice, and optionally generate a JSON report summarizing the results.",
            "total_tokens": 5731,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 3.53415
        },
        "gpt-4o": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The `_pattern_matcher.py` file provides a framework for identifying and reporting anti-patterns in PyTorch code by analyzing profiling data. It defines a base `Pattern` class and specific subclasses for detecting inefficiencies, such as unnecessary data copying between CPU and GPU, non-vectorized tensor indexing, suboptimal tensor operations, or improper configuration of PyTorch modules. Additionally, it includes functionality for detailed event traversal, benchmarking, and generating reports to help developers optimize their code.",
            "total_tokens": 5701,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 2.117479
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a framework for detecting performance anti-patterns within a PyTorch profiler trace. It includes a base `Pattern` class and several subclasses, each implementing logic to identify specific inefficiencies like unnecessary CUDA copies, suboptimal optimizer usage, or misconfigured layers. The main function `report_all_anti_patterns` applies these patterns to a given profile and generates a report detailing the findings and suggested optimizations.",
            "total_tokens": 7038,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.553372
        },
        "gpt-4.5": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "The provided Python file defines a framework that analyzes performance profiles from PyTorch models by matching them against various predefined anti-patterns, such as wasteful CUDA tensor transfers, inefficient optimizer implementations, or lack of vectorization. It includes logic to identify problematic code patterns within PyTorch execution (e.g., unnecessary tensor copy from CPU to GPU or using single-tensor optimizers) and to generate human-readable performance improvement recommendations. Additionally, it can produce summarized reports and JSON output detailing matched anti-patterns along with suggestions for optimization.",
            "total_tokens": 5711,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 7.605196
        },
        "gemini-2.5": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": -1,
            "project_name": "pytorch",
            "message": "This Python file defines a framework for analyzing PyTorch profiler output to detect common performance anti-patterns. It includes a base `Pattern` class and several specific subclasses that identify issues like unnecessary CUDA copies, inefficient data loading, or suboptimal operator usage. The main function `report_all_anti_patterns` uses these patterns to scan a given profile, generate a report summarizing findings with source code locations, and optionally benchmark potential speedups.",
            "total_tokens": 7580,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 9.746848
        }
    }
}