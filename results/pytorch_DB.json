{
    "/torch/_functorch/aot_autograd.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"aot_autograd.py\" file is to provide a framework for Ahead-of-Time (AOT) compilation of PyTorch models and functions, enabling optimizations and performance improvements by converting them into a format suitable for execution on specialized hardware. This file includes utilities for tracing, functionalizing, and compiling PyTorch models and functions, handling various edge cases related to input mutations, metadata changes, and aliasing, and generating metadata and signatures for exported models. The file also supports different compilation modes, such as autograd, inference, and export, and allows customization through configuration options and compiler functions.",
            "total_tokens": 18344,
            "latency": 2.230268,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "b3106686-164e-4833-8fa9-3d08cb9d76c4"
        },
        "titan_premier": {
            "file_summary": "The file `aot_autograd.py` is a part of PyTorch's AOT Autograd (Ahead-Of-Time Autograd) module. This module is designed to optimize the performance of PyTorch models by compiling the forward and backward passes of a model into a single, efficient function.\n\nThe main functionality of this file is to provide two main functions: `aot_function` and `aot_module`. These functions take in a Python function or a PyTorch module, respectively, and trace the forward and backward passes of the function or module using PyTorch's dispatch mechanism. The traced forward and backward graphs are then compiled using a specified compiler function, and the resulting compiled function is returned.\n\nThe `aot_function` function takes in a Python function, a forward compiler function, and an optional backward compiler function. It also takes in several optional parameters, such as a partition function, decompositions, and whether or not to enable dynamic shapes. The function first flattens the input arguments of the Python function and then traces the forward and backward graphs of the function using PyTorch's dispatch mechanism. The traced forward and backward graphs are then partitioned using the specified partition function, and the partitioned graphs are compiled using the specified compiler functions. The resulting compiled forward and backward functions are then wrapped up in a `torch.autograd.Function` object, and the original Python function is replaced with this new object.\n\nThe `aot_module` function takes in a PyTorch module, a forward compiler function, and an optional backward compiler function. It also takes in several optional parameters, such as a partition function, decompositions, and whether or not to enable dynamic shapes. The function first lifts the parameters and buffers of the module as inputs to a new callable, which is then compiled using the `aot_function` function. The resulting compiled callable is then wrapped up in a new `nn.Module` object, which retains the eager behavior of the original module but with the forward and backward graphs compiled.\n\nThe file also provides several helper functions, such as `aot_export_module`, `aot_export_joint_simple`, and `_aot_export_function`, which are used to export the forward and backward graphs of a module as a single FX graph. These functions are used by higher-",
            "total_tokens": 500,
            "latency": 13.617909,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "adb1c509-e200-4e53-8adb-f7ad126290e0"
        },
        "gpt-4.5": {
            "file_summary": "The given Python file contains the implementation of \"Ahead-of-Time Autograd\" (AOT Autograd) functionality within PyTorch's framework. Its main purpose is to trace the forward and backward computational graphs ahead of time (AOT), handle mutation and aliasing edge cases systematically, and compile these functionalized graphs to optimize execution during training or inference. It provides advanced support for compiling PyTorch models and functions into optimized forms, respecting constraints like mutation elimination and alias handling for correct gradient computation.",
            "total_tokens": 15631,
            "latency": 16.967573,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "4f28e82a-958d-4d20-97d5-d5dc205dd143"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file provides functionality for exporting PyTorch modules using Ahead-of-Time (AOT) compilation. It traces forward and backward graphs of functions or modules using torch dispatch tracing mechanisms and compiles them using specified compilers. The file contains functions to handle aliasing, mutations, and edge cases around inputs, outputs, metadata, and tensors, ensuring efficient compilation of graphs for export or inference purposes. Additionally, it includes functions for processing inputs, creating synthetic bases, and setting up calling conventions for joint graphs, enabling the generation of functionalized and efficient graphs for export.",
            "total_tokens": 15563,
            "latency": 2.938408,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "91e56357-69fe-49e7-b266-6ba3cff3684a"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"aot_autograd.py\" file is to provide ahead-of-time (AOT) compilation and optimization for PyTorch modules and functions. It includes implementations for tracing and compiling forward and backward graphs, handling input mutations and aliasing, and exporting compiled modules for various use cases including inference and training.",
            "total_tokens": 19782,
            "latency": 5.028594,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "21265123-6277-4172-ae29-a72f39844538"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `aot_autograd.py` file is to provide a set of tools and decorators for tracing, compiling, and exporting PyTorch models and functions using Ahead-of-Time (AOT) compilation techniques. It includes functionalities for handling input and output mutations, aliasing, and functionalization of graphs, as well as utilities for creating and managing fake tensors and metadata. The file supports both training and inference modes, and offers simplified versions of its main functions for reduced overhead in certain use cases.",
            "total_tokens": 18324,
            "latency": 5.021529,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e62be452-40a6-4202-9545-ef5d07572f05"
        },
        "gpt-4o": {
            "file_summary": "The `\"aot_autograd.py\"` file provides a framework for advanced compilation and optimization of PyTorch functions and models using Ahead-of-Time (AOT) Autograd. It introduces tools for tracing functions, partitioning forward and backward graphs, managing complex edge cases involving input/output mutations, aliasing, and custom dispatch mechanisms while ensuring functional graphs for efficient compilation. The file also includes utilities to export models to functional intermediate representations and handle dynamic shape inference and specialized compilation scenarios.",
            "total_tokens": 15627,
            "latency": 2.631832,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "17e37cda-f38c-4413-930e-a42a529a34bf"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the file is to provide a set of functions and classes for Ahead-of-Time (AOT) Autograd, which is a mechanism for compiling PyTorch models to improve their performance. The file includes functions for tracing and compiling forward and backward graphs, as well as classes for representing AOT Autograd configurations and metadata. The file also includes various utility functions for working with PyTorch tensors, modules, and graphs.",
            "total_tokens": 15537,
            "latency": 48.957108,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "05414245-d41c-47d5-b377-afd0052b8aae"
        },
        "gpt-4": {
            "file_summary": "The Python file 'aot_autograd.py' primarily deals with the augmentation and compilation of computational graphs in PyTorch, particularly targeting automatic differentiation and export functionality tailored for customized performance enhancements like GPU acceleration or streamlined deployment. This includes selectively compiling forward and backward graph computations based on runtime configurations, handling dynamic shapes, and ensuring the proper handling of module attributes to maintain support for PyTorch\u2019s module framework. The file supports generating both inference and training graphs according to the needs specified and handles specialized cases like mutations in inputs and side-effects from certain operations. It also incorporates graph partitioning and captures operational metadata that can be useful for debugging and further optimization processes.",
            "total_tokens": 15580,
            "latency": 5.020207,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "4fd483e4-e09e-42f0-b36f-4338895f1b15"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement Ahead-of-Time (AOT) Autograd, a mechanism for compiling PyTorch models to improve performance by tracing and compiling the forward and backward graphs of a model ahead of time. The file provides functions such as `aot_function` and `aot_module` to compile PyTorch functions and modules, respectively, and `aot_export_module` to export a compiled model. The compilation process involves tracing the model's forward and backward graphs, applying optimizations, and generating a compiled function that can be executed at runtime.",
            "total_tokens": 15567,
            "latency": 4.278123,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "169af2a5-b27c-4ea7-81af-c2c6dab24ac9"
        },
        "gpt-4.1": {
            "file_summary": "The file \"aot_autograd.py\" provides advanced functionality for tracing, transforming, and compiling PyTorch functions and nn.Modules using the Ahead-Of-Time (AOT) Autograd system. Its main role is to convert PyTorch code (including forward and backward computations) into functional, traceable graphs, allowing custom compilation, export, and optimization while carefully managing input/output aliasing, mutations, and tensor subclass behavior. The module exposes utilities and APIs for compiling standalone functions or full modules, handling decompositions, exporting computation graphs, and supporting backends that require functionalized, stateless computational graphs suitable for further optimization or deployment.",
            "total_tokens": 15660,
            "latency": 2.77618,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "0866adb6-5454-41a0-a7ef-0f62090c3695"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_functorch/aot_autograd.py",
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "total_tokens": 19423,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.887094,
            "file_summary": "This Python file implements AOT (Ahead-of-Time) Autograd for PyTorch, providing mechanisms to trace and compile Python functions and `nn.Module`s. It handles complex tensor behaviors like mutations, aliasing, and subclasses by functionalizing the computation graph. The system generates compiled runtime functions or exportable FX graphs suitable for various backends and use cases, including training and inference.",
            "uuid": "eddd0f1b-c1f7-4dd7-b3cc-7d3065ba5119"
        },
        "gemini-2.5": {
            "file_name": "/torch/_functorch/aot_autograd.py",
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "total_tokens": 19465,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.432005,
            "file_summary": "This Python file implements the core logic for PyTorch's AOT Autograd mechanism. It traces user-defined functions or `nn.Module`s, functionalizes the computation graph to handle side effects like mutations and aliasing, and partitions the graph into forward and backward passes if needed. Finally, it provides hooks (`aot_function`, `aot_module`, `aot_export_module`) to compile these graphs using user-specified backend compilers or prepare them for export.",
            "uuid": "4a14cbf6-fc23-4e4d-9522-28cbc4930d4d"
        },
        "Claude3.7": {
            "file_name": "/torch/_functorch/aot_autograd.py",
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "total_tokens": 19792,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 5.181041,
            "file_summary": "The `aot_autograd.py` file implements AOT (Ahead-Of-Time) Autograd functionality for PyTorch, providing mechanisms to trace forward and backward computation graphs ahead of time and compile them for optimized execution. It contains extensive functionality for handling edge cases around tensor aliasing, mutations, and subclasses, along with various compilation strategies for both training and inference scenarios.",
            "uuid": "96464d86-5052-48a5-a6e6-8f98300b06e4"
        },
        "mistral-small": {
            "file_name": "/torch/_functorch/aot_autograd.py",
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "total_tokens": 23472,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 53.759446,
            "file_summary": " The provided Python code defines a function called `aot_autograd_decompositions` which is an empty dictionary. This function does not perform any operations or calculations. It is likely used as a placeholder or to initialize a variable with an empty dictionary.\n\nThe code also includes several import statements for various Python libraries and modules, including `torch`, `torch._dynamo.logging`, `torch._dynamo.source`, `torch._dynamo.utils._disable_saved_tensors_hooks_during_tracing`, `torch._dispatch.python`, `torch._inductor.cudagraph_utils`, `torch._inductor.output_code`, `torch._inductor.utils`, `torch._subclasses`, `torch.fx.experimental.proxy_tensor`, `torch.fx.experimental.symbolic_shapes`, `torch.nn`, `torch.utils._python_dispatch`, `torch.utils._pytree`, `torch.utils._tree_flatten`, `torch.utils._tree_unflatten`, `torch.utils._tree_utils`, `torch.utils.dlpack`, `torch.utils.dlpack.from_dlpack`, `torch.utils.dlpack.to_dlpack`, `torch.utils.dlpack.to_tensor`, `torch.utils.dlpack.to_tensor_async`, `torch.utils.dlpack.to_tensor_cpu`, `torch.utils.dlpack.to_tensor_cuda`, `torch.utils.dlpack.to_tensor_cuda_async`, `torch.utils.dlpack.to_tensor_cuda_pinned`, `torch.utils.dlpack.to_tensor_cuda_pinned_async`, `torch.utils.dlpack.to_tensor_cpu_pinned`, `torch.utils.dlpack.to_tensor_cpu_pinned_async`, `torch.utils.dlpack.to_tensor_cpu_shared`, `torch.utils.dlpack.to_tensor_cpu_shared_async`, `torch.utils.dlpack.to_tensor_cuda_shared`, `torch.utils.dlpack.to_tensor_cuda_shared_async`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_async`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_async_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_async`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_async_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cuda_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_cpu_shared_pinned_copy_async_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy_copy`, `torch.utils.dlpack.to_tensor_c",
            "uuid": "0c14b10c-1ead-4c4c-9029-0b7a7d24af6d"
        },
        "mistral-7b": {
            "file_name": "/torch/_functorch/aot_autograd.py",
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "total_tokens": 20740,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 9.123943,
            "file_summary": " This Python file is a part of PyTorch's Autograd Optimization and Tracing (AOT) module. It contains various functions and classes used for automatic differentiation, functionalization, and compilation of PyTorch tensors and functions. The main functionality of the file can be summarized as follows:\n\n1. The file imports necessary modules and libraries for autograd, including torch, torch.nn, and various utility functions.\n2. It defines several constants, variables, and functions for handling input data, metadata mutations, and outputs aliasing inputs or intermediates.\n3. It contains the `aot_function` function, which is used to trace and compile a given PyTorch function or module using PyTorch's dispatch mechanism. This function takes the function or module, forward and backward compilers, a partitioner, and optional decompositions as arguments. It returns a compiled version of the function or module.\n4. It also contains the `aot_module` function, which is a wrapper around `aot_function` that is used to trace and compile PyTorch modules. This function takes the module, input arguments, and optional arguments as arguments and returns a compiled version of the module.\n5. The file also includes several helper functions and classes for handling various aspects of autograd, such as input and output processing, metadata collection, and view and mutation handling.\n\nOverall, the main functionality of the file is to provide a way to trace, compile, and functionalize PyTorch functions and modules for automatic differentiation and optimization.",
            "uuid": "c053c18e-c9ca-400a-82a1-417229b8a67d"
        },
        "cohere-v1": {
            "file_name": "/torch/_functorch/aot_autograd.py",
            "number_of_lines": 1703,
            "project_name": "pytorch",
            "file_summary": "The code defines several functions for compiling PyTorch models using Ahead-of-Time (AOT) Autograd. The main functionality is the ability to trace and compile models into optimized functions or modules, with options for compiling both forward and backward passes. This process involves flattening the input arguments, creating dispatcher functions, and compiling them using provided compiler functions. The compiled functions aim to improve the performance of the models while retaining the eager behavior.",
            "total_tokens": 17839,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 3.488042,
            "uuid": "cf770d07-f5cd-48de-b446-1ab2ea8e5c62"
        }
    },
    "/torch/fx/graph.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file defines a `Graph` class and related functionalities for representing and manipulating computational graphs, primarily used in the context of symbolic tracing and intermediate representation in PyTorch's FX framework. The `Graph` class models a series of interconnected nodes, each representing a callsite or other syntactic constructs, which together form a valid Python function. The file includes methods for creating various types of nodes (placeholders, attribute fetches, module calls, method calls, function calls, and outputs), managing the graph's namespace and insertion points, generating Python code from the graph, and performing linting and dead code elimination.",
            "total_tokens": 19591,
            "latency": 2.323423,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "b415e9a9-ef68-496a-881e-f284ec38550d"
        },
        "titan_premier": {
            "file_summary": "The file `graph.py` is a part of the PyTorch FX library, which provides tools for analyzing and transforming PyTorch models. The main functionality of this file is to define the `Graph` class, which represents a computation graph.\n\nThe `Graph` class has several methods for creating and manipulating nodes in the graph. These methods include:\n\n* `create_node`: Creates a new node with the given opcode, target, arguments, and keyword arguments.\n* `placeholder`: Creates a placeholder node with the given name and type annotation.\n* `get_attr`: Creates a node that gets the attribute with the given qualified name.\n* `call_module`: Creates a node that calls the forward method of the given module.\n* `call_method`: Creates a node that calls the given method on the first argument.\n* `call_function`: Creates a node that calls the given function with the given arguments and keyword arguments.\n* `output`: Creates an output node with the given value.\n* `erase_node`: Erases the given node from the graph.\n* `inserting_before` and `inserting_after`: Set the point at which new nodes will be inserted into the graph.\n* `node_copy`: Creates a copy of the given node in the current graph.\n* `python_code`: Generates Python code that represents the computation graph.\n* `lint`: Checks the graph for errors and warnings.\n* `eliminate_dead_code`: Removes all dead code from the graph.\n* `set_codegen`: Sets a custom code generation function.\n* `on_generate_code`: Registers a code transformation function that will be applied when generating Python code.\n\nThe `Graph` class also has several helper methods for working with nodes, such as `find_nodes`, `process_inputs`, and `process_outputs`. These methods are used to map the values in the graph to their corresponding Python expressions.\n\nOverall, the `Graph` class provides a high-level interface for working with computation graphs in PyTorch. It allows users to easily create, manipulate, and analyze graphs, as well as generate Python code that represents the graph.",
            "total_tokens": 499,
            "latency": 13.994906,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "96dbad13-7adb-4a1f-8385-8ad82e969313"
        },
        "gpt-4.5": {
            "file_summary": "The provided file is a comprehensive implementation of a graph-based intermediate representation used for symbolic manipulation and transformation of Python code, specifically geared towards PyTorch models. It includes classes and mechanisms for representing and manipulating code as nodes, generating source code from graph representations, supporting code transformations, and providing utility functions to analyze and optimize graphs like eliminating dead code or linting. Essentially, this module serves as infrastructure for PyTorch's FX (functional transforms) subsystem in enabling tracing and transformation of neural network code.",
            "total_tokens": 16880,
            "latency": 16.092808,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "ed7afaab-d5d1-454f-80d5-88d885a23825"
        },
        "Claude3.5": {
            "file_summary": "This Python file defines the Graph class, which is a core component of the PyTorch FX (Function Transforms) system. The main functionality of this file is to provide a data structure and methods for representing and manipulating computational graphs. Here's a summary of the key aspects:\n\n1. The Graph class represents a graph of nodes, where each node corresponds to an operation in a PyTorch computation.\n\n2. It provides methods for creating different types of nodes (e.g., placeholder, get_attr, call_module, call_method, call_function, output) and manipulating the graph structure.\n\n3. The class includes functionality for code generation, allowing the graph to be converted back into executable Python code.\n\n4. It also includes utility methods for graph analysis, such as dead code elimination and linting.\n\nIn essence, this file is crucial for the intermediate representation used in PyTorch's FX system, enabling various code transformations and optimizations on PyTorch models.",
            "total_tokens": 20990,
            "latency": 8.359343,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "452027f3-2564-4702-9f63-eb3a9e007ace"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `graph.py` file is to define the `Graph` class, which represents the intermediate representation used in the FX framework. This class allows for the creation, manipulation, and transformation of computational graphs, enabling the generation of Python code from these graphs and facilitating various optimizations and analyses.",
            "total_tokens": 19529,
            "latency": 4.062797,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "112627ed-b824-4c7d-826b-ca2c84d5ba53"
        },
        "gpt-4o": {
            "file_summary": "The `graph.py` file is dedicated to implementing a flexible intermediate representation (IR) for computational graphs, primarily within PyTorch's FX module. It provides a `Graph` class that manages a collection of `Node` objects representing operations (`call_function`, `call_method`, `placeholder`, etc.), enabling tracing, manipulation, and code generation for Python functions representing deep learning models. Additionally, it includes utilities for generating Python source code, ensuring correctness via linting, handling name uniqueness, and optimizing graphs through dead code elimination.",
            "total_tokens": 16887,
            "latency": 2.37422,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "67b8816d-eb45-4c26-9240-2cafcebec76f"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the given Python file is to define a Graph class, which represents a data structure used in the FX Intermediate Representation. The Graph class consists of a series of Node objects, each representing callsites or other syntactic constructs, and provides methods for creating, manipulating, and analyzing the graph. The file also defines a CodeGen class, which is responsible for generating Python code from the graph, and a _PyTreeCodeGen class, which is a subclass of CodeGen that handles PyTree logic.",
            "total_tokens": 16761,
            "latency": 49.835752,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a22078de-aa24-4928-8c59-2634bd375bd4"
        },
        "gpt-4": {
            "file_summary": "The provided Python file defines a module primarily focused on generating Python code from an intermediate representation, such as a computation graph, often utilized in frameworks like PyTorch. It includes classes and methods to transform, manage node relationships, maintain the correct execution order, and convert these relationships and operations into structured Python code. The file supports handling different data types, managing side effects in the code generation, and adapting to changes made to the computational graph through direct manipulation of nodes and operations, ensuring the integrity and accuracy of the generated Python code.",
            "total_tokens": 16759,
            "latency": 5.396311,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "6d134f29-671f-4f01-bacc-3c171418cce7"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to define the `Graph` class, which represents the main data structure used in the FX Intermediate Representation. The `Graph` class consists of a series of `Node` objects, each representing callsites or other syntactic constructs, and provides methods for creating and manipulating these nodes, as well as generating Python code from the graph. The file also defines several related classes and functions, including `Node`, `CodeGen`, and `PythonCode`, which are used to support the functionality of the `Graph` class.",
            "total_tokens": 16771,
            "latency": 4.26742,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "881acc90-16ed-436b-8b85-34616a45846f"
        },
        "gpt-4.1": {
            "file_summary": "The file defines and implements an FX Graph Intermediate Representation system used by PyTorch to capture, represent, and transform computation graphs as sequences of Nodes, enabling symbolic tracing and manipulation of Python code and models. It provides the core Graph class along with utilities for safe node naming, efficient node lookup, namespace management, graph copying, dead code elimination, and flexible Python code generation from the graph via the CodeGen and PythonCode classes. The module also supports advanced features such as maintaining pytree structure, on-the-fly code transformation, and integration with Torch's namespace and special operator formatting, to facilitate model analysis, transformation, and code export.",
            "total_tokens": 16909,
            "latency": 3.036516,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "6894f26f-846f-4f06-a46b-ef3fa185fee8"
        },
        "gemini-2.5": {
            "file_name": "/torch/fx/graph.py",
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "total_tokens": 20474,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.230827,
            "file_summary": "This file defines the `Graph` class, the core data structure for PyTorch FX's intermediate representation, which represents a Python function as a directed acyclic graph of `Node` objects corresponding to operations. It provides methods to construct and manipulate this graph, including adding, copying, erasing nodes, and managing insertion points. The file also includes the `CodeGen` class and associated utilities for generating executable Python code from the `Graph`, handling namespaces, formatting, and supporting features like PyTree integration.",
            "uuid": "2e5f49f8-a8dc-4882-b6aa-e3388e13e852"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/fx/graph.py",
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "total_tokens": 20166,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.411046,
            "file_summary": "This Python file defines the `Graph` class, a core data structure in PyTorch's FX library representing a computation graph as a sequence of `Node` objects. It provides methods for building and manipulating this graph, such as creating, erasing, and copying nodes. Additionally, it includes functionality to generate executable Python code from the graph representation, handling aspects like variable naming, imports, and type annotations.",
            "uuid": "2199372b-a2dd-4835-831f-5e5cdb3afb6b"
        },
        "Claude3.7": {
            "file_name": "/torch/fx/graph.py",
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "total_tokens": 20857,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 5.670365,
            "file_summary": "The main functionality of this file is to define the Graph class, which is the core data structure in PyTorch's FX intermediate representation. It represents a computational graph consisting of nodes that represent operations like function calls, method calls, attribute access, and module calls, allowing for code generation, manipulation, and analysis of PyTorch models through a graph-based representation.",
            "uuid": "a87cd29d-4867-4a89-b0b0-fc6604ea030d"
        },
        "mistral-small": {
            "file_name": "/torch/fx/graph.py",
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "total_tokens": 21816,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 6.687327,
            "file_summary": " It appears that you've shared a Python script that defines a class named `Graph`. This class is used to represent a graph data structure in the context of PyTorch's FX Intermediate Representation (IR).\n\nThe `Graph` class has several methods for creating and manipulating nodes in the graph, such as `create_node`, `placeholder`, `get_attr`, `call_module`, `call_method`, and `call_function`. These methods allow you to add new nodes to the graph, each representing a different operation or function call.\n\nThe `Graph` class also has methods for generating Python code from the graph, such as `python_code` and `_python_code`. These methods take the graph and return a `PythonCode` object, which contains the generated Python code and a dictionary of global variables used in the code.\n\nAdditionally, the `Graph` class has methods for linting the graph, eliminating dead code, and setting a code generator.\n\nOverall, the `Graph` class appears to be a key component of PyTorch's FX IR, allowing users to create and manipulate graphs of operations and function calls, and generate Python code from those graphs.",
            "uuid": "3e20017f-0c8c-4d71-be52-583b93bae934"
        },
        "mistral-7b": {
            "file_name": "/torch/fx/graph.py",
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "total_tokens": 21833,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 7.007855,
            "file_summary": " This Python file defines a class named `Graph` which represents an intermediate representation (IR) of a Python function. The main functionality of the file can be summarized as follows:\n\n1. The `Graph` class is used to represent an IR of a Python function. It consists of a list of nodes, each representing a call site or other syntactic constructs in the function.\n2. The class provides methods to create new nodes in the graph, such as `create_node` for creating a new node with a given operation, target, arguments, and keywords.\n3. The class also provides methods to manipulate the graph, such as `inserting_before` and `inserting_after`, which allow you to set the insertion point for new nodes in the graph.\n4. The `python_code` method generates Python code from the `Graph` representation.\n5. The file also includes various utility functions and classes, such as `_PyTreeCodeGen` for handling PyTree information, `_FindNodesLookupTable` for fast queries, and various decorators and utilities for code generation and formatting.\n\nOverall, the main functionality of the file is to provide a way to represent and manipulate the IR of a Python function, and to generate Python code from that IR.",
            "uuid": "aa8842e4-9b29-4f5f-92c7-a9f0d25b7f33"
        },
        "cohere-v1": {
            "file_name": "/torch/fx/graph.py",
            "number_of_lines": 1989,
            "project_name": "pytorch",
            "file_summary": "The given python file defines a Graph class and its functionalities. The Graph class represents a computational graph and has methods to manipulate the graph, such as adding, copying, and removing nodes. It also has methods to generate Python code from the graph and vice versa. The file also defines helper functions for code generation and handling the nodes in the graph.",
            "total_tokens": 18407,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 3.27983,
            "uuid": "734fbe28-c771-4a72-9c5d-f34029f73d74"
        }
    },
    "/tools/linter/adapters/workflow_consistency_linter.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `workflow_consistency_linter.py` file is to lint GitHub workflow YAML files for consistency among jobs that share the same `sync-tag`. It checks that jobs with the same `sync-tag` have identical configurations, excluding certain fields like `if` and `test-matrix`, and reports any inconsistencies as lint errors. The script can be run from the command line, taking one or more workflow file paths as input.",
            "total_tokens": 1191,
            "latency": 1.051534,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "433a4df3-782f-49a3-a7b1-193c9b413786"
        },
        "titan_premier": {
            "file_summary": "The Python file \"workflow\\_consistency\\_linter.py\" is a script that checks for consistency of jobs between different GitHub workflows. It does this by comparing the jobs with the same \"sync-tag\" across all the provided files. If any inconsistencies are found, it prints out a lint message with the path, line number, and a description of the issue. The script also allows for different triggering conditions and test matrices for jobs with the same \"sync-tag\". The script takes a list of file paths as input and can be run from the command line.",
            "total_tokens": 120,
            "latency": 2.791189,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "23f3bc4e-135d-4304-9167-8948386e2d22"
        },
        "gpt-4.5": {
            "file_summary": "This script acts as a consistency linter for GitHub workflow files, verifying that jobs sharing a common `sync-tag` have identical configurations. It scans multiple YAML workflow files, omitting differences allowed in specified keys, and logs issues when jobs with the same tag differ. Detected inconsistencies are outputted as structured JSON lint messages that specify file locations and problem details.",
            "total_tokens": 1049,
            "latency": 3.961656,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "81de9da3-0947-4431-bfe4-e83abe86ffbc"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The python file `workflow_consistency_linter.py` is a script that checks for consistency of jobs between different GitHub workflows. It reads YAML workflow files provided as arguments, validates if they contain jobs, and then compares jobs based on a specific `sync-tag`. If jobs with the same `sync-tag` do not match, it outputs lint messages indicating inconsistencies. The script aims to ensure that all jobs with a specific `sync-tag` are consistent within the workflow files.",
            "total_tokens": 1056,
            "latency": 1.262518,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "34e64026-0ee7-45c6-ab3e-1e4532715166"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"workflow_consistency_linter.py\" file is to check for consistency between jobs in different GitHub workflows that share the same \"sync-tag\". It analyzes YAML files containing workflow definitions, compares jobs with matching sync tags, and reports any inconsistencies as lint errors. The script also provides a command-line interface for specifying the files to be checked.",
            "total_tokens": 1274,
            "latency": 2.975642,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3f9f5d2b-329b-4aae-8fbe-a1e2d933a53e"
        },
        "nova-pro-v1": {
            "file_summary": "The `workflow_consistency_linter.py` script checks for consistency of jobs between different GitHub workflows by ensuring that any job with a specific `sync-tag` matches all other jobs with the same `sync-tag`. It processes YAML workflow files, compares jobs with the same `sync-tag`, and prints lint messages for inconsistencies.",
            "total_tokens": 1166,
            "latency": 0.989755,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "050232e8-afde-4f53-9e92-0ae64fe39f7b"
        },
        "gpt-4o": {
            "file_summary": "The Python file `workflow_consistency_linter.py` is a linter tool designed to ensure consistency across GitHub workflow jobs that share the same `sync-tag`. It analyzes YAML configuration files, identifies jobs with matching `sync-tag` values, and verifies that these jobs are identical apart from specific allowed variations, such as conditions in the \"if\" field or the \"test-matrix\". If inconsistencies are detected, the tool outputs structured lint messages indicating the discrepancies.",
            "total_tokens": 1069,
            "latency": 1.50831,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "c31312b5-2703-4f07-a5f8-2db4a2900c58"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to check for consistency of jobs between different GitHub workflows. It does this by parsing YAML files, identifying jobs with a specific `sync-tag`, and ensuring that all jobs with the same `sync-tag` have the same configuration. If any inconsistencies are reported as lint messages in JSON format.",
            "total_tokens": 1033,
            "latency": 2.813256,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "402561bb-8c93-430f-a301-c9e2efc29ca9"
        },
        "gpt-4": {
            "file_summary": "The file `workflow_consistency_linter.py` serves as a linter to ensure consistency across GitHub workflow files that use a specific synchronization tag (`sync-tag`). The script examines multiple YAML workflow files to detect jobs tagged with the same `sync-tag` and checks if their configurations match exactly, excluding conditional fields or test matrices which can vary. It prints out linting messages in JSON format for any discrepancies found, helping maintain standardized job configurations across similar workflows.",
            "total_tokens": 1053,
            "latency": 4.882183,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "3163bb03-f1a9-4011-826d-198d800f51fd"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `workflow_consistency_linter.py`, is to check for consistency of jobs between different GitHub workflows, specifically ensuring that jobs with the same `sync-tag` have identical configurations. The script achieves this by parsing YAML files, aggregating jobs with matching `sync-tag` values, and comparing their configurations to detect any inconsistencies. It reports any inconsistencies found as lint messages in JSON format, highlighting the paths and lines where the inconsistencies occur.",
            "total_tokens": 1064,
            "latency": 1.392245,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "ee705317-bc4f-4fe3-8564-928b16273d74"
        },
        "gpt-4.1": {
            "file_summary": "The main functionality of workflow_consistency_linter.py is to lint multiple GitHub Actions workflow YAML files to ensure that any jobs sharing the same sync-tag have identical job definitions, except for permitted differences (e.g., the if field or with.test-matrix). It parses the specified YAML workflow files, groups jobs by their sync-tag, and compares them, reporting inconsistencies as lint error messages in JSON format indicating the file and job causing the mismatch. This tool helps maintain synchronization and consistency between shared workflow job definitions across multiple files.",
            "total_tokens": 1081,
            "latency": 1.536632,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 134,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "1d134a8a-93e7-49e2-b905-63aa79e12016"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/linter/adapters/workflow_consistency_linter.py",
            "number_of_lines": 134,
            "project_name": "pytorch",
            "total_tokens": 1426,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.039555,
            "file_summary": "This Python script acts as a linter for GitHub Actions workflow files. It checks for consistency among jobs that share the same `sync-tag` across different workflow files. Jobs with the same `sync-tag` are expected to have identical configurations (excluding `if` and `test-matrix` fields), and inconsistencies are reported as lint errors.",
            "uuid": "c4718a7d-2359-486b-bae5-6897d080a783"
        },
        "gemini-2.5": {
            "file_name": "/tools/linter/adapters/workflow_consistency_linter.py",
            "number_of_lines": 134,
            "project_name": "pytorch",
            "total_tokens": 1613,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.387007,
            "file_summary": "This Python script acts as a linter for GitHub Actions workflow YAML files. It checks that jobs across different workflow files which share the same `sync-tag` value in their `with` section have identical configurations, ignoring specific fields like `if` conditions and `test-matrix`. If inconsistencies are detected between jobs with the same tag, it prints JSON-formatted error messages indicating the file and job causing the discrepancy.",
            "uuid": "039b7f4f-f8de-4b64-acd2-ed66d7b4b46e"
        },
        "Claude3.7": {
            "file_name": "/tools/linter/adapters/workflow_consistency_linter.py",
            "number_of_lines": 134,
            "project_name": "pytorch",
            "total_tokens": 1279,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.964448,
            "file_summary": "This Python file implements a linter that checks for consistency between GitHub workflow jobs that share the same \"sync-tag\". It analyzes YAML workflow files, identifies jobs with matching sync tags, and reports errors when these jobs have inconsistent configurations (with exceptions for certain fields like \"if\" and \"test-matrix\" which are allowed to differ). The linter outputs standardized JSON error messages when inconsistencies are found.",
            "uuid": "3e720c23-99af-4700-bee3-ba532f064742"
        },
        "mistral-small": {
            "file_name": "/tools/linter/adapters/workflow_consistency_linter.py",
            "number_of_lines": 134,
            "project_name": "pytorch",
            "total_tokens": 1344,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.546114,
            "file_summary": " The main functionality of the file \"workflow_consistency_linter.py\" is to check for consistency of jobs between different GitHub workflows. It does this by ensuring that any job with a specific `sync-tag` matches all other jobs with the same `sync-tag`. If a job doesn't match, it prints a lint message with an error severity.",
            "uuid": "6cba785f-ae18-41f7-a8fe-20d6534ca2e3"
        },
        "mistral-7b": {
            "file_name": "/tools/linter/adapters/workflow_consistency_linter.py",
            "number_of_lines": 134,
            "project_name": "pytorch",
            "total_tokens": 1357,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.193878,
            "file_summary": " The given Python file, named \"workflow\\_consistency\\_linter.py\", is designed to check for consistency of jobs between different GitHub workflows. It loads YAML files specified as arguments, extracts jobs with the same sync-tag, and ensures that the code for each job matches the baseline job with the same sync-tag. If any inconsistency is found, an error message is printed to the console.",
            "uuid": "47351002-1611-4af1-b7bc-24f301f9725f"
        },
        "cohere-v1": {
            "file_name": "/tools/linter/adapters/workflow_consistency_linter.py",
            "number_of_lines": 134,
            "project_name": "pytorch",
            "file_summary": "The Python script analyzes GitHub workflow files (with the YAML format) to check the consistency of jobs with a specific 'sync-tag'. It compares the jobs' content, ignoring some fields like 'if' and 'with['test-matrix']', and ensures they match a baseline job definition. Any inconsistencies are reported as errors using the LintMessage class and JSON output.",
            "total_tokens": 1131,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.113629,
            "uuid": "59095261-daf1-41da-bdc1-50999a92194b"
        }
    },
    "/torch/fx/experimental/migrate_gradual_types/constraint.py": {
        "nova-lite-v1": {
            "file_summary": "The `constraint.py` file defines a set of constraint classes and utility functions for representing and manipulating constraints related to tensor operations and dimensions in a symbolic manner. These constraints include binary operations, conjunctions, disjunctions, and specific operations like reshaping, transposing, and convolution. The classes provide methods for equality checks and string representations, facilitating the symbolic reasoning and manipulation of tensor constraints.",
            "total_tokens": 4627,
            "latency": 0.960443,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "d2e37c9b-39bc-4aca-8008-4556c9d09d70"
        },
        "titan_premier": {
            "file_summary": "The python file \"constraint.py\" contains a class hierarchy for representing constraints in a system. The constraints include conjunctions, disjunctions, products, and binary operations on tensors and dimensions. The file also includes helper functions for checking if a constraint is an algebraic expression or a boolean expression. The constraints are used to represent various operations such as index selection, transpose, and convolution. The file is part of a larger system for migrating gradual types in PyTorch.",
            "total_tokens": 97,
            "latency": 2.819873,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "1af4c2a2-c009-4955-9311-7e0c55551fac"
        },
        "gpt-4.5": {
            "file_summary": "This Python file defines a structured way to represent various constraints for tensor operations, primarily geared towards representing types, dimensions, and shape constraints in computations involving tensors. It provides a hierarchy of constraint classes (such as binary constraints for tensor and dimension constraints, reshaping, broadcasting, indexing, transposing, convolutions, pooling, and algebraic operations), along with helper classes for tensor, dimension, and boolean variables. This structured approach is likely designed to facilitate constraint checking, validation, or type inference for dynamic tensor computation scenarios, particularly in frameworks involving tensor shape manipulations or type checking.",
            "total_tokens": 4158,
            "latency": 7.521991,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "cfa565f0-0f18-41a4-a21c-8fae918f2444"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided python file \"constraint.py\" contains class definitions that represent various constraints applicable to tensors and dimensions in a program. These constraints include conjunctions, disjunctions, products, binary constraints, greatest upper bounds, reshape constraints, index selections, transpositions, convolutions, max pools, broadcasting, product calculations, and various variable types such as tensor variables (TVar), dimension variables (DVar), and boolean variables (BVar). The file also includes methods for comparison and string representation of the constraints.",
            "total_tokens": 4130,
            "latency": 1.615327,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "eadfd48a-2ea7-4e83-a98b-473c0b5ff9fa"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"constraint.py\" file is to define a set of constraint classes and related data structures for representing and manipulating constraints in a tensor type system. It includes various constraint types such as conjunctions, disjunctions, binary constraints, and specialized constraints for tensor operations like convolution and broadcasting. The file also defines helper classes for representing tensor variables, dimension variables, and boolean variables, as well as utility functions for checking constraint types.",
            "total_tokens": 5093,
            "latency": 3.731095,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "41ccec11-0e12-40d7-8b0a-8f892d5f098f"
        },
        "nova-pro-v1": {
            "file_summary": "The `constraint.py` file defines various classes representing different types of constraints, primarily focused on tensors and their dimensions within a computational graph context. It includes classes for conjunctions, disjunctions, binary operations, and specific constraints like reshaping, transposing, and calculating convolutions. The file also provides utility functions to check the type of constraints and variables.",
            "total_tokens": 4622,
            "latency": 1.335093,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "bb3a432b-62cc-498a-b754-e771fc1ebba5"
        },
        "gpt-4o": {
            "file_summary": "The file defines a system of classes and methods for representing and manipulating type constraints, including operations on tensor shapes, dimensions, and algebraic expressions, largely tailored for use in PyTorch's type inference and shape system. It includes specialized classes for handling conjunctions, disjunctions, binary constraints, tensor-specific constraints, and operations like reshaping, indexing, broadcasting, and convolution. The focus is on creating a structured representation of constraints for dynamic type analysis and tensor computation verification.",
            "total_tokens": 4137,
            "latency": 2.46364,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "a9f20758-7687-40c4-af99-aefdddc674b2"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a set of classes and functions for representing and manipulating constraints in the context of tensor operations, such as convolution, max pooling, and broadcasting. The constraints are used to enforce relationships between tensor dimensions, shapes, and values. The file provides a way to create, compare, and manipulate these constraints using various classes and functions.",
            "total_tokens": 4105,
            "latency": 3.752748,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "94de4037-207e-4dec-b7ea-2651f53b67b3"
        },
        "gpt-4": {
            "file_summary": "The Python file defines a `Constraint` class and various subclasses to represent different types of constraints in a system, likely in tensor computations based on the Torch library. Classes such as `Conj`, `Disj`, `BinaryConstraint`, and `CalcConv` suggest handling logical, relational, and computational constraints. Further, the use of classes like `TVar` and operations from `torch.fx.experimental` indicate integration with tensor types and dynamic typing, supporting flexible dimension and type operations within tensor computations.",
            "total_tokens": 4128,
            "latency": 2.931484,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "a336b2e3-b9a3-431b-a880-316166e15fa9"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the given Python file is to define a set of classes and functions for representing and manipulating constraints related to tensor operations, such as convolution, max pooling, and broadcasting. These constraints are used to reason about the properties of tensors and their relationships, and are likely used in the context of a larger system for optimizing or analyzing neural networks. The file defines a range of constraint classes, including `BinaryConstraint`, `Conj`, `Disj`, and `Prod`, as well as classes for specific tensor operations like `CalcConv` and `CalcMaxPool`.",
            "total_tokens": 4147,
            "latency": 2.22207,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "4241b0ca-dce1-405d-b00c-39dc0ebc452a"
        },
        "gpt-4.1": {
            "file_summary": "This file defines a system of constraint classes for representing and reasoning about tensor, dimension, and boolean constraints within symbolic shape/type inference, likely connected to PyTorch's FX tracing tools. It provides a variety of constraint types (e.g. conjunctions, disjunctions, binary operations, reshape and indexing constraints, convolution/maxpool/broadcasting operations) and variable types (tensor, dimension, boolean), along with utility functions to identify the category of a constraint. The primary purpose of the file is to formalize constraints and operations between tensors and their shapes to enable compile-time or symbolic reasoning about tensor computations.",
            "total_tokens": 4163,
            "latency": 2.058685,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 644,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b6181905-bde5-43a1-84ee-2877ec6e5ec7"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/fx/experimental/migrate_gradual_types/constraint.py",
            "number_of_lines": 644,
            "project_name": "pytorch",
            "total_tokens": 5075,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.905499,
            "file_summary": "This Python file defines a hierarchy of classes inheriting from `Constraint` to represent various types of constraints, primarily focused on tensor shapes and dimensions. It includes classes for logical combinations (Conj, Disj), binary operations, tensor/dimension bounds, and specific tensor operations like reshaping, indexing, convolution, and broadcasting. The file also defines variable types (`TVar`, `DVar`, `BVar`) used within these constraints and helper functions to categorize expressions.",
            "uuid": "6ea420c3-3e2c-44a3-b87f-d4dafd445ed9"
        },
        "gemini-2.5": {
            "file_name": "/torch/fx/experimental/migrate_gradual_types/constraint.py",
            "number_of_lines": 644,
            "project_name": "pytorch",
            "total_tokens": 5151,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.645665,
            "file_summary": "This Python file defines a system for representing type constraints, primarily for tensors and their dimensions within the PyTorch FX framework. It establishes a base `Constraint` class and numerous subclasses to model logical combinations (And/Or), binary operations, tensor-specific operations (like convolution, broadcasting, indexing), and dimension calculations. The file also defines variable types (TVar, DVar, BVar) to represent unknown tensors, dimensions, and booleans within these constraints.",
            "uuid": "5cc8af65-270e-4417-acf1-cf07b9ebf199"
        },
        "Claude3.7": {
            "file_name": "/torch/fx/experimental/migrate_gradual_types/constraint.py",
            "number_of_lines": 644,
            "project_name": "pytorch",
            "total_tokens": 5101,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.358502,
            "file_summary": "This file defines a constraint system for tensor operations in PyTorch, implementing various constraint classes that represent relationships between tensors and their dimensions. The constraint hierarchy includes basic logical constraints (Conjunction, Disjunction), tensor-specific constraints (broadcasting, convolution, max pooling), and variable types for tensors, dimensions, and boolean values. The system appears to be part of a type checking or static analysis framework for PyTorch, likely used to verify tensor shape compatibility during operations.",
            "uuid": "d5ca1666-63d2-4e59-8b43-7eb6e3eb4926"
        },
        "mistral-small": {
            "file_name": "/torch/fx/experimental/migrate_gradual_types/constraint.py",
            "number_of_lines": 644,
            "project_name": "pytorch",
            "total_tokens": 5238,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 3.18508,
            "file_summary": " The main functionality of the provided Python file, named \"constraint.py\", is to define various types of constraints used in a symbolic execution framework for PyTorch. These constraints are used to represent and manipulate the shapes and types of tensors in PyTorch programs. The file includes classes for basic constraints such as True and False, as well as more complex constraints such as conjunctions, disjunctions, and binary constraints. It also includes classes for specific constraints related to tensors, such as constraints for tensor shapes, tensor types, and tensor operations. The file uses the mypy type checker to allow untyped function definitions.",
            "uuid": "4f3c718f-7d44-4131-b361-e9c547202568"
        },
        "mistral-7b": {
            "file_name": "/torch/fx/experimental/migrate_gradual_types/constraint.py",
            "number_of_lines": 644,
            "project_name": "pytorch",
            "total_tokens": 5223,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 4.802781,
            "file_summary": " The given Python file defines a set of classes and functions for representing and manipulating constraints related to tensors and their dimensions. These constraints include binary operations, greatest upper bounds, can reshape, index select, transpose, and getting item from a tensor. The constraints are represented as instances of various subclasses of the Constraint class, such as BinaryConstraint, TGreatestUpperBound, CanReshape, IndexSelect, and Transpose. The file also includes helper functions for checking if a constraint is an algebraic expression or a boolean expression.",
            "uuid": "8bcb9341-7324-49b9-b2ca-3fed8bcbff01"
        },
        "cohere-v1": {
            "file_name": "/torch/fx/experimental/migrate_gradual_types/constraint.py",
            "number_of_lines": 644,
            "project_name": "pytorch",
            "file_summary": "The Python script defines several classes representing different constraint types, such as conjunctions, disjunctions, binary constraints, and tensor constraints, along with helper functions for creating and evaluating these constraints. These constraints seem to be focused on describing relationships between tensors and their dimensions. The script also defines variable classes like TVar, DVar, and BVar for representing tensor, dimension, and boolean variables.",
            "total_tokens": 4343,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.728353,
            "uuid": "9afce29b-c1d0-41d1-8c0f-62efc10490fa"
        }
    },
    "/torch/nn/modules/rnn.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file defines various recurrent neural network (RNN) modules using PyTorch, including RNN, LSTM, and GRU, along with their cell counterparts. The modules support both forward and bidirectional processing, dropout for regularization, and different non-linearities such as tanh and ReLU. They are designed to handle variable-length sequences and can be stacked to create deeper architectures. The file also includes utility functions and classes for managing the weights and parameters of these RNN modules.",
            "total_tokens": 20734,
            "latency": 2.087994,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "6f7000cf-c1b0-463b-bea6-380713360cb1"
        },
        "titan_premier": {
            "file_summary": "The given Python file, `rnn.py`, contains the implementation of various Recurrent Neural Network (RNN) modules, including RNN, LSTM, GRU, RNNCell, LSTMCell, and GRUCell. These modules are subclasses of the `Module` class in PyTorch and are used to create RNN models.\n\nThe `RNNBase` class is the base class for all RNN modules and defines common attributes and methods. It initializes the weights and biases of the RNN module and provides methods for checking input and hidden states.\n\nThe `RNN` class is a specific implementation of the RNN module that uses the tanh or ReLU non-linearity. It takes in the input size, hidden size, number of layers, bias, batch first, dropout, bidirectional, and proj size as arguments.\n\nThe `LSTM` class is a specific implementation of the LSTM module that uses the LSTM cell. It takes in the input size, hidden size, number of layers, bias, batch first, dropout, bidirectional, and proj size as arguments.\n\nThe `GRU` class is a specific implementation of the GRU module that uses the GRU cell. It takes in the input size, hidden size, number of layers, bias, batch first, dropout, and bidirectional as arguments.\n\nThe `RNNCellBase` class is the base class for all RNN cell modules and defines common attributes and methods. It initializes the weights and biases of the RNN cell module and provides methods for checking input and hidden states.\n\nThe `RNNCell` class is a specific implementation of the RNN cell module that uses the tanh or ReLU non-linearity. It takes in the input size, hidden size, bias, and nonlinearity as arguments.\n\nThe `LSTMCell` class is a specific implementation of the LSTM cell module. It takes in the input size, hidden size, and bias as arguments.\n\nThe `GRUCell` class is a specific implementation of the GRU cell module. It takes in the input size, hidden size, and bias as arguments.\n\nOverall, the `rnn.py` file provides a comprehensive set of RNN modules and cells that can be used to create RNN models in PyTorch.",
            "total_tokens": 491,
            "latency": 13.756909,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "f4018c09-b67d-49ef-81d7-0a8dfbabf36b"
        },
        "gpt-4.5": {
            "file_summary": "This Python file implements recurrent neural network (RNN) modules including standard RNNs, Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRU), along with their corresponding cell-based variants using PyTorch. The core implementation is structured around an `RNNBase` class that handles initialization, parameter management, and internal checks for input and hidden states across these modules. The provided classes are ready-to-use PyTorch neural network layers designed for processing sequential data in tasks such as language modeling or sequence analysis.",
            "total_tokens": 18462,
            "latency": 12.468074,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d8a298b1-1876-469a-b79f-58bc09a0990a"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"rnn.py\" file is to implement various Recurrent Neural Network (RNN) architectures and their cell variants in PyTorch. Specifically, it defines classes for RNN, LSTM, and GRU models, as well as their corresponding cell implementations (RNNCell, LSTMCell, and GRUCell). These classes provide the core building blocks for creating and using recurrent neural networks in PyTorch, including forward pass computations, parameter initialization, and handling of different input formats.",
            "total_tokens": 22509,
            "latency": 6.333466,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "fefa776c-3d3a-4189-aa33-eb9373f1623f"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `rnn.py` file is to define various recurrent neural network (RNN) modules and cells in PyTorch, including RNN, LSTM, GRU, and their corresponding cell versions, along with their initialization, parameter management, and forward pass implementations.",
            "total_tokens": 20694,
            "latency": 3.949821,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "f777b881-0bc9-4030-b0a5-ecfc0f9db9eb"
        },
        "gpt-4o": {
            "file_summary": "The file `rnn.py` provides the implementation of core Recurrent Neural Network (RNN) modules, including general-purpose RNNs (`RNNBase`, `RNN`), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU), along with their corresponding cell-level classes (`RNNCell`, `LSTMCell`, `GRUCell`). These classes serve as the backbone for defining and managing recurrent layers in PyTorch, supporting features like multi-layer architecture, bidirectionality, dropout, and custom initialization of weights. The file also includes utility methods for handling parameter management, input validation, compatibility with batch-first tensors, and enabling efficient forward computations using the cuDNN library when possible.",
            "total_tokens": 18503,
            "latency": 4.880212,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b5f09a32-e081-4136-b737-45cd44620d8f"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to implement various types of Recurrent Neural Networks (RNNs) in PyTorch, including RNN, LSTM, and GRU.",
            "total_tokens": 18168,
            "latency": 50.650161,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "753a5bd0-5ac0-4e78-b4ae-c6bcd68a8c7d"
        },
        "gpt-4": {
            "file_summary": "The provided Python file defines several classes and functions for constructing and working with different types of recurrent neural networks (RNNs) such as vanilla RNNs, Long Short-Term Memory networks (LSTMs), and Gated Recurrent Unit networks (GRUs). It includes base classes and cell variations for each RNN type, and contains implementations for initializing RNN parameters, managing data flow through the network, and executing forward passes. The code leverages PyTorch's functionalities and modules for its operations, focusing on aspects like weight initializations, forward computations, and parameter management.",
            "total_tokens": 18244,
            "latency": 6.775216,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "5b1bf906-dcf9-445d-b768-2cc2bfa67b8b"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement various types of Recurrent Neural Network (RNN) cells and layers, including RNN, LSTM, and GRU, as part of the PyTorch library. These classes provide the core functionality for building and training RNN models, including forward passes, weight initialization, and input validation. The file also includes documentation and example usage for each type of RNN cell and layer.",
            "total_tokens": 18221,
            "latency": 4.602141,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "2c6d6ccd-44fb-4c0e-833b-2870d3f49086"
        },
        "gpt-4.1": {
            "file_summary": "This file defines the core PyTorch neural network modules for recurrent neural networks, including multi-layer and single-layer (cell) implementations of vanilla RNNs, LSTMs, and GRUs. It provides classes (RNN, LSTM, GRU, RNNCell, LSTMCell, GRUCell) that encapsulate all parameters, initialization, and the forward computation needed for these recurrent units, supporting options for bias, batch layout, dropout, bidirectionality, and projections (for LSTM). The file handles both batched and unbatched inputs, allows packed sequences for variable-length input, and manages parameter flattening for backend optimization.",
            "total_tokens": 18488,
            "latency": 3.028873,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "a9c875dc-802e-406a-89aa-c2618dfbaf96"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/modules/rnn.py",
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "total_tokens": 21449,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.311906,
            "file_summary": "This Python file provides implementations of standard Recurrent Neural Network (RNN) modules for the PyTorch library. It defines classes for multi-layer RNN, LSTM, and GRU networks, including support for features like bidirectional processing, dropout, and batch handling. Additionally, it includes corresponding single-step cell implementations for RNN, LSTM, and GRU.",
            "uuid": "f2490566-cf66-486a-9f12-1306d563f1bc"
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/modules/rnn.py",
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "total_tokens": 21543,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 5.83553,
            "file_summary": "This Python file defines core PyTorch modules for Recurrent Neural Networks (RNNs), including base classes (`RNNBase`, `RNNCellBase`) and specific implementations like `RNN`, `LSTM`, and `GRU`. It also provides the corresponding single-step cell versions (`RNNCell`, `LSTMCell`, `GRUCell`) for more granular control. These modules handle sequence processing, hidden/cell state management, parameter initialization, and support features like multi-layer stacking, bidirectionality, dropout, and batching options.",
            "uuid": "5259447b-2fe8-4556-b4d6-874a47cd4c58"
        },
        "Claude3.7": {
            "file_name": "/torch/nn/modules/rnn.py",
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "total_tokens": 22491,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 6.031665,
            "file_summary": "The file \"rnn.py\" implements various Recurrent Neural Network (RNN) architectures in PyTorch, including standard RNN, LSTM (Long Short-Term Memory), and GRU (Gated Recurrent Unit) models, along with their cell variants for single-step operations. It provides both full sequence processing modules and individual cell implementations with comprehensive parameter management, forward pass logic, and utilities for handling batched and packed sequence inputs.",
            "uuid": "f07069cd-1912-49ea-b0c9-b1f746d3bdc0"
        },
        "mistral-small": {
            "file_name": "/torch/nn/modules/rnn.py",
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "total_tokens": 25915,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 51.850699,
            "file_summary": " The given Python file is a PyTorch implementation of a multi-layer long short-term memory (LSTM) RNN. The LSTM RNN takes in an input sequence and computes the output sequence using LSTM cells. The LSTM cells can be either unidirectional or bidirectional, and the user can specify the number of layers, the input size, the hidden size, the bias, the batch first flag, the dropout probability, and the projection size. The LSTM RNN also supports packed variable length sequences.\n\nThe LSTM RNN is implemented using PyTorch's autograd module, which allows for automatic differentiation of the computations. The LSTM RNN uses PyTorch's Variable class to represent the input and output sequences, as well as the hidden and cell states. The LSTM RNN also uses PyTorch's nn.Module class to define the LSTM cells and the LSTM RNN itself.\n\nThe LSTM RNN is initialized with random weights and biases, which are sampled from a uniform distribution. The weights and biases are stored as PyTorch Variables, and can be updated using backpropagation through time (BPTT) or other optimization algorithms.\n\nThe LSTM RNN can be used for a variety of tasks, such as language modeling, machine translation, and speech recognition. It is a powerful tool for modeling sequential data, and has been shown to achieve state-of-the-art results on many benchmark tasks.\n\nHere is an example of how to use the LSTM RNN to process an input sequence:\n```\nimport torch\nimport torch.nn as nn\n\n# Define the LSTM RNN\nlstm_rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=False)\n\n# Define the input sequence\ninput = torch.randn(5, 3, 10)\n\n# Define the initial hidden and cell states\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\n\n# Process the input sequence using the LSTM RNN\noutput, (hn, cn) = lstm_rnn(input, (h0, c0))\n\n# Print the output sequence\nprint(output)\n```\nIn this example, the LSTM RNN takes in an input sequence of shape (5, 3, 10), where 5 is the number of time steps, 3 is the batch size, and 10 is the number of input features. The LSTM RNN also takes in initial hidden and cell states of shape (2, 3, 20), where 2 is the number of layers, 3 is the batch size, and 20 is the number of hidden features. The LSTM RNN processes the input sequence using two LSTM layers, and returns the output sequence of shape (5, 3, 40), where 40 is the number of output features (20 features per layer). The LSTM RNN also returns the final hidden and cell states of shape (2, 3, 20).\n\nThe LSTM RNN can also be used with packed variable length sequences, which can be more efficient for processing sequences of varying lengths. Here is an example of how to use the LSTM RNN with packed sequences:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils.rnn as rnn_utils\n\n# Define the LSTM RNN\nlstm_rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=False)\n\n# Define the input sequence as a packed sequence\ninput = torch.randn(3, 5, 10)  # 3 sequences of lengths 5, 3, and 4\ninput_lengths = torch.tensor([5, 3, 4])\ninput_packed = rnn_utils.pack_padded_sequence(input, input_lengths)\n\n# Define the initial hidden and cell states\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\n\n# Process the input sequence using the LSTM RNN\noutput_packed, (hn, cn) = lstm_rnn(input_packed, (h0, c0))\n\n# Unpack the output sequence\noutput, output_lengths = rnn_utils.pad_packed_sequence(output_packed)\n\n# Print the output sequence\nprint(output)\n```\nIn this example, the LSTM RNN takes in an input sequence as a packed sequence, where each sequence has a different length. The LSTM RNN processes the input sequence using two LSTM layers, and returns the output sequence as a packed sequence. The output sequence is then unpacked using the `pad_packed_sequence` function, which pads the sequences to the maximum length and returns the padded sequences and their lengths. The LSTM RNN also returns the final hidden and cell states of shape (2, 3, 20).\n\nThe LSTM RNN can also be used with bidirectional LSTM cells, which can capture both forward and backward dependencies in the input sequence. Here is an example of how to use the LSTM RNN with bidirectional LSTM cells:\n```\nimport torch\nimport torch.nn as nn\n\n# Define the LSTM RNN with bidirectional LSTM cells\nlstm_rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=False, bidirectional=True)\n\n# Define the input sequence\ninput = torch.randn(5, 3, 10)\n\n# Define the initial hidden and cell states\nh0 = torch.randn(4, 3, 20)  # 4 states for the forward and backward directions\nc0 = torch.randn(4, 3, 20)\n\n# Process the input sequence using the LSTM RNN\noutput, (hn, cn) = lstm_rnn(input, (h0, c0))\n\n# Print the output sequence\nprint(output)\n```\nIn this example, the LSTM RNN takes in an input sequence of shape (5, 3, 10), where 5 is the number of time steps, 3 is the batch size, and 10 is the number of input features. The LSTM RNN also takes in initial hidden and cell states of shape (4, 3, 20), where 4 is the number of states (2 states for the forward direction and 2 states for the backward direction), 3 is the batch size, and 20 is the number of hidden features. The LSTM RNN processes the input sequence using two bidirectional LSTM layers, and returns the output sequence of shape (5, 3, 80), where 80 is the number of output features (20 features per layer per direction). The LSTM RNN also returns the final hidden and cell states of shape (4, 3, 20).\n\nThe LSTM RNN can also be used with projection layers, which can reduce the dimensionality of the hidden states. Here is an example of how to use the LSTM RNN with projection layers:\n```\nimport torch\nimport torch.nn as nn\n\n# Define the LSTM RNN with projection layers\nlstm_rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=False, projection_size=10)\n\n# Define the input sequence\ninput = torch.randn(5, 3, 10)\n\n# Define the initial hidden and cell states\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\n\n# Process the input sequence using the LSTM RNN\noutput, (hn, cn) = lstm_rnn(input, (h0, c0))\n\n# Print the output sequence\nprint(output)\n```\nIn this example, the LSTM RNN takes in an input sequence of shape (5, 3, 10), where 5 is the number of time steps, 3 is the batch size, and 10 is the number of input features. The LSTM RNN also takes in initial hidden and cell states of shape (2, 3, 20), where 2 is the number of layers, 3 is the batch size, and 20 is the number of hidden features. The LSTM RNN processes the input sequence using two LSTM layers with projection layers, and returns the output sequence of shape (5, 3, 10), where 10 is the number of output features (10 features per layer). The LSTM RNN also returns the final hidden and cell states of shape (2, 3, 10).\n\nThe LSTM RNN can also be used with dropout layers, which can prevent overfitting by randomly dropping out some of the hidden units during training. Here is an example of how to use the LSTM RNN with dropout layers:\n```\nimport torch\nimport torch.nn as nn\n\n# Define the LSTM RNN with dropout layers\nlstm_rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=False, dropout=0.5)\n\n# Define the input sequence\ninput = torch.randn(5, 3, 10)\n\n# Define the initial hidden and cell states\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\n\n# Process the input sequence using the LSTM RNN\noutput, (hn, cn) = lstm_rnn(input, (h0, c0))\n\n# Print the output sequence\nprint(output)\n```\nIn this example, the LSTM RNN takes in an input sequence of shape (5, 3, 10), where 5 is the number of time steps, 3 is the batch size, and 10 is the number of input features. The LSTM RNN also takes in initial hidden and cell states of shape (2, 3, 20), where 2 is the number of layers, 3 is the batch size, and 20 is the number of hidden features. The LSTM RNN processes the input sequence using two LSTM layers with dropout layers, and returns the output sequence of shape (5, 3, 20), where 20 is the number of output features (20 features per layer). The LSTM RNN also returns the final hidden and cell states of shape (2, 3, 20).\n\nThe LSTM RNN can also be used with custom non-linearities, such as ReLU or ELU. Here is an example of how to use the LSTM RNN with a custom non-linearity:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define a custom non-linearity\nclass CustomNonlinearity(nn.Module):\n    def forward(self, x):\n        return F.relu(x) + F.elu(x)\n\n# Define the LSTM RNN with a custom non-linearity\nlstm_rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=False, nonlinearity=CustomNonlinearity())\n\n# Define the input sequence\ninput = torch.randn(5, 3, 10)\n\n# Define the initial hidden and cell states\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\n\n# Process the input sequence using the LSTM RNN\noutput, (hn, cn) = lstm_rnn(input, (h0, c0))\n\n# Print the output sequence\nprint(output)\n```\nIn this example, the LSTM RNN takes in an input sequence of shape (5, 3, 10), where 5 is the number of time steps, 3 is the batch size, and 10 is the number of input features. The LSTM RNN also takes in initial hidden and cell states of shape (2, 3, 20), where 2 is the number of layers, 3 is the batch size, and 20 is the number of hidden features. The LSTM RNN processes the input sequence using two LSTM layers with a custom non-linearity, which is a combination of ReLU and ELU, and returns the output sequence of shape (5, 3, 20), where 20 is the number of output features (20 features per layer). The LSTM RNN also returns the final hidden and cell states of shape (2, 3, 20).\n\nThe LSTM RNN can also be used with custom weight initializations, such as Xavier or orthogonal initialization. Here is an example of how to use the LSTM RNN with a custom weight initialization:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n# Define a custom weight initialization\ndef custom_weight_init(module):",
            "uuid": "b508cbc1-535b-45be-9cf2-87d720112066"
        },
        "mistral-7b": {
            "file_name": "/torch/nn/modules/rnn.py",
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "total_tokens": 23133,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 7.711636,
            "file_summary": " This Python file defines several classes for recurrent neural networks (RNNs) using PyTorch. The main classes are `RNNBase`, `RNN`, `LSTM`, and `GRU`, which correspond to base RNN, vanilla RNN, Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU) networks, respectively. The file also includes a `RNNCellBase` class, which is a base class for defining RNN cells.\n\nThe `RNNBase` class is a base class for RNN modules, providing common functionality such as initialization and utility methods for parameter storage management. It does not implement the forward method.\n\nThe `RNN`, `LSTM`, and `GRU` classes inherit from `RNNBase` and define the forward pass for their respective RNN architectures. The `RNN` class implements a simple Elman RNN with tanh or ReLU non-linearity. The `LSTM` and `GRU` classes implement long short-term memory and gated recurrent unit networks, respectively, with their specific update rules.\n\nThe file also includes helper functions for applying permutations to tensors and deprecated functions that are being replaced by the `index_select` function. Additionally, it defines some constants and properties for the RNN classes.",
            "uuid": "26942b08-2c1d-4eaa-877d-6b8ef7542fc9"
        },
        "cohere-v1": {
            "file_name": "/torch/nn/modules/rnn.py",
            "number_of_lines": 1826,
            "project_name": "pytorch",
            "file_summary": "The given python file defines several classes of Recurrent Neural Networks (RNN) layers and cells such as RNN, LSTM, GRU, RNNCell, LSTMCell and GRUCell. These classes allow users to construct deep learning models with recurrent connections.",
            "total_tokens": 19729,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 3.182031,
            "uuid": "2c6c101f-6ea3-4551-9ae0-f69002ef98a9"
        }
    },
    "/torch/nn/utils/parametrizations.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file defines three functions: `orthogonal`, `weight_norm`, and `spectral_norm`. These functions are used to apply orthogonal, weight, and spectral normalization, respectively, to the weights of a given PyTorch module. The file provides a way to constrain the weights of neural network layers to specific manifolds, which can help stabilize training and improve the performance of models, especially in generative adversarial networks (GANs).",
            "total_tokens": 7256,
            "latency": 1.968425,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "28411cfe-3d0b-4b50-9376-334dbdc54955"
        },
        "titan_premier": {
            "file_summary": "The file `parametrizations.py` contains three main functionalities:\n\n1. `orthogonal`: This function applies an orthogonal or unitary parametrization to a matrix or a batch of matrices. It uses three different orthogonal maps: `matrix_exp`, `cayley`, and `householder`. The `matrix_exp` and `cayley` maps are applied to a skew-symmetric matrix to give an orthogonal matrix, while the `householder` map computes a product of Householder reflectors. The function also supports the dynamic trivialization framework, which helps the convergence of the parametrized layer at the expense of some extra memory use.\n\n2. `weight_norm`: This function applies weight normalization to a parameter in the given module. Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. It replaces the parameter specified by the `name` argument with two parameters: one specifying the magnitude and one specifying the direction.\n\n3. `spectral_norm`: This function applies spectral normalization to a parameter in the given module. Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by reducing the Lipschitz constant of the model. It approximates the spectral norm of the weight tensor by performing one iteration of the power method every time the weight is accessed.\n\nOverall, the file provides various parametrization techniques for matrices and weight tensors, which can be useful for improving the performance and stability of deep learning models.",
            "total_tokens": 329,
            "latency": 9.431468,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "f178d1b9-a252-4f2b-a317-d30fd9ec9d4c"
        },
        "gpt-4.5": {
            "file_summary": "This Python file implements three tensor parametrizations: orthogonal (or unitary) parametrization via matrix exponential, Cayley map, or Householder reflectors; weight normalization, which reparameterizes weights to separate magnitude and direction; and spectral normalization, which constrains weights by their spectral norm to stabilize GAN training. These parametrizations leverage PyTorch's parametrization framework, enabling modules to have mathematical constraints automatically enforced during training.",
            "total_tokens": 6509,
            "latency": 7.758492,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "11f11a16-7f99-4f9e-b698-34b5f9e5eded"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the given Python file is to provide parametrizations for orthogonal/unitary matrices, weight normalization, and spectral normalization in PyTorch. It includes functions such as ```orthogonal```, ```weight_norm```, and ```spectral_norm``` that allow users to apply these parametrizations to specific modules and parameters within a neural network, enabling better training stability and convergence through constraint enforcement and reparameterization. The file contains implementations of the parametrization logic, including the use of different techniques such as the matrix exponential, Cayley transform, Householder reflections, and power iterations for spectral normalization, to ensure the desired properties are maintained during optimization.",
            "total_tokens": 6548,
            "latency": 8.982642,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "bd658291-c124-4c2f-96f3-78d2df48c9e2"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"parametrizations.py\" file is to implement various parametrization techniques for neural network layers, specifically focusing on orthogonal, weight normalization, and spectral normalization. It provides functions to apply these parametrizations to module weights, allowing for more stable and controlled training of neural networks, particularly in scenarios like Generative Adversarial Networks (GANs).",
            "total_tokens": 7821,
            "latency": 6.322465,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d82dd167-8dee-48d3-82e8-ed708e1877b4"
        },
        "nova-pro-v1": {
            "file_summary": "The `parametrizations.py` file provides functions to apply various normalizations to parameters within PyTorch modules. Specifically, it includes implementations for orthogonal parametrization, weight normalization, and spectral normalization, each designed to impose constraints on the parameters to improve training stability and performance.",
            "total_tokens": 7225,
            "latency": 1.51616,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3e621416-b05f-41a7-a4cc-67f40e1a5b46"
        },
        "gpt-4o": {
            "file_summary": "The `parametrizations.py` file provides implementations of advanced parameterization techniques for PyTorch models, including orthogonal transformations, weight normalization, and spectral normalization. It defines custom modules and utilities to parametrize or modify weight tensors in terms of orthogonality, norm decomposition, or spectral constraints, enabling easier optimization and improving the stability of network training. These methods are especially useful for tasks like regularization, improving convergence, or stabilizing adversarial training setups like GANs.",
            "total_tokens": 6519,
            "latency": 2.054276,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "e038fb92-ec32-4532-81cf-6dead5cea777"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide three functions: `orthogonal`, `weight_norm`, and `spectral_norm`, which are used to apply different types of parametrizations to the weights of a PyTorch module. These parametrizations are used to constrain the weights of the module to have certain properties, such as being orthogonal or having a specific norm. The functions take in a PyTorch module, the name of the weight to be parametrized, and other optional arguments, and return the modified module with the parametrization applied.",
            "total_tokens": 6534,
            "latency": 6.401872,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "2d71f4b6-9e25-4a65-af6b-44a3d46ae856"
        },
        "gpt-4": {
            "file_summary": "The provided Python file, named `parametrizations.py`, defines three main parametrization techniques for neural network weights in PyTorch: orthogonal, weight norm, and spectral norm. These techniques are implemented as classes (`_Orthogonal`, `_WeightNorm`, `_SpectralNorm`) and corresponding functions (`orthogonal`, `weight_norm`, `spectral_norm`) that can be applied to `torch.nn.Module` parameters. These parametrizations enable training stability and better convergence by enforcing specific constraints on the weights\u2014orthogonality, weight normalization, and spectral normalization, respectively. The methods provided also handle complex tensors and multi-dimensional weight matrices, ensuring the appropriate constraints and transformations are applied to maintain the desired parametrizations during training.",
            "total_tokens": 6563,
            "latency": 4.008268,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "76beb801-fd77-45d9-8ce8-3b5242c710d5"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement various parametrization techniques for PyTorch modules, specifically orthogonal, weight norm, and spectral norm parametrizations. These techniques are used to constrain or normalize the weights of neural network layers, which can help improve the stability and performance of the network. The file defines several classes and functions that implement these parametrizations, including `_Orthogonal`, `_WeightNorm`, and `_SpectralNorm`, as well as the `orthogonal`, `weight_norm`, and `spectral_norm` functions that apply these parametrizations to PyTorch modules.",
            "total_tokens": 6544,
            "latency": 3.297651,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "03dddab0-bacf-4fa3-b77b-e63752e26070"
        },
        "gpt-4.1": {
            "file_summary": "The file provides utilities to apply parameterizations to PyTorch neural network layers, specifically implementing orthogonal (or unitary), spectral norm, and weight norm constraints on module weights via parametrization hooks. It defines custom parametrization classes for each method, enabling their registration to arbitrary layer parameters, ensuring the weight matrices maintain the desired mathematical properties (e.g., orthogonality, unit spectral norm, or normalized magnitude/direction) during training. These parameterizations help in stabilizing and regularizing deep learning models, supporting advanced optimization and convergence behavior by leveraging PyTorch's parametrize API.",
            "total_tokens": 6540,
            "latency": 2.072698,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 629,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "dafd6f01-7340-4e82-bb85-9baefe29b752"
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/utils/parametrizations.py",
            "number_of_lines": 629,
            "project_name": "pytorch",
            "total_tokens": 7720,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 14.851756,
            "file_summary": "This Python file defines functions (`orthogonal`, `weight_norm`, `spectral_norm`) that apply specific mathematical parametrizations to tensors within PyTorch modules, typically network weights. These parametrizations enforce constraints like orthogonality, decouple weight magnitude and direction, or limit the spectral norm using PyTorch's `parametrize` utility. The overall goal is often to improve training stability or enforce desired properties on the learned parameters.",
            "uuid": "cf82fa20-de03-47df-9da9-b395837495bb"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/utils/parametrizations.py",
            "number_of_lines": 629,
            "project_name": "pytorch",
            "total_tokens": 7563,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.764533,
            "file_summary": "This Python file provides implementations for common weight parametrizations in PyTorch modules, leveraging the `torch.nn.utils.parametrize` utility. It defines functions to apply orthogonal, weight normalization, and spectral normalization constraints to module weights. These parametrizations modify how the weight tensor is computed from underlying parameters to enforce desired mathematical properties like orthogonality, unit norm, or a spectral norm of one.",
            "uuid": "f8c7ae23-cd3d-4723-b5d8-132b3540b351"
        },
        "Claude3.7": {
            "file_name": "/torch/nn/utils/parametrizations.py",
            "number_of_lines": 629,
            "project_name": "pytorch",
            "total_tokens": 7827,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 17.869411,
            "file_summary": "The file \"parametrizations.py\" implements various weight parametrization techniques for neural networks in PyTorch, specifically orthogonal parametrization, weight normalization, and spectral normalization. These parametrizations constrain network weights to have specific mathematical properties (orthogonality, normalized weight vectors, or bounded spectral norm) which can improve training stability and model performance by enforcing desirable constraints on the parameter space.",
            "uuid": "e7b9b609-c365-4f1e-8cf2-ee94f6769a2b"
        },
        "mistral-small": {
            "file_name": "/torch/nn/utils/parametrizations.py",
            "number_of_lines": 629,
            "project_name": "pytorch",
            "total_tokens": 8438,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 9.547571,
            "file_summary": " The given Python file is a module that provides functions for applying various types of normalization to the weights of a PyTorch neural network module. The main functionality of the file is provided by three functions: `orthogonal`, `weight_norm`, and `spectral_norm`.\n\n1. `orthogonal`: This function applies orthogonal or unitary parametrization to a matrix or a batch of matrices in a given PyTorch module. It takes four arguments: the module to which the parametrization is to be applied, the name of the weight parameter to be parametrized, the type of orthogonal map to be used (default: \"matrix\\_exp\" for square or complex matrices, \"householder\" otherwise), and a boolean flag indicating whether to use the dynamic trivialization framework (default: True). The function returns the original module with the orthogonal parametrization registered to the specified weight.\n2. `weight_norm`: This function applies weight normalization to a parameter in a given PyTorch module. It takes three arguments: the module containing the parameter to be normalized, the name of the parameter to be normalized, and the dimension over which to compute the norm (default: 0). The function returns the original module with the weight norm hook registered to the specified parameter.\n3. `spectral_norm`: This function applies spectral normalization to a parameter in a given PyTorch module. It takes five arguments: the module containing the parameter to be normalized, the name of the parameter to be normalized, the number of power iterations to be used for approximating the spectral norm (default: 1), a small epsilon value for numerical stability (default: 1e-12), and the dimension corresponding to the number of outputs (default: 0 for most modules, 1 for ConvTranspose{1,2,3}d modules). The function returns the original module with the spectral norm hook registered to the specified parameter.\n\nThe file also includes several helper classes and functions that are used by the main functions. These include the `_Orthogonal`, `_WeightNorm`, and `_SpectralNorm` classes, which implement the orthogonal, weight, and spectral normalizations, respectively. The file also includes several utility functions for reshaping matrices, computing matrix norms, and performing power iterations.\n\nOverall, the file provides a convenient way to apply various types of normalization to the weights of a PyTorch neural network module, which can help improve the stability and convergence of the training process.",
            "uuid": "e9eb9db5-60c5-4328-abce-8d55ee085670"
        },
        "mistral-7b": {
            "file_name": "/torch/nn/utils/parametrizations.py",
            "number_of_lines": 629,
            "project_name": "pytorch",
            "total_tokens": 8027,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.688132,
            "file_summary": " The given Python file defines functions for applying orthogonal, weight normal, and spectral norm transformations to the weights of neural networks. These transformations help in stabilizing the training of discriminators in Generative Adversarial Networks (GANs) by reducing the Lipschitz constant of the model. The file includes classes and functions for computing orthogonal matrices using different methods like matrix exponentiation, Cayley map, and Householder reflection, as well as functions for weight normalization and spectral normalization. The functions are registered as parametrizations for PyTorch modules to be applied to the specified weight parameters.",
            "uuid": "ccbd1fef-efab-4dc7-871d-7b31f74587de"
        },
        "cohere-v1": {
            "file_name": "/torch/nn/utils/parametrizations.py",
            "number_of_lines": 629,
            "project_name": "pytorch",
            "file_summary": "The code defines three functions, orthogonal, weight_norm, and spectral_norm, which add constraints to the parameters of a PyTorch module. These constraints help stabilize the training process. orthogonal applies an orthogonal or unitary parametrization to a matrix or batch of matrices, weight_norm decouples the magnitude of a weight tensor from its direction, and spectral_norm applies spectral normalization to a parameter, which helps reduce the Lipschitz constant of the model.",
            "total_tokens": 6864,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 6.992194,
            "uuid": "c89701fd-3ad3-4bba-b536-9dbb388aab6c"
        }
    },
    "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file is a part of a larger framework for optimizing matrix multiplication operations in PyTorch. It defines functions to decompose certain matrix multiplication operations (like `bmm` and `mm`) into more manageable parts based on specific conditions, such as tensor dimensions and device type. The decomposition is intended to improve performance by breaking down large matrix multiplications into smaller, more efficient operations.",
            "total_tokens": 1936,
            "latency": 0.951171,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "cfec2fe3-1e1f-4704-9c6b-d3fdab2227f3"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"decompose_mem_bound_mm.py\" is to decompose matrix multiplication operations (mm, bmm, addmm) into smaller sub-operations, based on certain conditions related to the dimensions of the input matrices and the device they are on. The file uses the torch.fx library to match and replace the matrix multiplication operations with the decomposed sub-operations. The file also includes logging statements to track the number of decompositions performed.",
            "total_tokens": 99,
            "latency": 2.480415,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "abb924bd-5405-4d21-b23a-1bc17e569262"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines transformations for decomposing certain large or specifically shaped matrix multiplication operations (including matrix-matrix multiplication like `mm`, batched matrix multiplication `bmm`, and addition-matrix multiplication `addmm`) in PyTorch models into more memory-efficient, element-wise operations followed by reductions. These decompositions are conditionally applied based on the input tensor dimensions, devices (GPU or CPU), and configurable thresholds. When certain conditions are met, matrix operations are replaced by equivalent computations that leverage element-wise multiplication and summation, potentially improving execution efficiency on memory-bound operations.",
            "total_tokens": 1649,
            "latency": 6.214685,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "6793606c-9b82-4d6f-982d-b93596c3a48b"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file \"decompose_mem_bound_mm.py\" defines functions to decompose matrix multiplication operations based on certain conditions like matrix shape and device type. It also includes pattern matcher passes for decomposing specific torch operations such as 'bmm', 'addmm', and 'mm'. The decomposed operations aim to optimize memory usage during matrix multiplications by breaking down the computation into smaller pieces.",
            "total_tokens": 1591,
            "latency": 1.093899,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "7d9110ea-17ae-4ccf-a793-293baf4a82c8"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"decompose_mem_bound_mm.py\" file is to decompose memory-bound matrix multiplication operations in PyTorch. It defines patterns and conditions for decomposing various matrix multiplication operations (bmm, addmm, mm) into more memory-efficient computations, particularly for large matrices on CUDA devices or small matrices on CPU. The file also includes utility functions for checking device compatibility and validating node metadata, as well as registering graph patterns for the decomposition process.",
            "total_tokens": 2084,
            "latency": 3.434122,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "1cb48c3e-968b-4fbb-84ba-1ce58ccc5667"
        },
        "nova-pro-v1": {
            "file_summary": "The file `decompose_mem_bound_mm.py` defines functions to decompose specific matrix multiplication operations (`bmm`, `addmm`, and `mm`) in PyTorch graphs under certain conditions related to tensor dimensions and device type, aiming to optimize memory usage. It utilizes pattern matching to identify and replace these operations within the graph. The decomposition strategy and conditions are configurable via the `config.post_grad_fusion_options`.",
            "total_tokens": 1950,
            "latency": 1.386178,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "ea009e73-7131-4fca-b05c-05073e2ffda6"
        },
        "gpt-4o": {
            "file_summary": "The script `decompose_mem_bound_mm.py` defines strategies and patterns to decompose certain matrix operations (`mm`, `bmm`, and `addmm`) in PyTorch for optimization purposes. It implements checks based on tensor shapes, devices (CPU or CUDA), and configurable thresholds to determine whether decomposition should be applied. This decomposition is done using pattern matching to replace operations with alternative implementations for better memory or computational efficiency.",
            "total_tokens": 1618,
            "latency": 1.77769,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "bb71283f-5b59-41b4-91b9-9cabf465466c"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to decompose matrix multiplication (mm) and batched matrix multiplication (bmm) operations in PyTorch graphs, specifically for CUDA and CPU devices, based on certain conditions such as matrix dimensions and device type. The file defines several functions to check if decomposition is necessary and to perform the decomposition, and registers these functions as graph patterns to be applied during the graph transformation process.",
            "total_tokens": 1602,
            "latency": 3.58213,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d5cf3451-62d3-449f-be54-80e7ef023435"
        },
        "gpt-4": {
            "file_summary": "The Python file `decompose_mem_bound_mm.py` is primarily focused on implementing pattern matching and decomposition strategies for matrix multiplication operations (e.g., `mm`, `bmm`, and `addmm`) in PyTorch, specifically tailored for optimization on different hardware devices like CPU and CUDA. It uses configuration settings to define decomposition strategies based on dimensions, and integrates these decompositions into a pattern matching registration system using decorators, which allows for transforming specific computational graph patterns according to the defined decomposition functions. The file also includes logging and debugging utilities to assist with the monitoring of the transformations applied during the decomposition process.",
            "total_tokens": 1634,
            "latency": 3.145152,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "adc4a6ad-08f2-452c-b40b-fc13ee3f56cb"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to decompose matrix multiplication operations (such as `bmm`, `addmm`, and `mm`) in PyTorch into more efficient forms, specifically for large matrices on CUDA devices. The decomposition is done based on certain conditions, including the size of the matrices and the device they are on, with the goal of improving performance. The file uses PyTorch's `torch.fx` module to analyze and modify the computation graph, and it registers pattern matching passes to apply the decomposition transformations.",
            "total_tokens": 1627,
            "latency": 1.797343,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0d7fc6d5-9449-4b3c-9746-e42f6de61811"
        },
        "gpt-4.1": {
            "file_summary": "The file decompose_mem_bound_mm.py defines pattern-matching and decomposition logic for certain memory-bound matrix multiplication operations (specifically mm, bmm, and addmm) in PyTorch computation graphs. It provides heuristics to detect when these operations are likely memory-bound based on tensor shapes and device, and replaces them with manually decomposed operations optimized for specific scenarios. This functionality is integrated into PyTorch's FX graph rewriting infrastructure to enable automatic transformation and optimization during model graph compilation.",
            "total_tokens": 1630,
            "latency": 1.507814,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 161,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b8054d54-b1cc-45b1-aa0d-991ac8087e1a"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py",
            "number_of_lines": 161,
            "project_name": "pytorch",
            "total_tokens": 2320,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.006164,
            "file_summary": "This Python file implements graph pattern matchers within the PyTorch FX framework to identify specific matrix multiplication operations (`aten.mm`, `aten.bmm`, `aten.addmm`). It checks if the input tensors meet certain shape and device conditions, particularly for memory-bound cases on CUDA or specific CPU patterns. If the conditions are met, the matrix multiplication is decomposed into an equivalent expression using element-wise multiplication and summation.",
            "uuid": "ce615025-9152-4de0-a77b-a958dfc3c158"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py",
            "number_of_lines": 161,
            "project_name": "pytorch",
            "total_tokens": 2525,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.668705,
            "file_summary": "This Python file defines `torch.fx` graph patterns to identify specific memory-bound matrix multiplication operations (`aten.bmm`, `aten.addmm`, `aten.mm`). Based on configurable tensor shape and device criteria (primarily large first dimension and small other dimensions on CUDA, or specific small shapes on CPU), it decomposes these operations into element-wise multiplications followed by sums. This decomposition aims to potentially improve performance in specific scenarios by replacing the original matrix multiplication call within the graph.",
            "uuid": "d175f3c6-584c-4018-ab25-ad92b3444e64"
        },
        "Claude3.7": {
            "file_name": "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py",
            "number_of_lines": 161,
            "project_name": "pytorch",
            "total_tokens": 2079,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.34115,
            "file_summary": "The file \"decompose_mem_bound_mm.py\" implements a pattern matching system to decompose memory-bound matrix multiplication operations in PyTorch. It defines functions to identify and transform specific matrix multiplication patterns (mm, bmm, addmm) into more memory-efficient operations when certain dimensional conditions are met, particularly for large first dimensions and small other dimensions. The transformations are applied selectively based on device type (CUDA or CPU) and tensor shape characteristics to optimize performance.",
            "uuid": "1023b882-a0fc-4774-8eac-a9bbe04d2b00"
        },
        "mistral-small": {
            "file_name": "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py",
            "number_of_lines": 161,
            "project_name": "pytorch",
            "total_tokens": 2162,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 63.661561,
            "file_summary": " The main functionality of the file \"decompose_mem_bound_mm.py\" is to decompose matrix multiplication operations in PyTorch computational graphs. It defines functions to check if a decomposition is necessary based on the shape and device of the matrices, and if so, it replaces the original matrix multiplication operation with a decomposed version. This is done to optimize memory usage and improve performance in certain scenarios. The file also includes logging and configuration options.",
            "uuid": "cb5f0cac-a4b5-4394-aa47-5783eb3221f1"
        },
        "mistral-7b": {
            "file_name": "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py",
            "number_of_lines": 161,
            "project_name": "pytorch",
            "total_tokens": 2171,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.301499,
            "file_summary": " This Python file is a part of a deep learning framework, specifically for Torch, and it defines functions for decomposing batched matrix multiplication (BMM) and matrix multiplication (MM) operations based on their input shapes and sizes. The main functionality of the file includes defining functions for checking if the operations should be decomposed based on their input shapes and device, and registering graph patterns for BMM, addmm, and MM operations to replace them with decomposed versions if certain conditions are met.",
            "uuid": "7ef3fd2a-29ed-491d-84f1-5c21fe221a0f"
        },
        "cohere-v1": {
            "file_name": "/torch/_inductor/fx_passes/decompose_mem_bound_mm.py",
            "number_of_lines": 161,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, named decompose_mem_bound_mm.py, focuses on optimizing memory-bound matrix operations in PyTorch. It defines functions to check if certain matrix multiplications should be decomposed and replaced with more memory-efficient alternatives. The script patterns matrix multiplication operations using torch.fx and replaces them with decomposed implementations when specific conditions are met, logging the transformations.",
            "total_tokens": 1908,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.28408,
            "uuid": "3efbe4ac-1179-49cd-900f-c81228e73e7c"
        }
    },
    "/torch/autograd/profiler.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"profiler.py\" file is to provide a profiling tool for PyTorch autograd operations. It allows developers to measure the performance of their PyTorch code by tracking the execution time of various operations, memory usage, and other metrics. The profiler includes a context manager (`profile`) to wrap code sections for profiling, a decorator (`record_function`) to label specific code blocks, and support for emitting annotations for external profilers like Intel VTune and NVTX. The profiler can also accumulate events across multiple profiling cycles and export profiling results in various formats.",
            "total_tokens": 12551,
            "latency": 2.015811,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "22eddcc7-2df9-4e51-ba47-021e3e4a2541"
        },
        "titan_premier": {
            "file_summary": "The file `profiler.py` is a part of the PyTorch library and contains various functionalities related to profiling. It includes classes and functions that allow users to profile their PyTorch code, analyze the performance, and identify bottlenecks.\n\nThe main functionalities of the file are:\n\n1. `profile`: A context manager that manages the autograd profiler state and holds a summary of results. It allows users to enable and disable profiling, specify profiling options, and access profiling results.\n\n2. `record_function`: A context manager and function decorator that adds a label to a code block or function when running the autograd profiler. It is useful for annotating code blocks or functions for better understanding of the profiling results.\n\n3. `emit_itt`: A context manager that makes every autograd operation emit an ITT range. It is useful for profiling with Intel(R) VTune Profiler.\n\n4. `emit_nvtx`: A context manager that makes every autograd operation emit an NVTX range. It is useful for profiling with nvprof.\n\n5. `load_nvprof`: A function that loads an nvprof trace file and parses autograd annotations. It allows users to analyze the profiling results from an nvprof trace file.\n\n6. `EnforceUnique`: A class that raises an error if a key is seen more than once. It is used to ensure uniqueness of keys in certain contexts.\n\n7. `parse_nvprof_trace`: A function that parses an nvprof trace file and returns a list of FunctionEvents. It allows users to extract and analyze the profiling results from an nvprof trace file.\n\n8. `KinetoStepTracker`: A class that provides an abstraction for incrementing the step count globally. It is used to maintain the step count for different requesters and ensure correct increments of the step count.\n\nOverall, the `profiler.py` file provides a comprehensive set of tools for profiling PyTorch code and analyzing the performance. It allows users to enable and disable profiling, specify profiling options, annotate code blocks or functions, and analyze profiling results from different profiling tools.",
            "total_tokens": 486,
            "latency": 12.220889,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "ae9431c9-dd24-4c74-a04a-3127dbf28dd8"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements a comprehensive performance profiling module using PyTorch's autograd system, specifically designed to profile CPU and hardware accelerator activities (e.g., CUDA, XPU, MTIA, HPU). It allows users to measure runtime, memory utilization, floating-point operations (FLOPs), and stack information of PyTorch operations within a context manager or decorators like `profile`, `record_function`, `emit_nvtx`, and `emit_itt`. Additionally, the module includes utilities to parse, summarize, and export profiling results for detailed performance analysis and debugging.",
            "total_tokens": 10597,
            "latency": 8.680461,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8b65fd7e-a1e0-43cf-b8b6-9174fc41127d"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given `profiler.py` file contains classes and functions related to the autograd profiler in PyTorch. It includes functionality for profiling code blocks, emitting ITT and NVTX ranges, as well as loading and parsing nvprof traces. Additionally, it provides a `KinetoStepTracker` class for tracking step counts globally. The main functionality revolves around context managers for profiling, managing step counts, and parsing trace files for analysis and visualization.",
            "total_tokens": 10539,
            "latency": 2.016911,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "77806295-7265-407f-a5a6-0ae062267e0c"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"profiler.py\" file is to provide a profiling system for PyTorch, allowing users to measure and analyze the performance of their code. Here's a summary in three sentences:\n\n1. The file implements a profiling context manager and various utility functions for collecting, processing, and visualizing performance data in PyTorch applications.\n\n2. It supports different profiling modes, including CPU profiling, CUDA profiling, and integration with external profiling tools like Intel VTune and NVIDIA's nvprof.\n\n3. The profiler can record function events, memory usage, and device-specific metrics, and provides methods for generating performance reports and exporting data in various formats for further analysis.",
            "total_tokens": 13421,
            "latency": 6.518788,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "fc4b63bd-e399-46e4-b543-b9fc75827a18"
        },
        "nova-pro-v1": {
            "file_summary": "The `profiler.py` file provides various context managers and utilities for profiling PyTorch operations, including detailed timing, memory usage, and event tracing. It supports both CPU and various device-specific profiling, with options for recording shapes, flops, and stack traces. Additionally, it includes mechanisms for annotating code blocks and integrating with external profiling tools like Intel VTune and NVIDIA nvprof.",
            "total_tokens": 12513,
            "latency": 1.915225,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "9cfa0021-cdcd-4f54-8e33-abaee6d8163e"
        },
        "gpt-4o": {
            "file_summary": "The `profiler.py` file provides tools and utilities for profiling PyTorch operations, enabling developers to measure and analyze the performance of their code. It includes context managers (`profile`, `record_function`, `emit_itt`, `emit_nvtx`) that allow users to collect runtime and memory usage data during execution, optionally including details like input shapes, module hierarchy, and floating-point operations. Additionally, the file provides mechanisms for tracking optimization steps (`KinetoStepTracker`), exporting profiling results in various formats, and interacting with external profiling tools like Intel VTune and NVIDIA nvprof.",
            "total_tokens": 10600,
            "latency": 2.837901,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "276798d7-283a-4b73-a716-4c34c3fac507"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide a set of tools for profiling and tracing the execution of PyTorch code, including the ability to record function calls, emit ITT (Intel Trace Tool) and NVTX (NVIDIA Tools Extension) ranges, and load nvprof traces. The file also includes a context manager for managing autograd profiler state and holding a summary of results.",
            "total_tokens": 10533,
            "latency": 48.310432,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "01d3043d-de1f-4332-9387-863170e393fd"
        },
        "gpt-4": {
            "file_summary": "The Python file defines a profiling utility leveraging PyTorch's autograd profiler capabilities. It provides a context manager `profile` to gather performance data about PyTorch operations, measuring execution times and memory usage on CPU and GPU devices. The profiler supports various configurations, such as including device-specific events or accumulating events across multiple runs. Additional utilities include functions to emit profiling markers for ITT and NVTX tools, and handle related profiling functionalities for better traceability and debugging of PyTorch applications.",
            "total_tokens": 10548,
            "latency": 4.558577,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "faeda09a-69a4-4808-9a0e-529bc1ed4ca0"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement a profiling tool for PyTorch, allowing users to measure the execution time and memory usage of their models and identify performance bottlenecks. The profiler provides various features, including CPU and CUDA profiling, memory profiling, and support for Intel VTune Amplifier and NVIDIA Nsight Systems. The profiler can be used as a context manager or as a decorator to profile specific sections of code.",
            "total_tokens": 10544,
            "latency": 9.17942,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a1b3e871-59e2-4abe-a901-c2584e2d48b0"
        },
        "gpt-4.1": {
            "file_summary": "The provided \"profiler.py\" file implements a robust profiling toolset for PyTorch, allowing developers to record and analyze the performance of PyTorch operations, including profiling CPU and device activities, tracking memory usage, and annotating code ranges for visualization in external tools like Intel VTune and NVIDIA's nvprof. It provides high-level context managers such as profile, record_function, emit_nvtx, and emit_itt to instrument and collect detailed execution traces, and utility functions to parse and process profiling results from both internal (Kineto) and nvprof traces. Additional classes and utilities within the module help enforce event uniqueness, manage global step tracking across requesters, and present comprehensive summaries and exports of collected profiling data.",
            "total_tokens": 10626,
            "latency": 2.721833,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "673aee3c-fce6-4474-b420-e2f6ddb87d1b"
        },
        "gemini-2.5": {
            "file_name": "/torch/autograd/profiler.py",
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "total_tokens": 13092,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.574149,
            "file_summary": "This Python file provides tools for profiling PyTorch code execution, primarily through the `profile` context manager which records performance metrics like execution time, memory usage, and operator details for CPU and various device types (CUDA, XPU, etc.). It allows users to analyze performance bottlenecks by collecting data on function calls, input shapes, FLOPs, and call stacks. Additionally, the file includes utilities like `record_function` for adding custom labels to code blocks and context managers (`emit_itt`, `emit_nvtx`) for exporting traces compatible with external profiling tools like Intel VTune and NVIDIA Nsight.",
            "uuid": "f82f016e-f8fd-452a-91a4-5d174b7b6b0c"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/autograd/profiler.py",
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "total_tokens": 13099,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.413076,
            "file_summary": "This Python file provides tools for profiling PyTorch code execution, including CPU and device activities. It defines context managers like `profile`, `record_function`, `emit_itt`, and `emit_nvtx` to enable and control profiling sessions and add custom annotations. The module also includes utilities for processing and exporting profiling results, such as generating tables, Chrome traces, and parsing nvprof traces.",
            "uuid": "9e95eeb8-62f0-4caa-b0b6-c0ea9a9fadbd"
        },
        "Claude3.7": {
            "file_name": "/torch/autograd/profiler.py",
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "total_tokens": 13383,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 5.777179,
            "file_summary": "The profiler.py file provides a comprehensive profiling system for PyTorch operations. It defines classes and functions for tracking and analyzing the performance of PyTorch code, including CPU and GPU operations. The main functionality includes the `profile` context manager for collecting execution statistics, `record_function` for labeling code blocks during profiling, and utilities for exporting profiling data to formats like Chrome tracing. It also includes specialized profilers like `emit_nvtx` and `emit_itt` for NVIDIA and Intel profiling tools respectively.",
            "uuid": "63dad3fe-97a2-494e-9559-a01f95b4f56b"
        },
        "mistral-small": {
            "file_name": "/torch/autograd/profiler.py",
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "total_tokens": 16822,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 48.818701,
            "file_summary": " The main functionality of the given Python file is to provide profiling capabilities for PyTorch. It defines several classes and functions that allow users to profile their PyTorch code and analyze the performance of their models.\n\nThe main classes defined in the file are:\n\n1. `profile`: A context manager that manages autograd profiler state and holds a summary of results. It records events of functions being executed in C++ and exposes those events to Python.\n2. `record_function`: A context manager/function decorator that adds a label to a code block/function when running autograd profiler.\n3. `emit_itt`: A context manager that makes every autograd operation emit an ITT range. It is useful when running the program under Intel(R) VTune Profiler.\n4. `emit_nvtx`: A context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof.\n5. `load_nvprof`: A function that opens an nvprof trace file and parses autograd annotations.\n6. `EnforceUnique`: A class that raises an error if a key is seen more than once.\n7. `parse_nvprof_trace`: A function that parses an nvprof trace file and returns a list of FunctionEvent objects.\n8. `KinetoStepTracker`: A class that provides an abstraction for incrementing the step count globally.\n\nThe file also defines several functions that are used internally by the classes, such as `_run_on_profiler_start`, `_run_on_profiler_stop`, `_set_is_profiler_enabled`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_entry`, `_rewrite_name`, `_get_privateuse1_backend_name`, `_filter_name`, `_filter_stack_",
            "uuid": "5503efc2-dda6-4a4d-a093-178e28cef4f4"
        },
        "mistral-7b": {
            "file_name": "/torch/autograd/profiler.py",
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "total_tokens": 13845,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 3.938856,
            "file_summary": " The main functionality of the given Python file is to provide profiling capabilities for PyTorch autograd operations. It includes context managers for CPU, ITT, and NVTX profiling, as well as functions for recording functions, emitting ITT and NVTX ranges, and loading NVprof traces. The file also includes a KinetoStepTracker class for managing step counts for different requesters and triggering kineto steps when necessary.",
            "uuid": "0763a713-9415-4893-b332-f728687b170e"
        },
        "cohere-v1": {
            "file_name": "/torch/autograd/profiler.py",
            "number_of_lines": 1209,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, named profiler.py, is a module that offers several profiling tools for analyzing the performance of PyTorch operations and functions. It defines classes and functions that enable users to measure and examine the execution time, memory usage, and other metrics of PyTorch code blocks and functions. The primary purpose is to help developers optimize their models and identify performance bottlenecks.",
            "total_tokens": 11722,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 2.500569,
            "uuid": "21f8262e-7823-4119-83b2-9d72658df6fc"
        }
    },
    "/torch/distributed/tensor/parallel/fsdp.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"fsdp.py\" file is to provide extensions and utilities for handling distributed tensors in a 2D Fully Sharded Data Parallel (FSDP) setup, specifically focusing on tensor flattening, chunking, and redistribution across devices. It includes methods for transforming tensors to and from sharded and distributed formats, managing tensor metadata, and facilitating communication between distributed processes. This file is designed to work with PyTorch's distributed package, particularly for optimizing model training across multiple devices or nodes by efficiently managing data distribution and synchronization.",
            "total_tokens": 4033,
            "latency": 1.402418,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "24b8f2e0-70a8-46fd-a561-ce429667ee0d"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to provide a class DTensorExtensions which is an extension for FSDP (Fully Sharded Data Parallel) and TP (Tensor Parallelism). The class has methods to flatten and unflatten tensors, chunk tensors and DTensors, preprocess state dictionary, and all gather DTensors. The file also has utility functions to get boxes for tensors, create ShardedTensorMetadata from DTensor, get process group from DTensor, rewrite ShardingSpec if needed, and create chunked ShardedTensor or DTensor. The file is a part of PyTorch's distributed package and is used for distributed training of models.",
            "total_tokens": 141,
            "latency": 3.931634,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "da1203b1-008a-42d1-a25f-f7886bb121bd"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements functionality related to PyTorch's Fully Sharded Data Parallel (FSDP) technique, particularly focusing on tensor partitioning, sharding logic, and state dict handling using DTensor and ShardedTensor types. It defines helper functions and a main class \"DTensorExtensions\" derived from \"FSDPExtensions,\" which provides operations to manage data parallelism and tensor parallelism in distributed training scenarios. Key operations include flattening/unflattening tensors, chunking tensors across device meshes, and redistributing DTensors to facilitate efficient distributed training.",
            "total_tokens": 3407,
            "latency": 8.990164,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "48c6fbec-87e6-4694-a3c3-a0216720f569"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided Python file `fsdp.py` is a module that contains functions and classes related to handling sharded tensors in distributed deep learning using the FullyShardedDataParallel (FSDP) technique. The file includes functions for creating sharded tensors, chunking tensors for distribution across multiple devices, all-gathering sharded tensors in the FSDP dimension, and transforming tensors for state dictionary loading. Additionally, the file defines a `DTensorExtensions` class that extends FSDPExtensions and provides methods for transforming and processing tensors in the distributed setting.",
            "total_tokens": 3362,
            "latency": 1.502411,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "f58c4b8f-eb52-456e-acac-590bbb811639"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"fsdp.py\" file is to implement the DTensorExtensions class, which extends the FSDPExtensions for handling Distributed Tensors (DTensors) in the context of Fully Sharded Data Parallel (FSDP) training. This file provides utility functions for tensor manipulation, including sharding, chunking, and gathering operations, specifically tailored for distributed training scenarios involving DTensors and FSDP. The code also includes helper functions for creating and managing sharded tensors and their metadata in a distributed environment.",
            "total_tokens": 4441,
            "latency": 4.237489,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "f57582ec-1f89-4bbd-b0d2-5c11de1ad406"
        },
        "nova-pro-v1": {
            "file_summary": "The `fsdp.py` file provides extensions and utilities for handling Distributed Tensors (`DTensor`) and Sharded Tensors (`ShardedTensor`) within the Fully Sharded Data Parallelism (FSDP) framework in PyTorch. It includes functions for flattening, unflattening, chunking tensors, and managing state dictionaries, aimed at optimizing distributed training across multiple devices.",
            "total_tokens": 3998,
            "latency": 1.294451,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "2f01b4b1-c485-495f-851b-5bd4f7aa9dba"
        },
        "gpt-4o": {
            "file_summary": "The \"fsdp.py\" file provides utilities for managing Fully Sharded Data Parallel (FSDP) and Tensor Parallel (TP) operations in PyTorch, focusing on distributing and managing tensors across devices efficiently. It includes methods for chunking, sharding, flattening, and all-gather operations on tensors (both standard and distributed tensors) using device meshes and distributed process groups. Additionally, the file defines the `DTensorExtensions` class for customizing tensor handling, specifically tailored for 2D FSDP + TP scenarios.",
            "total_tokens": 3400,
            "latency": 2.429268,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "bcaf0177-f7ca-4f8b-8e7a-cbaaa82ab2f8"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide extensions for PyTorch's Fully Sharded Data Parallel (FSDP) module to work with Distributed Tensors (DTensors). The extensions include methods for chunking tensors, loading state dictionaries, and all-gathering DTensors. The file defines a class `DTensorExtensions` that inherits from `FSDPExtensions` and provides the necessary functionality for FSDP to work with DTensors.",
            "total_tokens": 3346,
            "latency": 4.732086,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "855477c2-d82e-4591-911e-6a548a699b6e"
        },
        "gpt-4": {
            "file_summary": "The Python file `fsdp.py` primarily deals with distributed training using sharded tensors in PyTorch. It provides functions and classes to manage data distribution and computation across different devices and processes, specifically focusing on sharded versions of tensors and distributed tensors (DTensors). The file includes utility functions for creating, managing sharding metadata, reshaping tensors according to device meshes, and handling state dictionaries in fully sharded data-parallel (FSDP) setups. Additionally, it incorporates extensions for PyTorch's FSDP, enabling optimized tensor operations in distributed environments.",
            "total_tokens": 3365,
            "latency": 3.327466,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "e2e25ad1-5b5f-48fe-b4bb-61ba712b8655"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement extensions for Fully Sharded Data Parallelism (FSDP) with Distributed Tensors (DTensors) in PyTorch, enabling the integration of FSDP with Tensor Parallelism (TP) for 2D parallelism. The file defines a class `DTensorExtensions` that inherits from `FSDPExtensions` and provides methods for tensor flattening, unflattening, chunking, and all-gathering, specifically designed to work with DTensors. These extensions aim to facilitate the use of FSDP with TP, allowing for more efficient and flexible parallelization of deep learning models.",
            "total_tokens": 3390,
            "latency": 2.81118,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "4f3c28e6-a8c7-4a11-8932-4251bf99d726"
        },
        "gpt-4.1": {
            "file_summary": "The file fsdp.py defines utilities and extension mechanisms for integrating FSDP (Fully Sharded Data Parallel) with DTensor, ShardedTensor, and device mesh approaches in PyTorch distributed training. It provides functions for chunking, sharding, gathering, and transforming tensors (both standard and DTensor) appropriately across distributed devices, enabling efficient tensor handling and parallelization strategies (such as 2D FSDP + TP). The DTensorExtensions class implements the required FSDPExtensions interface to manage tensor flattening/unflattening, sharding, and device-aware tensor allocation, facilitating seamless distributed model state management.",
            "total_tokens": 3419,
            "latency": 2.110755,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 391,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8452b007-219d-4b0f-be90-4798568c51a3"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/tensor/parallel/fsdp.py",
            "number_of_lines": 391,
            "project_name": "pytorch",
            "total_tokens": 4993,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 5.476781,
            "file_summary": "This Python file defines `DTensorExtensions`, a class implementing `FSDPExtensions` to integrate PyTorch's DTensor and ShardedTensor with the FSDP framework. It provides methods for sharding, flattening, unflattening, and state management of tensors, particularly supporting configurations with 2D device meshes like combined FSDP and Tensor Parallelism. The code includes helper functions to manage tensor metadata and sharding logic for these distributed tensor types within the FSDP context.",
            "uuid": "889d2841-96e9-4b48-9ee2-7b0811548c1e"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/tensor/parallel/fsdp.py",
            "number_of_lines": 391,
            "project_name": "pytorch",
            "total_tokens": 4690,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.143324,
            "file_summary": "This file defines extensions to PyTorch's Fully Sharded Data Parallel (FSDP) framework to enable compatibility with `DTensor`s, which are used for tensor parallelism across a `DeviceMesh`. It provides helper functions and the `DTensorExtensions` class to manage the specific sharding, flattening/unflattening, and state dictionary handling required when using `DTensor`s within an FSDP context. This integration allows combining FSDP data parallelism with tensor parallelism for training large models.",
            "uuid": "7000b481-3916-43a8-b66c-35cee8adfd9a"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/tensor/parallel/fsdp.py",
            "number_of_lines": 391,
            "project_name": "pytorch",
            "total_tokens": 4420,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 6.476309,
            "file_summary": "The file implements DTensorExtensions, a class that extends FSDPExtensions to support Fully Sharded Data Parallel (FSDP) with Tensor Parallelism (TP) in PyTorch distributed training. It provides functionality for handling DTensor operations including flattening/unflattening tensors, chunking tensors across devices, and gathering distributed tensors, which enables efficient 2D parallelism for large-scale deep learning models.",
            "uuid": "68db1068-bcef-404c-a6c7-1d1072ed465c"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/tensor/parallel/fsdp.py",
            "number_of_lines": 391,
            "project_name": "pytorch",
            "total_tokens": 4446,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 4.186682,
            "file_summary": " The main functionality of the provided Python file, `fsdp.py`, is to define a class called `DTensorExtensions` which extends the `FSDPExtensions` class. This class is used for 2D FSDP (Fully Sharded Data Parallelism) + TP (Tensor Parallelism) in PyTorch. The class provides methods for flattening and unflattening tensors, chunking tensors and DTensor, pre-loading state dictionary, and all-gathering DTensor. These methods are used to handle the distribution and parallelization of tensors across multiple devices in a distributed training setup.",
            "uuid": "3a6a7991-bc3c-4ed0-b08f-ff8e713b9635"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/tensor/parallel/fsdp.py",
            "number_of_lines": 391,
            "project_name": "pytorch",
            "total_tokens": 4457,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 4.465875,
            "file_summary": " This Python file is a part of PyTorch's Fully Sharded Data Parallel (FSDP) implementation. It provides functions to handle and manipulate ShardedTensors and DTensors, which are specialized tensor types for distributed deep learning training using FSDP. The main functionality of the file includes:\n\n1. Defining helper functions to extract tensor information and create ShardMetadata and ShardedTensorMetadata.\n2. Implementing functions to chunk tensors and DTensors for distributed processing.\n3. Creating an extension class `DTensorExtensions` for handling FSDP-specific tensor operations, such as flattening and unflattening.",
            "uuid": "88a678d1-ec79-4843-818a-1a452c9c1528"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/tensor/parallel/fsdp.py",
            "number_of_lines": 391,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, fsdp.py, is part of the PyTorch library and extends the functionality of distributed training. It defines functions for managing and transforming sharded tensors, which are tensors distributed across multiple devices or nodes. The script enables operations like chunking tensors, converting tensors to a sharded format, and gathering sharded tensors from different devices. Additionally, it introduces the DTensorExtensions class for handling tensor flattening and unflattening, as well as providing methods for chunking and gathering tensors in distributed settings.",
            "total_tokens": 3819,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.940518,
            "uuid": "0460acd4-8ea0-4ffe-b889-ae129f40f61e"
        }
    },
    "/tools/stats/upload_utilization_stats/upload_utilization_stats.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `upload_utilization_stats.py` script is to download utilization logs from an S3 bucket, convert these logs into a database-friendly format, and then upload the processed data back to S3. The script includes functionalities to parse command-line arguments, handle JSON data, and perform data transformations using pandas. It also supports dry-run and debug modes for testing and troubleshooting.",
            "total_tokens": 4082,
            "latency": 1.817755,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d098ba55-eb1c-4165-8ac0-256878682e66"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"upload_utilization_stats.py\" is to convert raw log data into a database model and upload it to an S3 bucket. The script fetches raw log data from an S3 bucket, converts it into a log model, and then converts it into a database model. It also generates test segments from utilization records and uploads the converted data to an S3 bucket. The script can be run with command line arguments to specify the workflow run ID, workflow run attempt, workflow name, job ID, job name, and repository. The script also supports dry-run and debug modes.",
            "total_tokens": 127,
            "latency": 5.832449,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "7df350c1-82b0-4f36-8ef0-9f5b65b89a7a"
        },
        "gpt-4.5": {
            "file_summary": "The Python script fetches CI job utilization logs from AWS S3, processes and converts them into structured data models (metadata and time series data), and identifies continuous segments of executed Python commands. It ultimately uploads the processed utilization stats back into S3 in a structured format for storage and analysis. The script supports running in debug and dry-run modes for safer testing and verification.",
            "total_tokens": 3372,
            "latency": 5.057978,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "123ada24-8275-4cb7-9ace-c481e806d71a"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file \"upload_utilization_stats.py\" handles the conversion and upload of utilization data to an S3 bucket. It includes functionalities such as generating test segments from utilization records, converting log models to database models, handling raw log data retrieval from S3, and uploading the converted data to the S3 bucket. The script also supports command line arguments for workflow information and has classes and methods for data conversion and uploading.",
            "total_tokens": 3361,
            "latency": 1.585529,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "e7a9f62b-1145-49e6-995a-53c38db266b0"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"upload_utilization_stats.py\" file is to process and upload utilization data from GitHub workflow runs to Amazon S3. It fetches raw log data from S3, converts it to a log model, then to a database model, and finally uploads the processed data back to S3. The script includes functionality for generating test segments, converting utilization records, and handling metadata and time series data for CI/CD workflow utilization statistics.",
            "total_tokens": 4548,
            "latency": 5.454349,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "840960f2-074e-4305-ba28-ba4dcecd2f20"
        },
        "nova-pro-v1": {
            "file_summary": "The `upload_utilization_stats.py` script is designed to process utilization data logs from a specified GitHub workflow, convert them into a database-friendly format, and upload them to an S3 bucket. It includes functionalities for downloading artifacts, segmenting utilization records, converting log models to database models, and handling dry-run and debug modes.",
            "total_tokens": 4069,
            "latency": 1.473245,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "67e74bf5-3acf-4599-afbf-d65871381c87"
        },
        "gpt-4o": {
            "file_summary": "The file `upload_utilization_stats.py` provides functionality to process and manage utilization statistics for specific workflows, primarily by downloading raw utilization logs from S3, converting them into structured database models, and subsequently uploading the processed data back to S3. It includes classes to generate segments of utilization data based on activity, transform raw logs into database-friendly schemas, and handle the complete data flow, including validation and debugging options. This script is designed to be run as a command-line tool, with various arguments for workflow and job identifiers, and supports both production and dry-run modes. ",
            "total_tokens": 3413,
            "latency": 2.733887,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "2b36c32c-7d5d-4b47-930d-840f725b4d70"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to upload utilization data to Amazon S3. It fetches raw log data from S3, converts it to a log model, then converts it to a database model, and finally uploads it to S3. The file also includes classes for generating segments from utilization records, converting utilization log models to database models, and uploading utilization data to S3.",
            "total_tokens": 3360,
            "latency": 3.824086,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "ef825db1-d8af-4230-84b0-98a20c697b07"
        },
        "gpt-4": {
            "file_summary": "The Python script \"upload_utilization_stats.py\" is designed to handle and process utilization data, specifically for command line Python usage in workflows, and upload this data to an AWS S3 bucket. It includes functionality to download raw utilization logs, parse these logs into data models, generate segmentation for continuous command usage, and convert these into a database model format for storage. The script supports arguments for workflow identification, debug options, and a dry-run mode to simulate uploads without actual data transfer. The script uses Pandas for data manipulation and handles JSON data for easy transfer and storage.",
            "total_tokens": 3391,
            "latency": 3.141176,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "fed2a4b9-c8e9-499c-8216-98fd5bde8e96"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `upload_utilization_stats.py`, is to collect utilization data from GitHub workflow logs, convert it into a database model, and upload it to Amazon S3. The script fetches raw log data from S3, processes it into a log model, and then converts it into a database model before uploading it to S3 for storage and potential analysis.",
            "total_tokens": 3362,
            "latency": 2.118446,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "1e1aa52c-b188-497f-9c43-2a9f55f0fe40"
        },
        "gpt-4.1": {
            "file_summary": "The main functionality of upload_utilization_stats.py is to extract and process raw utilization log artifacts from S3 associated with CI workflows, convert them into structured database models including time series and metadata (such as resource usage and detected command segments), and then upload these processed statistics back to S3 for further analysis or storage. It achieves this through a command-line interface that accepts workflow-related identifiers, handles log parsing, data validation, and various error scenarios, and supports both dry-run and debug modes. The script also provides mechanisms for segmenting usage data (e.g., identifying Python command segments) to enrich the utilization statistics.",
            "total_tokens": 3421,
            "latency": 1.895835,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 444,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "6fe1ce04-4532-4534-a92a-37fa03ce7a76"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/stats/upload_utilization_stats/upload_utilization_stats.py",
            "number_of_lines": 444,
            "project_name": "pytorch",
            "total_tokens": 4977,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.412693,
            "file_summary": "This Python script downloads raw utilization logs from S3 artifacts associated with a specific CI workflow run and job. It processes these logs, identifies continuous execution segments (specifically for Python commands), and converts the data into structured database models for metadata and time series. Finally, the script uploads these processed utilization statistics to a dedicated S3 bucket for storage and analysis.",
            "uuid": "93766f43-2226-4e2a-9063-558c9f22a456"
        },
        "gemini-2.5": {
            "file_name": "/tools/stats/upload_utilization_stats/upload_utilization_stats.py",
            "number_of_lines": 444,
            "project_name": "pytorch",
            "total_tokens": 4767,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.440765,
            "file_summary": "This script downloads CI job usage logs from S3 artifacts, specifically looking for a `usage_log.txt` file. It then parses these logs, identifies continuous segments of Python command execution, and converts the raw log data and segments into structured metadata and time-series database models. Finally, the script uploads these processed utilization statistics back to a designated S3 bucket, organized by workflow and job identifiers.",
            "uuid": "c78e4860-ca56-4ec0-8807-86232acace67"
        },
        "Claude3.7": {
            "file_name": "/tools/stats/upload_utilization_stats/upload_utilization_stats.py",
            "number_of_lines": 444,
            "project_name": "pytorch",
            "total_tokens": 4538,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.810793,
            "file_summary": "This Python script is designed to upload utilization statistics from CI workflows to S3 storage. It processes raw utilization logs from workflow artifacts, converts them into structured data models, identifies Python command segments, and uploads the processed metadata and time series data to S3 buckets for further analysis. The script includes functionality for downloading artifacts, parsing utilization records, generating continuous time segments, and handling both normal and debug upload modes.",
            "uuid": "bdb2d9ed-5d4c-4688-9e7d-b7bc3f381439"
        },
        "mistral-small": {
            "file_name": "/tools/stats/upload_utilization_stats/upload_utilization_stats.py",
            "number_of_lines": 444,
            "project_name": "pytorch",
            "total_tokens": 4732,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 3.850027,
            "file_summary": " The main functionality of the provided Python file, `upload_utilization_stats.py`, is to analyze and upload utilization statistics to Amazon S3. The file processes test logs, converts them into utilization records, generates segments based on Python command levels, and then converts the utilization records and segments into a database model. Finally, it uploads the converted data to an S3 bucket. The file uses various libraries such as pandas, argparse, zipfile, and dataclasses for its functionality. The main classes in the file are `SegmentGenerator`, `UtilizationDbConverter`, and `UploadUtilizationData`. The `SegmentGenerator` class generates segments from utilization records, the `UtilizationDbConverter` class converts utilization records and segments into a database model, and the `UploadUtilizationData` class handles the overall process of fetching raw log data, converting it to a log model, converting it to a database model, and uploading it to S3.",
            "uuid": "acc3b3ba-2ad5-449d-8589-b74bf0cc1e48"
        },
        "mistral-7b": {
            "file_name": "/tools/stats/upload_utilization_stats/upload_utilization_stats.py",
            "number_of_lines": 444,
            "project_name": "pytorch",
            "total_tokens": 4649,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.010047,
            "file_summary": " This Python script is designed to process utilization logs, generate segments based on detected Python commands, convert the log data into a database model, and upload the data to an S3 bucket. The script uses the `argparse` library to accept command-line arguments and the `pandas` library for data processing. The main functionality is encapsulated in the `UploadUtilizationData` class, which handles downloading log data from an S3 bucket, converting it to log models, generating segments, converting to database models, and uploading the data back to an S3 bucket.",
            "uuid": "f0cf8998-828c-4b9b-9ab9-28e151666869"
        },
        "cohere-v1": {
            "file_name": "/tools/stats/upload_utilization_stats/upload_utilization_stats.py",
            "number_of_lines": 444,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, upload_utilization_stats.py, is designed to process utilization statistics data for CI workflows. It converts the data from log files to a database model and uploads the converted data to an S3 bucket. The script also includes functionality for segment generation, error handling, and argument parsing for workflow and job details.",
            "total_tokens": 3921,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.470854,
            "uuid": "a244ace1-3890-4352-9876-7302aae677ad"
        }
    },
    "/torch/_inductor/cpp_builder.py": {
        "nova-lite-v1": {
            "file_summary": "The \"cpp_builder.py\" file is a Python script designed to facilitate the compilation of C++ code into shared libraries or executables, supporting multiple operating systems including Windows, Linux, and MacOS. It provides a flexible and configurable way to specify build options such as compiler selection, include directories, and various flags for optimization and compatibility. The script includes functionalities for handling different compilers, managing build environments, and generating build commands that can be executed directly or saved into CMake files for integration with CMake-based build systems. It is particularly tailored for building components related to PyTorch, with support for specific PyTorch-related build options and configurations.",
            "total_tokens": 18299,
            "latency": 2.895052,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "50e4f4be-e5cf-4004-aff6-7e47a1b41c01"
        },
        "titan_premier": {
            "file_summary": "The `cpp_builder.py` file is a Python script that provides functionality for building C++ code. It includes classes for managing build options and a `CppBuilder` class for building C++ code.\n\nThe `BuildOptionsBase` class is a base class for storing C++ build options. It includes attributes for the compiler, definitions, include directories, C++ flags, linker flags, library directories, and libraries. The class also includes methods for finalizing the options and saving the options to a JSON file.\n\nThe `CppOptions` class is a subclass of `BuildOptionsBase` that includes additional attributes for the AOT mode, use of relative paths, and compile-only mode. The class also includes methods for getting the compiler, definitions, include directories, C++ flags, linker flags, library directories, and libraries.\n\nThe `CppTorchOptions` class is a subclass of `CppOptions` that includes additional attributes for the vector instruction set architecture (ISA), inclusion of PyTorch, AOT mode, compile-only mode, use of relative paths, use of mmap weights, and shared mode. The class also includes methods for getting the definitions, include directories, C++ flags, linker flags, library directories, libraries, and passthrough arguments.\n\nThe `CppTorchDeviceOptions` class is a subclass of `CppTorchOptions` that includes additional attributes for the device type (CUDA or XPU), AOT mode, compile-only mode, and use of relative paths. The class also includes methods for getting the definitions, include directories, C++ flags, linker flags, library directories, libraries, and passthrough arguments.\n\nThe `CppBuilder` class is a class for building C++ code. It includes attributes for the name, sources, build options, output directory, and target file. The class also includes methods for getting the command line, target file path, building the code, and saving the build command to a CMake file.\n\nThe `CppBuilder` class includes a `build` method that builds the C++ code using the command line. The method creates a temporary directory for the build and runs the command line in the temporary directory. If the build is successful, the temporary directory is deleted.\n\nThe `CppBuilder` class includes a `save_compile_cmd_to_cmake",
            "total_tokens": 500,
            "latency": 15.544391,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e81df68a-d67f-420a-9e25-841fd527e2ec"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements a comprehensive JIT compilation utility named 'CppBuilder', which is designed to handle cross-platform (Windows, Linux, macOS) compilation of C++ source code for PyTorch-related projects. It features OS-specific compiler flags, supports detection and configuration of different compilers (gcc, clang, MSVC, Intel), manages paths and libraries for various hardware backends (CUDA, ROCm, XPU), and includes special considerations such as OpenMP loading and Python Torch embedding. Overall, it streamlines building C++ extensions or components compatible with PyTorch and Python runtime environments across different systems.",
            "total_tokens": 14948,
            "latency": 15.226072,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "3408e275-8565-4acd-b481-2412f46b6289"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file is a script designed to build a C++ library, supporting both Windows and Linux OS. It includes functionality to find the compiler, set up compilation and linking options, handle Windows-specific temp directories, and prepare the build commands. The script organizes and executes the build process based on the specified build options, including defining macros, setting compile flags, including directories, linking flags, and libraries. It also provides the functionality to save the build commands to a CMake configuration file.",
            "total_tokens": 14701,
            "latency": 2.453614,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "e6087e0a-376d-4505-a9ef-3628f243eaf2"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"cpp_builder.py\" file is to provide a flexible and cross-platform C++ build system for PyTorch's inductor compiler. It includes classes and functions for:\n\n1. Configuring and managing C++ build options for different platforms (Windows, Linux, macOS) and compilers (GCC, Clang, MSVC).\n\n2. Handling various build scenarios, including AOT (Ahead-of-Time) compilation, OpenMP support, and device-specific options (CUDA, ROCm, XPU).\n\n3. Generating and executing compilation commands for C++ source files, including preprocessing, compiling, and linking stages.",
            "total_tokens": 19380,
            "latency": 8.418087,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0f633543-c8b6-43fb-a985-5f4650d5965c"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `cpp_builder.py` file is to provide a comprehensive C++ builder for compiling C++ code, supporting both Windows and Linux operating systems. It includes functionalities for setting up build options, handling different compilers, managing build processes, and generating CMake files for further build automation.",
            "total_tokens": 18229,
            "latency": 4.041063,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "50a4e67a-be8a-41e1-834e-16c590db3313"
        },
        "gpt-4o": {
            "file_summary": "The file `cpp_builder.py` serves as a utility for building compiled C++ libraries for PyTorch, supporting configurations compatible for both Windows and Linux operating systems. It includes logic for detecting and configuring various C++ compilers, setting build options, handling dependencies like OpenMP and Python, and managing multiple compilation modes such as precompiled headers, AOT, and device-specific options (e.g., CUDA or XPU). It also supports generating CMake files and remote builds for FBCode environments.",
            "total_tokens": 14924,
            "latency": 2.25934,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "ef8a26a9-776d-4db2-aa03-6425dfe9aef8"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the file is to provide a C++ builder class, `CppBuilder`, which can be used to compile C++ code on various platforms, including Windows, Linux, and MacOS. The class takes in source code files, build options, and an output directory, and generates a command line to compile the code. It also provides methods to save the compile command to a CMake file and to build the code.",
            "total_tokens": 14694,
            "latency": 50.258223,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "b5636098-7b4a-4dfe-80f9-8f5da39b8fb5"
        },
        "gpt-4": {
            "file_summary": "The Python file, named `cpp_builder.py`, is a detailed and complex builder module primarily designed to dynamically compile and link C++ code across different operating systems, specifically targeting the construction of Python extensions or other C++ libraries. It heavily utilizes the Torch libraries and C++ features, supporting varying compilation flags, linker configurations, and definitions based on the OS and user requirements. Additionally, the script handles platform-specific challenges such as handling dynamic libraries, precompiling headers, conditions for different compilers, and optimizing for performance using specialized tools or flags. Moreover, the script is tailored to enable support and usage within Facebook's code base which introduces additional branch handling and environment setup, especially around CUDA and GPU settings for compilation and linking processes.",
            "total_tokens": 14747,
            "latency": 5.191262,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "3c1e694c-4f67-413c-b933-c14815d75fab"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `cpp_builder.py`, is to build and compile C++ code for PyTorch, a popular deep learning framework, on various operating systems including Windows, Linux, and MacOS. The file contains classes and functions that handle tasks such as setting up build options, compiling C++ code, and linking libraries. It also provides functionality for building and compiling PyTorch-related C++ code, including support for CUDA and XPU devices. The file is designed to be used in conjunction with PyTorch's just-in-time (JIT) compiler and provides a way to build and compile C++ code for use with PyTorch models.",
            "total_tokens": 14745,
            "latency": 6.411135,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "cf50c5bb-0290-4714-bf4a-c235132316f2"
        },
        "gpt-4.1": {
            "file_summary": "The file \"cpp_builder.py\" defines a cross-platform C++ JIT builder for PyTorch, enabling dynamic compilation of C++ extensions on Linux, Windows, and MacOS, with specialized support for various compilers and device types (e.g., CUDA, XPU). It provides classes and functions for selecting appropriate compilers, constructing compilation and linking options, managing build directories, and generating or invoking build commands, with abstraction layers to support features like Torch AOT compilation, OpenMP, and integration with Python and PyTorch build systems. The code also includes utilities for generating CMake files and handling system-specific quirks, ensuring consistent extension building across environments.",
            "total_tokens": 14957,
            "latency": 2.872681,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "eb4fea4a-e039-4630-925f-7e5fa75803bd"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/cpp_builder.py",
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "total_tokens": 18926,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.315763,
            "file_summary": "This Python module provides a cross-platform C++ builder for PyTorch's inductor backend, handling compiler detection (GCC, Clang, MSVC, Intel) and flag configuration across Linux, Windows, and macOS. It defines classes to manage build options, including Torch-specific and device-specific (CUDA/XPU) settings. The core `CppBuilder` class takes source files and options to execute the compilation process and generate the final shared library or object file.",
            "uuid": "79be3ed8-090b-4f96-b4c8-2630f6d8fa41"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/cpp_builder.py",
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "total_tokens": 19070,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.147013,
            "file_summary": "This Python file implements a cross-platform C++ build system for PyTorch's Inductor backend. It provides classes to manage complex compilation options, supporting various operating systems, compilers (GCC, Clang, MSVC, Intel), and hardware targets (CPU, CUDA, XPU). The module handles build flags, dependencies, and executes the compilation and linking process to generate shared libraries or object files.",
            "uuid": "bf6173d3-c12a-4bed-92b1-6d4eaff09382"
        },
        "Claude3.7": {
            "file_name": "/torch/_inductor/cpp_builder.py",
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "total_tokens": 19328,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 7.508032,
            "file_summary": "The cpp_builder.py file is a comprehensive C++ builder designed to support both Windows and Linux operating systems for PyTorch's inductor compilation system. It provides functionality to compile C++ code with appropriate compiler flags, handle platform-specific differences, manage dependencies (including PyTorch, Python, OpenMP), and support various compilation modes (AOT, preprocessing, etc.) across different toolchains (GCC, Clang, MSVC, Intel).",
            "uuid": "eaf7493b-5832-44cf-9bf2-b8e0199a1a67"
        },
        "mistral-small": {
            "file_name": "/torch/_inductor/cpp_builder.py",
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "total_tokens": 20338,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 7.661981,
            "file_summary": " The given Python file is a C++ builder for PyTorch that supports building C++ code for PyTorch on Windows, Linux, and MacOS. The builder takes in a name for the target file, a list of source files, a BuildOption object that contains the build options, and an optional output directory. The builder initializes the compiler, include directories, definitions, flags, libraries, and libraries directories based on the BuildOption object. It also sets the output file path based on the name and the output directory. The builder provides methods to get the command line for building the code, the target file path, and to build the code. The builder also supports building code remotely using a temporary directory. The builder is designed to be used with PyTorch's C++ extension module.",
            "uuid": "52d48067-a80e-4e35-8583-bf17d26e895c"
        },
        "mistral-7b": {
            "file_name": "/torch/_inductor/cpp_builder.py",
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "total_tokens": 20309,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 5.797559,
            "file_summary": " The main functionality of the given Python file is to build C++ shared libraries or modules using various compilers (gcc, clang, Intel Compiler, etc.) on both Windows and Linux platforms. The file provides functions to search for and initialize the appropriate compiler, check if required libraries and dependencies are available, and build the C++ code using the specified build options. The build options include compiler flags, include directories, libraries, and other parameters. The file also supports precompiled headers and automatic generation of precompiled header files. Additionally, it provides functions to save the build commands to CMake files for easier integration with CMake projects.",
            "uuid": "ea88cfe3-6754-4e3b-863b-3445174a4092"
        },
        "cohere-v1": {
            "file_name": "/torch/_inductor/cpp_builder.py",
            "number_of_lines": 1752,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, `cpp_builder.py`, is a module designed for cross-platform C++ code compilation. It defines several classes for managing C++ build options and offers methods to compile C++ source code, producing object files, shared libraries, or Python module extensions. The script supports different compilers and platforms and can be used to create Facebook Code (FBCode) builds.",
            "total_tokens": 17262,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 4.787817,
            "uuid": "c9e14dda-7455-4e3f-ac1a-e46aaaeb3a0c"
        }
    },
    "/torch/distributed/fsdp/_dynamo_utils.py": {
        "nova-lite-v1": {
            "file_summary": "The `_dynamo_utils.py` file provides a utility function `_annotate_modules_for_dynamo` that annotates submodules within a given PyTorch module, excluding those specified in an ignored set, to indicate they are managed by Fully Sharded Data Parallel (FSDP). This annotation also saves the `use_orig_params` setting for FSDP, ensuring that Dynamo treats these modules as unspecialized, which is crucial for optimizing the interleaving of communication and computation during the backward pass.",
            "total_tokens": 793,
            "latency": 1.111393,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "65dc231e-778a-4028-a4b2-5ab5517c8fa6"
        },
        "titan_premier": {
            "file_summary": "The file \"_dynamo_utils.py\" contains a function named \"_annotate_modules_for_dynamo\" that takes three parameters: \"module\", \"ignored_modules\", and \"use_orig_params\". This function iterates through the submodules of the given module and annotates them as FSDP-managed modules, indicating that they should be treated as unspecialized NNModules by Dynamo. Additionally, it saves the \"use_orig_params\" setting passed to the FSDP constructor for each submodule. This annotation is necessary because Dynamo does not see the FSDP-wrapped modules during tracing, and it ensures that the wrapped modules are treated as unspecialized NNModules, which allows for better overlap of communication and computation during backward passes.",
            "total_tokens": 164,
            "latency": 3.718337,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "39f4ce6d-5f70-4535-9383-ba51d386d3a0"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines a utility function `_annotate_modules_for_dynamo` used specifically to annotate PyTorch modules managed by FullyShardedDataParallel (FSDP) for proper compatibility with Torch Dynamo (a tracing and compilation framework). The function identifies which modules need to be treated as unspecialized by Dynamo, an essential step to correctly manage parameter views and ensure proper computation-communication overlap during backward execution. Additionally, it sets internal module flags (`_is_fsdp_managed_module` and `_fsdp_use_orig_params`) to signal Dynamo how to handle FSDP modules appropriately.",
            "total_tokens": 773,
            "latency": 6.105133,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "2e7eb019-157c-4fe1-bac8-b54fd22be165"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `_dynamo_utils.py` file is to define a function `_annotate_modules_for_dynamo` that annotates submodules in a given module's tree to indicate they are managed by FullyShardedDataParallel (FSDP) and saves the setting for using original parameters. The function helps Dynamo treat the wrapped modules inside FSDP as 'unspecialized' to avoid issues during graph execution, specifically related to handling original parameters and ensuring proper execution order during backward passes for overlapping computation and communication operations.",
            "total_tokens": 756,
            "latency": 6.783788,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "f329520c-7bcd-457a-8e83-5d2ccd2ff440"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"_dynamo_utils.py\" file is to annotate submodules in a PyTorch neural network module tree for use with FSDP (Fully Sharded Data Parallel) and Dynamo. It marks FSDP-managed modules and saves the \"use_orig_params\" setting, which is crucial for Dynamo to treat these modules as UnspecializedNNModules during tracing, enabling better performance and correct behavior in distributed training scenarios.",
            "total_tokens": 879,
            "latency": 2.617717,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "58a1cd5b-99df-483d-b958-6f5b4a4e8692"
        },
        "nova-pro-v1": {
            "file_summary": "The `_dynamo_utils.py` file contains utility functions for integrating PyTorch's Fully Sharded Data Parallel (FSDP) with Dynamo, a PyTorch feature for optimizing model execution. Specifically, the file defines a function `_annotate_modules_for_dynamo` that annotates submodules within a given PyTorch module to indicate they are managed by FSDP and sets a flag to ensure Dynamo treats these modules as \"UnspecializedNNModule\" to handle parameters correctly during execution. This annotation is crucial for Dynamo to interleave communication and computation effectively during backward passes.",
            "total_tokens": 807,
            "latency": 1.60498,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "588604c7-e105-401b-af37-f888e6b6cc46"
        },
        "gpt-4o": {
            "file_summary": "The `_dynamo_utils.py` file provides a utility function `_annotate_modules_for_dynamo` designed to annotate PyTorch submodules within a module tree to inform the Dynamo system about modules managed by `FullyShardedDataParallel` (FSDP). It excludes specified ignored modules and associates metadata flags to indicate FSDP management and the `use_orig_params` setting. This ensures Dynamo handles FSDP-wrapped modules as `UnspecializedNNModules`, enabling flexible tracing, effective graph handling, and improved computation-communication overlap in distributed training scenarios.",
            "total_tokens": 767,
            "latency": 1.623754,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8efb217e-219c-4dbf-a895-ed8c03a52b5e"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the `_dynamo_utils.py` file is to annotate PyTorch modules for use with the Dynamo compiler, specifically for Fully Sharded Data Parallel (FSDP) modules. The `_annotate_modules_for_dynamo` function recursively annotates submodules in a given module, indicating that they are FSDP-managed and saving the `use_orig_params` setting. This annotation is necessary because Dynamo treats FSDP-wrapped modules as unspecialized NNModules, which allows for more thorough tracing and guards, and is required for correct backward execution.",
            "total_tokens": 771,
            "latency": 4.391246,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "600e8a01-e416-4c02-88e8-6a5239b72053"
        },
        "gpt-4": {
            "file_summary": "The Python script `_dynamo_utils.py` is designed to work with neural network modules, specifically to annotate and handle submodules within the primary module provided they are not part of the `ignored_modules` set. The annotation process mainly deals with managing Fully Sharded Data Parallel (FSDP) modules used in distributed training, marking them as managed by Dynamo for specialized processing during tracing and execution. This process includes handling FSDP specific conventions, like ensuring that original parameters are used (`use_orig_params=True`) and that dynamo treats these modules as unspecialized for optimization purposes during backward execution, aiding in the more efficient overlap of computation and communication tasks.",
            "total_tokens": 782,
            "latency": 4.376885,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "276ae405-69d7-42e7-acb6-872c2b568370"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `_dynamo_utils.py` file is to annotate submodules in a PyTorch module tree, excluding ignored modules, to indicate that they are managed by Fully Sharded Data Parallel (FSDP) and to store the `use_orig_params` setting. This annotation is necessary because Dynamo, a PyTorch compiler, treats FSDP-wrapped modules as unspecialized neural network modules, allowing for more flexible and efficient tracing and execution. The annotation is done by setting two attributes, `_is_fsdp_managed_module` and `_fsdp_use_orig_params`, on each submodule.",
            "total_tokens": 780,
            "latency": 1.909557,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "db5fdf22-1766-4855-9e1d-204c0c19dc53"
        },
        "gpt-4.1": {
            "file_summary": "The file \"_dynamo_utils.py\" defines a utility function that traverses a given PyTorch module tree and annotates its submodules to indicate they are managed by Fully Sharded Data Parallel (FSDP), unless they are in a specified set of ignored modules. This annotation is necessary for integration with TorchDynamo, ensuring that FSDP-managed modules are treated in a way that allows proper tracing and handling of parameter views during dynamic graph compilation. Additionally, the function records whether original parameters are used, as required for Dynamo's support of FSDP.",
            "total_tokens": 767,
            "latency": 1.624568,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 44,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "c54bf7ce-2430-494f-a1cf-f8abc9b91e5a"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/fsdp/_dynamo_utils.py",
            "number_of_lines": 44,
            "project_name": "pytorch",
            "total_tokens": 1199,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.506757,
            "file_summary": "This Python file defines a utility function `_annotate_modules_for_dynamo` designed to work with PyTorch's FSDP (Fully Sharded Data Parallel) and Dynamo compiler. The function iterates through the submodules of a given `nn.Module` and annotates them with special attributes (`_is_fsdp_managed_module` and `_fsdp_use_orig_params`). These annotations signal to Dynamo that the modules are managed by FSDP and should be traced \"unspecially\", ensuring correct behavior, especially regarding parameter handling when `use_orig_params` is true.",
            "uuid": "1b0f3c23-9b25-43ff-88a3-d7983fef9964"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/fsdp/_dynamo_utils.py",
            "number_of_lines": 44,
            "project_name": "pytorch",
            "total_tokens": 1070,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.799161,
            "file_summary": "This Python file defines a function `_annotate_modules_for_dynamo` that iterates through submodules of a given PyTorch module. It adds specific attributes (`_is_fsdp_managed_module` and `_fsdp_use_orig_params`) to these submodules, excluding those in an ignored list. These annotations are used by PyTorch Dynamo to identify FSDP-managed modules and ensure they are treated as 'UnspecializedNNModule' for correct tracing and compilation behavior.",
            "uuid": "05667d46-06f0-48cb-99df-4a4debe21369"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/fsdp/_dynamo_utils.py",
            "number_of_lines": 44,
            "project_name": "pytorch",
            "total_tokens": 890,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.226943,
            "file_summary": "This file provides utility functions for PyTorch's Dynamo compiler to work with Fully Sharded Data Parallel (FSDP) modules. Specifically, it contains a function `_annotate_modules_for_dynamo` that marks modules as FSDP-managed by adding special attributes, allowing Dynamo to treat them as \"UnspecializedNNModule\" during tracing. The extensive comments explain how this annotation helps Dynamo properly handle FSDP modules during compilation, particularly regarding parameter views and backward pass execution order.",
            "uuid": "d00d9028-2a63-4501-a7f3-e8481b34d5b2"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/fsdp/_dynamo_utils.py",
            "number_of_lines": 44,
            "project_name": "pytorch",
            "total_tokens": 871,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 4.768719,
            "file_summary": " The main functionality of the file \"_dynamo_utils.py\" is to annotate submodules in a given PyTorch module, excluding those in a specified ignored_modules set. The annotation indicates that the submodules are FSDP-managed and saves the use_orig_params setting passed to the FSDP constructor. This is done to allow Dynamo, a PyTorch compiler, to treat the wrapped module inside FSDP as an UnspecializedNNModule, which is traced-through without any assumptions and with thorough guards.",
            "uuid": "90a36f13-d6eb-4fa7-b02c-9a9054159856"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/fsdp/_dynamo_utils.py",
            "number_of_lines": 44,
            "project_name": "pytorch",
            "total_tokens": 914,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 3.31279,
            "file_summary": " This Python file, \"_dynamo\\_utils.py\", defines a function `_annotate_modules_for_dynamo` that annotates submodules in a given PyTorch `nn.Module` object, except those in a given set of ignored modules. It sets flags on these submodules to indicate that they are managed by PyTorch's Fully Sharded Data Parallel (FSDP) and saves the `use_orig_params` setting passed to the FSDP constructor. This is done to enable Dynamo, a deep learning compiler, to treat the wrapped modules inside FSDP as unspecialized and capture the module code more functionally, allowing for overlapping of communication and computation during backward propagation.",
            "uuid": "680bf41b-76d0-4804-b08d-22b9542954a0"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/fsdp/_dynamo_utils.py",
            "number_of_lines": 44,
            "project_name": "pytorch",
            "file_summary": "The `_dynamo_utils.py` file contains a function called `_annotate_modules_for_dynamo` which is used to annotate submodules of a given `nn.Module` to indicate they are managed by FSDP and to save the `use_orig_params` setting. This is used by Dynamo, a tool that traces and optimizes PyTorch code, to handle modules wrapped by PyTorch's Fully Sharded Data Parallelism (FSDP) mechanism.",
            "total_tokens": 745,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 8.553057,
            "uuid": "14da1dce-01ac-4958-87f3-1657319bf160"
        }
    },
    "/torch/distributions/studentT.py": {
        "nova-lite-v1": {
            "file_summary": "The file \"studentT.py\" defines a custom `StudentT` distribution class that inherits from PyTorch's `Distribution` class. This class models a Student's t-distribution with parameters for degrees of freedom (`df`), mean (`loc`), and scale (`scale`). It includes methods for sampling, calculating log probabilities, and determining entropy, mean, mode, and variance of the distribution.",
            "total_tokens": 1418,
            "latency": 1.009313,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "47f279a5-63e1-4a81-b74e-73899037aab5"
        },
        "titan_premier": {
            "file_summary": "The Python file \"studentT.py\" defines a class \"StudentT\" that inherits from the \"Distribution\" class in the PyTorch library. This class represents a Student's t-distribution with parameters for degrees of freedom, mean, and scale. The class includes methods for generating random samples, calculating the log probability, and computing the entropy of the distribution. The file also includes helper functions for validating input arguments and broadcasting parameters to match the desired batch shape. Overall, the main functionality of the file is to provide a way to work with Student's t-distributions in a PyTorch environment.",
            "total_tokens": 128,
            "latency": 2.999454,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "85b73a2c-b0ca-4093-afd7-8a0588885cc9"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements a Student's t-distribution class for PyTorch, allowing users to create and work with Student's t-distributed random variables parametrized by degrees of freedom, location, and scale parameters. It supports calculation of statistical properties like mean, mode, variance, entropy, sampling (including differentiable samples via `rsample`), and evaluation of log probabilities. This class is suitable for probabilistic modeling and Bayesian inference tasks using PyTorch.",
            "total_tokens": 1230,
            "latency": 3.66496,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b4ba5fd0-82de-42db-9395-bdb7ab50b240"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The \"studentT.py\" file defines a Student's t-distribution class in PyTorch for generating samples and calculating properties such as mean, mode, variance, log probability, and entropy. The distribution is parameterized by degrees of freedom, mean, and scale, and includes methods for sampling from the distribution, computing probabilities, and calculating entropy. The file also includes parameter constraints and examples demonstrating the usage of the StudentT class.",
            "total_tokens": 1224,
            "latency": 1.035149,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "3a1d2577-413e-4a84-943a-b381792c98e2"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"studentT.py\" file is to implement a Student's t-distribution in PyTorch. It defines a StudentT class that inherits from the Distribution class, providing methods for sampling, calculating log probabilities, and computing statistical properties such as mean, mode, and variance. The implementation allows for parameterization by degrees of freedom, location, and scale, making it a flexible tool for working with Student's t-distributions in PyTorch-based machine learning and statistical applications.",
            "total_tokens": 1514,
            "latency": 3.137438,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0bdaeade-cd58-465e-a602-44b6c01a6600"
        },
        "nova-pro-v1": {
            "file_summary": "The `studentT.py` file defines a `StudentT` class that implements a Student's t-distribution within the PyTorch framework. It includes methods for sampling, calculating log probability, and entropy, and properties for mean, mode, and variance.",
            "total_tokens": 1387,
            "latency": 0.888129,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "4095f106-8a0b-46a3-a159-9a63e8ab186f"
        },
        "gpt-4o": {
            "file_summary": "The given Python file defines a `StudentT` class, which represents a statistical Student's t-distribution. It provides methods for sampling from the distribution (`rsample`), calculating log probabilities (`log_prob`), and entropy, while handling distribution parameters such as degrees of freedom, mean, and scale. The implementation also includes functionality for broadcasting, expanding batch shapes, and calculating properties like mean, mode, and variance.",
            "total_tokens": 1222,
            "latency": 1.962168,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d1173ade-4ba2-45cd-a69f-8c474fbe74f3"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a class `StudentT` that represents a Student's t-distribution, a continuous probability distribution used in statistics. The class provides methods for calculating the mean, mode, variance, and entropy of the distribution, as well as for sampling from the distribution and calculating the log probability of a given value. The class is designed to work with PyTorch tensors and is part of the PyTorch distribution library.",
            "total_tokens": 1235,
            "latency": 3.755238,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "bbeff96b-515d-40ff-aa72-6a4afa3f654a"
        },
        "gpt-4": {
            "file_summary": "The `studentT.py` file defines a `StudentT` class, which is a PyTorch implementation of the Student's t-distribution with parameters for degrees of freedom (`df`), location (`loc`), and scale (`scale`). This class inherits from Torch's `Distribution` class and includes methods for sampling (`rsample`), calculating the logarithm of probability density (`log_prob`), and computing the entropy of the distribution. It also handles various properties such as `mean`, `mode`, and `variance` of the distribution, which are calculated based on the distribution's parameters.",
            "total_tokens": 1258,
            "latency": 2.833733,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "6e71b036-7ccc-4a3e-ba8f-86985ab8d35c"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement a Student's t-distribution using PyTorch, allowing for the creation of a distribution object with specified degrees of freedom, mean, and scale. The class `StudentT` inherits from PyTorch's `Distribution` and provides methods for calculating properties such as mean, mode, variance, and entropy, as well as sampling from the distribution. The implementation includes input validation, broadcasting, and support for various mathematical operations, making it a robust and flexible tool for working with Student's t-distributions in PyTorch.",
            "total_tokens": 1260,
            "latency": 1.581986,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d99ba6d7-e3e0-4d68-b7f8-4aee20752aef"
        },
        "gpt-4.1": {
            "file_summary": "The file implements a Student's t-distribution for PyTorch by defining a StudentT class, inheriting from torch.distributions.Distribution. This class supports sampling (including reparameterized sampling), computing log probabilities, entropy, mean, mode, and variance, and handles broadcasting of parameters (degrees of freedom, location, and scale). Overall, it enables probabilistic modeling and inference with Student's t-distributions in a tensor-based, differentiable context.",
            "total_tokens": 1229,
            "latency": 2.058752,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "e1250c55-e79c-4d15-bab6-254ab0099640"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributions/studentT.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1748,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 3.892629,
            "file_summary": "This Python file defines a `StudentT` class within the PyTorch framework, representing the Student's t-distribution. It allows users to create instances of this distribution parameterized by degrees of freedom (`df`), location (`loc`), and scale (`scale`). The class provides methods for sampling from the distribution (`rsample`), calculating log probabilities (`log_prob`), and computing statistical properties like mean, variance, and entropy.",
            "uuid": "cda588ea-de02-40f5-bf86-70b4269fb3c6"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributions/studentT.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1602,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 1.722131,
            "file_summary": "This Python file defines a `StudentT` class, which represents the Student's t-distribution within the PyTorch distributions framework. The class is parameterized by degrees of freedom, location, and scale. It provides methods for sampling from the distribution, calculating the log probability density function, and computing statistical properties like mean, mode, and variance.",
            "uuid": "1787456d-8204-45b2-92eb-e13ee34cfd95"
        },
        "Claude3.7": {
            "file_name": "/torch/distributions/studentT.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1511,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.138598,
            "file_summary": "The StudentT.py file implements a PyTorch distribution class for Student's t-distribution, providing functionality for sampling, computing log probabilities, and calculating statistical properties like mean, mode, variance, and entropy. The class supports parameterization by degrees of freedom, location (mean), and scale, and includes methods for random sampling (rsample) and expanding batch dimensions. It integrates with PyTorch's distribution framework, allowing it to be used in probabilistic modeling and statistical computations.",
            "uuid": "34fa0503-93a8-4528-9c9a-a1a4e43c4671"
        },
        "mistral-small": {
            "file_name": "/torch/distributions/studentT.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1537,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.991896,
            "file_summary": " The provided Python file, named \"studentT.py\", defines a custom distribution class called \"StudentT\" in the PyTorch library. This class represents the Student's t-distribution, which is a continuous probability distribution that is similar to the normal distribution but has heavier tails. The StudentT class is parameterized by degrees of freedom, mean, and scale. It provides methods for calculating properties such as mean, mode, and variance, as well as for sampling from the distribution and computing the log probability and entropy.",
            "uuid": "bcb2d0f5-036b-47a5-a430-26731a31866c"
        },
        "mistral-7b": {
            "file_name": "/torch/distributions/studentT.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1518,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.106544,
            "file_summary": " The given Python file defines a custom distribution class named \"StudentT\" based on the Student's t-distribution using PyTorch. This class allows users to create instances of the distribution with specified degrees of freedom, mean, and scale. The main functionality of the file is to implement the Student's t-distribution with properties like mean, mode, and variance, and methods such as sampling and log probability calculation.",
            "uuid": "fc86e3d0-cce0-4237-b567-4b4642b7b716"
        },
        "cohere-v1": {
            "file_name": "/torch/distributions/studentT.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "file_summary": "The Python script defines a class called StudentT which represents a Student's t-distribution in the realm of statistics. It has methods to sample from the distribution, calculate various properties like mean, mode, and variance, and also provides utilities to work with the batch of distributions. The distribution is parameterized by degree of freedom, mean, and scale.",
            "total_tokens": 1341,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 2.441128,
            "uuid": "dd67883c-b0c7-4fda-ba2b-6e07ce005c73"
        }
    },
    "/torch/distributed/rpc/functions.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"functions.py\" file is to provide a decorator `@async_execution` for functions that need to execute asynchronously within a distributed computing environment, particularly when using PyTorch's distributed RPC (Remote Procedure Call) framework. This decorator ensures that the function returns a `Future` object, allowing the function to pause and resume as needed. The decorator can be combined with other decorators like `@staticmethod`, `@classmethod`, and TorchScript decorators, with specific ordering rules to ensure proper recognition and execution.",
            "total_tokens": 2179,
            "latency": 1.256884,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3af05f95-f123-42c5-b674-67463b2dab70"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to provide a decorator for a function that indicates the return value of the function is guaranteed to be a Future object and can run asynchronously on the RPC callee. The decorator allows the callee to extract the Future returned by the wrapped function and install subsequent processing steps as a callback to that Future. The installed callback will read the value from the Future when completed and send the value back as the RPC response. The decorator also works with TorchScript decorators, static or class methods, and RRef helpers.",
            "total_tokens": 115,
            "latency": 2.847109,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "93cd5892-f602-4374-981f-c299e75e007b"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines a decorator `async_execution`, which is meant to identify functions whose return values are guaranteed to be `Future` objects, allowing asynchronous remote procedure calls (RPC) in PyTorch's distributed framework. This decorator helps PyTorch RPC recognize the decorated functions as returning Futures and handle async execution accordingly, especially useful when interacting with `rpc_async` or other operations requiring waiting and callbacks. The extensive comments and examples illustrate usage with RPC APIs, TorchScript, and class or static methods within distributed PyTorch environments.",
            "total_tokens": 1953,
            "latency": 4.99254,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "c56bd5b4-f36d-4433-b48a-d6f045e6956c"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The file \"functions.py\" contains a Python decorator named \"async_execution\" that is used to mark functions whose return values are guaranteed to be a specific type of object and can run asynchronously on the RPC callee. The decorator allows for handling parallel, asynchronous execution of functions and their subsequent processing steps as callbacks on the RPC callee side. It provides examples and guidelines on how to use the decorator in various scenarios, including combining it with TorchScript decorators, static or class methods, and working with RRef helpers in distributed PyTorch applications.",
            "total_tokens": 1936,
            "latency": 1.509843,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "1af8cf11-20ff-441a-8aae-b93207c6c4a4"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"functions.py\" file is to define a decorator called \"async_execution\" for use in distributed RPC (Remote Procedure Call) scenarios in PyTorch. This decorator is designed to handle functions that return Future objects, allowing for asynchronous execution of RPC calls. It provides a way to manage and process the results of asynchronous operations in distributed computing environments using PyTorch's RPC framework.",
            "total_tokens": 2341,
            "latency": 2.846489,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "f81621e0-2448-43ed-a19f-57885f43b03a"
        },
        "nova-pro-v1": {
            "file_summary": "The `functions.py` file defines a decorator `async_execution` that marks a function to be executed asynchronously in a distributed computing environment using PyTorch's RPC framework. The decorator ensures that the function returns a `Future` object and handles the asynchronous execution on the RPC callee side.",
            "total_tokens": 2129,
            "latency": 1.027257,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a675ccb4-1e89-4698-a27d-ea95d7af1b57"
        },
        "gpt-4o": {
            "file_summary": "The `functions.py` file provides an `async_execution` decorator for enabling asynchronous execution of functions in distributed RPC (Remote Procedure Call) workflows using PyTorch. This decorator ensures that the function returns a `torch.futures.Future` object and allows subsequent processing steps to be installed as callbacks, facilitating asynchronous workflows across multiple workers. It supports integration with other decorators like `@staticmethod`, `@classmethod`, and TorchScript decorators, as demonstrated through examples in distributed computations.",
            "total_tokens": 1941,
            "latency": 1.84776,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "f26130c9-2e74-403f-a8f8-66a38bf9e018"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to define a decorator `async_execution` that indicates a function returns a `Future` object and can run asynchronously on the RPC callee. The decorator allows the callee to extract the `Future` returned by the wrapped function and install subsequent processing steps as a callback to that `Future`. This enables asynchronous execution of the function, allowing it to pause and resume due to containing `rpc_async` or waiting for other signals.",
            "total_tokens": 1925,
            "latency": 3.644145,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0f4d844f-4069-43f1-acdb-b6b2b1480fd4"
        },
        "gpt-4": {
            "file_summary": "The Python file 'functions.py' defines a decorator `async_execution` that transforms a regular function into one that guarantees an asynchronous execution style that returns a torch Future object. This decorator is specifically designed for use with RPC (Remote Procedure Call) in PyTorch distributed computing environments, facilitating non-blocking operations that can wait for other asynchronous operations or signals before proceeding. Examples and explanations within the file illustrate how this decorator interacts with different types of functions (e.g., static methods, class methods) and how to use it in various scenarios within a distributed system using RPC and Future objects, along with its integration with TorchScript and RRef helpers.",
            "total_tokens": 1957,
            "latency": 6.245066,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "a4edce5e-9649-4236-8b64-fcf8520fd5d8"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to define a decorator `async_execution` that allows a function to run asynchronously on the RPC callee, enabling the function to pause and resume execution due to operations like `torch.distributed.rpc.rpc_async` or waiting for other signals. This decorator is useful for functions that return a `torch.futures.Future` object and need to handle asynchronous execution. The decorator can be combined with other decorators like `@staticmethod`, `@classmethod`, or TorchScript decorators, and it works with RRef helpers like `torch.distributed.rpc.RRef.rpc_sync` and `torch.distributed.rpc.RRef.rpc_async`.",
            "total_tokens": 1965,
            "latency": 1.570909,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3a8c91d0-e753-4a91-a33b-63810439dabd"
        },
        "gpt-4.1": {
            "file_summary": "This file defines a decorator, async_execution, intended for use with PyTorch's RPC (remote procedure call) functions, marking that the decorated function will return a torch.futures.Future and can execute asynchronously on a remote worker. The decorator adds a special attribute to assist PyTorch RPC in recognizing such asynchronous functions, ensuring they are handled correctly within distributed settings. Extensive documentation and usage examples are provided, illustrating combinations with TorchScript, static/class methods, and RRef helpers, to guide users in various distributed computing scenarios.",
            "total_tokens": 1951,
            "latency": 1.973255,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 170,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "1a7ea160-7252-48bb-af89-b640cf1e2be2"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/rpc/functions.py",
            "number_of_lines": 170,
            "project_name": "pytorch",
            "total_tokens": 2322,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 1.799999,
            "file_summary": "This Python file defines a decorator named `async_execution`. This decorator is used to mark functions that are intended to be executed asynchronously within the PyTorch RPC framework. It signals that the decorated function will return a `torch.futures.Future`, allowing the RPC system to manage the asynchronous completion and result retrieval on the remote worker.",
            "uuid": "85af60f9-9ec8-4408-a492-82a7feab147b"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/rpc/functions.py",
            "number_of_lines": 170,
            "project_name": "pytorch",
            "total_tokens": 2757,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.093892,
            "file_summary": "This Python file defines the `async_execution` decorator for use with PyTorch's distributed RPC framework. This decorator marks a function to indicate that it returns a `torch.futures.Future` and can be executed asynchronously on the remote worker (callee). The RPC system leverages this decorator to properly handle the Future's completion on the callee side and send the final result back to the caller, facilitating non-blocking remote procedure calls.",
            "uuid": "8a7b8dd1-e2e8-473f-b3e9-0ef797d59210"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/rpc/functions.py",
            "number_of_lines": 170,
            "project_name": "pytorch",
            "total_tokens": 2346,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.264492,
            "file_summary": "The file defines a decorator function `async_execution` that marks Python functions which return a Future object and can run asynchronously in RPC (Remote Procedure Call) operations within PyTorch's distributed computing framework. This decorator allows the callee to extract the Future returned by the wrapped function, install processing steps as callbacks, and send the result back as an RPC response when completed, enabling functions to pause and resume execution when waiting for remote operations or other signals.",
            "uuid": "03909e07-fce3-4c5a-8b95-24c0930227a0"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/rpc/functions.py",
            "number_of_lines": 170,
            "project_name": "pytorch",
            "total_tokens": 2359,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.429534,
            "file_summary": " The main functionality of the Python file \"functions.py\" is to provide a decorator `@async_execution` that can be applied to functions to indicate that they return a `torch.futures.Future` object and can run asynchronously on the RPC callee. This decorator is useful when the wrapped function's execution needs to pause and resume due to containing `rpc_async` or waiting for other signals. The returned `Future` object only exists on the callee side and is never sent through RPC. The decorator is used in conjunction with RPC APIs to enable asynchronous execution of functions.",
            "uuid": "45ce1246-4de8-4958-b39e-b0257c2ee454"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/rpc/functions.py",
            "number_of_lines": 170,
            "project_name": "pytorch",
            "total_tokens": 2330,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.313282,
            "file_summary": " This Python file defines a decorator named `async_execution` that can be used to indicate that a function can run asynchronously on the RPC callee in PyTorch. The decorator returns a wrapper function that, when applied to a function, allows the function to return a `Future` object, which can be processed as a callback on the callee side. The main functionality of the file is to enable asynchronous execution of functions in PyTorch RPC calls.",
            "uuid": "8da8d228-9243-41d3-800f-15657d17baf9"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/rpc/functions.py",
            "number_of_lines": 170,
            "project_name": "pytorch",
            "file_summary": "The provided Python code defines a decorator called async_execution which allows a function's execution to run asynchronously on the RPC callee. It is designed to work with functions that return a Future object, enabling the function to pause and resume execution for certain operations. The decorator ensures that the callee processes the Future object returned by the function and sends the value back as the RPC response.",
            "total_tokens": 1978,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.228361,
            "uuid": "5f85663d-74ed-4984-ac34-628884c138f3"
        }
    },
    "/torch/jit/_dataclass_impls.py": {
        "nova-lite-v1": {
            "file_summary": "This Python file provides functionality for synthesizing magic methods (like `__init__`, `__repr__`, `__hash__`, `__eq__`, and `__ne__`) for dataclasses when they are JIT-compiled, specifically for use with PyTorch's TorchScript. It includes helper functions to generate and parse these methods, ensuring compatibility and proper behavior within the TorchScript environment. The file also handles special cases such as `InitVar` fields and optional types, while raising appropriate errors for unsupported features.",
            "total_tokens": 1978,
            "latency": 0.82649,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "1a107ee2-ef3f-44df-9a73-f3141003c992"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to synthesize magic methods for JIT-compiled dataclasses. It includes functions for generating the `__init__`, `__repr__`, `__hash__`, `__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, and `__ge__` methods. The file also handles InitVar annotations and optional fields in the dataclasses. The synthesized methods are stored in a dictionary called `DATACLASS_MAGIC_METHODS`.",
            "total_tokens": 113,
            "latency": 2.819236,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "2b98c46b-3bfc-41f8-8436-13e22a62da07"
        },
        "gpt-4.5": {
            "file_summary": "This Python module provides functionality to dynamically synthesize dataclass magic methods (such as `__init__`, `__repr__`, `__hash__`, and comparison/equality methods) specifically tailored for compatibility with TorchScript compilation. It parses and generates method definitions as abstract syntax trees (ASTs), handles special dataclass features like `InitVar` and optional fields, and ensures compliance with TorchScript interpreter constraints. It allows automatic JIT compilation and integration of Python dataclasses into PyTorch's TorchScript by defining placeholder and custom synthesized implementations for dataclass magic methods.",
            "total_tokens": 1766,
            "latency": 5.187836,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "132ea955-aa2a-47bc-9e9e-b2303e009b69"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the given Python file \"_dataclass_impls.py\" is to provide functions for synthesizing magic methods for JIT-compiled dataclasses using TorchScript. These functions include generating methods such as `__init__`, `__repr__`, `__hash__`, and other comparison methods like `__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, and `__ge__`. The file handles creating synthetic methods for dataclasses and dealing with specific cases like InitVars and optional field comparisons for TorchScript compatibility.",
            "total_tokens": 1778,
            "latency": 1.901149,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "82e2013c-ff85-4675-b49a-a5dd90f4837f"
        },
        "Claude3.5": {
            "file_summary": "This Python file contains functions for synthesizing magic methods for JIT-compiled dataclasses in the context of TorchScript. It provides implementations for various dunder methods such as __init__, __repr__, __hash__, and comparison methods (__eq__, __ne__, __lt__, __le__, __gt__, __ge__) for dataclasses. The file also includes helper functions to compose and parse these synthesized methods, handling special cases like InitVar annotations and optional fields.",
            "total_tokens": 2144,
            "latency": 4.374651,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "e259c71d-d6ad-46e7-8c4a-b37fb79e9edf"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `_dataclass_impls.py` file is to synthesize magic methods for JIT-compiled dataclasses in TorchScript, including `__init__`, `__repr__`, `__hash__`, and comparison methods like `__eq__`, `__ne__`, etc. It generates the appropriate method implementations based on the dataclass fields and their annotations. The synthesized methods are then parsed into abstract syntax trees (ASTs) for further processing.",
            "total_tokens": 1970,
            "latency": 1.356936,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "5e9e7931-1477-4282-bf4c-c778a05e1faf"
        },
        "gpt-4o": {
            "file_summary": "The `_dataclass_impls.py` file contains functions for synthesizing special magic methods, such as `__init__`, `__repr__`, `__hash__`, and comparison methods (`__eq__`, `__ne__`, etc.), for dataclasses in TorchScript, a subset of Python used in PyTorch for JIT compilation. It enables defining custom implementations of these methods by generating Python code snippets dynamically and parsing them into Abstract Syntax Trees (ASTs). The file also handles specific cases, such as optional fields, InitVar annotations, and raises appropriate errors for unsupported features like default factory initializers or `None` comparisons.",
            "total_tokens": 1781,
            "latency": 2.799993,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "11f3d482-de22-4211-a94f-eb6bdef08fad"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to synthesize magic methods for JIT-compiled dataclasses in PyTorch. It provides functions to generate implementations for special methods such as `__init__`, `__repr__`, `__hash__`, and comparison operators (`__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, `__ge__`) for dataclasses. These synthesized methods are used to support dataclass usage in TorchScript.",
            "total_tokens": 1766,
            "latency": 3.967819,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "2f272886-85f5-4a7f-a455-5aef0220a2e0"
        },
        "gpt-4": {
            "file_summary": "The Python file `_dataclass_impls.py` primarily provides functionality to synthesize magic methods for dataclasses that are meant to be compiled by TorchScript, a Python script to machine code compiler. It includes capabilities to synthesize constructors (`__init__`), representations (`__repr__`), hash functions (`__hash__`), and comparison operators (like `__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, `__ge__`) for dataclasses based on predefined templates and handling of special cases like optional fields or unsupported features (e.g., default factory initializers). The synthesized methods are transformed into parseable Python AST (Abstract Syntax Trees) structures, ensuring they are tailored to fit the requirements and limitations of TorchScript while still preserving original dataclass behavior where possible.",
            "total_tokens": 1829,
            "latency": 4.468397,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "cc69953a-fa2c-47ba-8ca9-d7e08e1a4985"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the given Python file `_dataclass_impls.py` is to synthesize magic methods for JIT-compiled dataclasses in PyTorch, including `__init__`, `__repr__`, `__hash__`, and comparison methods. These synthesized methods are used to support dataclasses in TorchScript, allowing for more seamless integration with PyTorch's just-in-time compilation. The file provides a set of functions that generate the necessary code for these magic methods based on the fields and annotations defined in the dataclass.",
            "total_tokens": 1775,
            "latency": 1.949948,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "253b1430-6969-436d-9016-bfb20e939ddc"
        },
        "gpt-4.1": {
            "file_summary": "This file provides functions to programmatically synthesize and generate magic methods (such as __init__, __repr__, __hash__, and rich comparison methods) for Python dataclasses to ensure compatibility with TorchScript\u2019s JIT compilation. It constructs method ASTs and signatures dynamically, handling Python dataclass features and TorchScript limitations (e.g., disallowing default_factory and handling InitVar and optional fields). The file exposes a mapping (DATACLASS_MAGIC_METHODS) that links method names to their synthetic generators, enabling automated injection of these methods into dataclasses during JIT compilation for PyTorch.",
            "total_tokens": 1771,
            "latency": 1.687974,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 191,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "ebe7f653-f15d-4c12-b087-a164145abe37"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/jit/_dataclass_impls.py",
            "number_of_lines": 191,
            "project_name": "pytorch",
            "total_tokens": 2240,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.240058,
            "file_summary": "This Python file provides functions to synthesize standard Python magic methods like `__init__`, `__repr__`, `__eq__`, etc., for dataclasses. These synthesized methods are specifically designed to be compatible with TorchScript's JIT compilation process. The code generates Python source strings for these methods and parses them into AST nodes wrapped in `ParsedDef` objects.",
            "uuid": "db2b047c-1171-4c62-a10e-a7e8553f0470"
        },
        "gemini-2.5": {
            "file_name": "/torch/jit/_dataclass_impls.py",
            "number_of_lines": 191,
            "project_name": "pytorch",
            "total_tokens": 2678,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 8.757888,
            "file_summary": "This Python file provides functionality to automatically synthesize the source code for standard magic methods (like `__init__`, `__repr__`, `__eq__`, etc.) for dataclasses. These generated methods are specifically designed to be compatible with PyTorch's TorchScript JIT compiler. The code inspects the dataclass fields and attributes to construct the method bodies as strings, which are then parsed into Abstract Syntax Trees (ASTs) for TorchScript consumption.",
            "uuid": "ccd002b4-38e0-4b7e-b906-6f72978328f3"
        },
        "Claude3.7": {
            "file_name": "/torch/jit/_dataclass_impls.py",
            "number_of_lines": 191,
            "project_name": "pytorch",
            "total_tokens": 2132,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.977442,
            "file_summary": "This file provides functionality for synthesizing magic methods for JIT-compiled dataclasses in PyTorch, enabling dataclasses to work properly within TorchScript. It contains functions that generate code for common dataclass methods like `__init__`, `__repr__`, comparison operators, and `__hash__`, handling special cases like optional fields and InitVars while ensuring type compatibility with the TorchScript interpreter.",
            "uuid": "122c3862-03c2-4c90-bee0-9e159dbf29b7"
        },
        "mistral-small": {
            "file_name": "/torch/jit/_dataclass_impls.py",
            "number_of_lines": 191,
            "project_name": "pytorch",
            "total_tokens": 2267,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.149403,
            "file_summary": " The main functionality of the file \"_dataclass_impls.py\" is to synthesize magic methods for JIT-compiled dataclasses in PyTorch. It defines functions to generate __init__, __repr__, __hash__, __eq__, __ne__, __lt__, __le__, __gt__, and __ge__ methods for dataclasses, which are required for proper functionality in the PyTorch JIT compiler. The file uses the ast, dataclasses, inspect, and os modules to parse and generate the required code.",
            "uuid": "c0e2b172-69aa-4566-86ba-e5c5a61fd740"
        },
        "mistral-7b": {
            "file_name": "/torch/jit/_dataclass_impls.py",
            "number_of_lines": 191,
            "project_name": "pytorch",
            "total_tokens": 2272,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.456935,
            "file_summary": " This Python file is a part of PyTorch library and provides functions for synthesizing magic methods for JIT-compiled dataclasses. It defines various helper functions to generate the implementation of special methods like `__init__`, `__repr__`, `__hash__`, `__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, and `__ge__` based on the defined fields in the dataclass. The generated methods handle assignment, comparison, and initialization of the dataclass instances.",
            "uuid": "3748593e-d179-42e9-ab9b-1fc170231a0a"
        },
        "cohere-v1": {
            "file_name": "/torch/jit/_dataclass_impls.py",
            "number_of_lines": 191,
            "project_name": "pytorch",
            "file_summary": ": Sure! The provided Python script appears to be focused on synthesizing magic methods for JIT-compiled dataclasses in PyTorch. It defines functions to create ParsedDef instances, which are likely used in the TorchScript JIT compilation process. These functions generate the implementations of special methods like __init__, __repr__, and various comparison methods.",
            "total_tokens": 1844,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.141503,
            "uuid": "7e22f451-685b-464c-9f2d-011400f99f65"
        }
    },
    "/tools/linter/adapters/shellcheck_linter.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"shellcheck_linter.py\" file is to run the shellcheck linter on specified files, capturing and formatting linting results as JSON output. It uses the `subprocess` module to execute shellcheck, processes the JSON output to create lint messages, and handles errors such as the absence of the shellcheck command. The script is designed to be run from the command line, accepting file paths as arguments and outputting lint messages in JSON format.",
            "total_tokens": 958,
            "latency": 1.092479,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "2f3a8c82-9122-48c6-a8b2-ffd9fec5a6f5"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the \"shellcheck_linter.py\" file is to run the shellcheck command on the provided list of files and output the results in JSON format. The file includes error handling for cases where shellcheck is not installed or the command fails. It also includes a function to run a command and log the execution time. The output includes information about the file path, line number, character position, code, severity, name, original text, replacement text, and description for each lint message.",
            "total_tokens": 104,
            "latency": 2.445549,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "fe88954a-9a94-4ec8-bae9-544267f4f2ac"
        },
        "gpt-4.5": {
            "file_summary": "This Python script acts as a wrapper around ShellCheck, a tool used to lint shell scripts for common issues and potential errors. It executes ShellCheck on provided file names, parses its JSON-formatted output, and transforms results into structured lint messages. The script also includes proper error handling for the scenario where ShellCheck is not installed or command execution fails.",
            "total_tokens": 829,
            "latency": 4.436866,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b6f2567b-2dc6-498b-a513-fd036c842ca5"
        },
        "gpt-3.5-turbo": {
            "file_summary": "This Python script is a shellcheck linter that runs shellcheck on specified files and outputs the lint results in a JSON format. It defines classes for lint severity and lint message, runs shellcheck using subprocess and captures the output as JSON, then parses the JSON output to create lint message objects for each issue found in the files being checked. Additionally, it includes a command-line interface for specifying files to lint and checks if shellcheck is installed before running.",
            "total_tokens": 845,
            "latency": 1.013356,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "543462b6-8154-447d-95f1-63c93e468501"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"shellcheck_linter.py\" file is to run the ShellCheck linter on specified shell script files and report any issues found. It uses the ShellCheck command-line tool to analyze the files, parses the JSON output, and generates lint messages in a standardized format. The script can be run as a standalone tool, accepting file paths as arguments, and it checks for the presence of ShellCheck before execution.",
            "total_tokens": 1061,
            "latency": 2.70611,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "80159d44-ce2f-48df-9d19-d7b62a9d1406"
        },
        "nova-pro-v1": {
            "file_summary": "The `shellcheck_linter.py` file is a Python script that runs the ShellCheck linter on specified shell script files. It processes the output in JSON format, converts it into a list of `LintMessage` objects, and prints each lint message as a JSON object. The script also handles errors, such as when ShellCheck is not installed.",
            "total_tokens": 933,
            "latency": 1.008776,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "2d86a2c9-2e5a-4f03-a377-5b82190976fb"
        },
        "gpt-4o": {
            "file_summary": "The file implements a Python-based linter wrapper for the `shellcheck` tool, which analyzes shell script files and returns linting messages in JSON format. It defines severity levels, formats the linting results into structured data, and handles errors like missing `shellcheck` installation. The script can be executed to check specified files for issues, outputting the results in a consistent JSON structure.",
            "total_tokens": 837,
            "latency": 1.404488,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "5ce07286-92a9-4e8f-af74-302519b42735"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to run the shellcheck linter on a list of files and output the linting results in JSON format. The script checks for errors, warnings, and advice in the files and prints out the corresponding lint messages. If the shellcheck command is not installed, the script will output an error message and exit.",
            "total_tokens": 830,
            "latency": 2.83789,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "6f809a2a-b227-4bf1-b373-8953d7e780dd"
        },
        "gpt-4": {
            "file_summary": "The Python file named \"shellcheck_linter.py\" is a tool that leverages the shellcheck utility to lint shell scripts. The file defines a command-line interface that accepts file paths as arguments to be linted by invoking the 'shellcheck' command with specific arguments and parsing its JSON-formatted output into structured lint messages. It encapsulates each linting outcome in a structured format with details about the file, line number, character position, severity of the issue, unique code, and description of the issue.",
            "total_tokens": 857,
            "latency": 3.348732,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d6a4db65-c1e8-4a77-968f-31ba20850341"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `shellcheck_linter.py` file is to run the ShellCheck linter on a list of provided files, parsing the output and printing any linting errors or warnings in a JSON format. The script uses the `subprocess` module to execute the `shellcheck` command and handles errors, such as the command not being installed. The script also supports command-line arguments for specifying the files to lint and checks if the `shellcheck` command is installed before attempting to run it.",
            "total_tokens": 864,
            "latency": 1.490354,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d051f822-c176-4b8a-90c0-6c332adc9023"
        },
        "gpt-4.1": {
            "file_summary": "The file shellcheck_linter.py is a command-line tool that runs ShellCheck on given shell script files, capturing and formatting linting diagnostics into a standardized JSON output. It parses ShellCheck's JSON output, maps lint messages to a unified LintMessage structure, and reports errors or warnings for each file checked. If ShellCheck is not installed or fails to run, the tool emits a descriptive error message in the same structured format.",
            "total_tokens": 846,
            "latency": 1.29149,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 121,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "55b35f9f-31a5-4607-a343-6ccded54d07c"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/linter/adapters/shellcheck_linter.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1524,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.178118,
            "file_summary": "This Python script acts as a wrapper for the `shellcheck` command-line tool. It takes shell script filenames as input, executes `shellcheck` with JSON output formatting, and parses the results. The script then prints each detected linting issue as a JSON object, including checks for `shellcheck` installation.",
            "uuid": "73c588ec-2e0d-4990-a6c4-869b41b81a6a"
        },
        "gemini-2.5": {
            "file_name": "/tools/linter/adapters/shellcheck_linter.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1350,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 5.337724,
            "file_summary": "This Python script serves as a wrapper for the `shellcheck` command-line tool to lint shell script files. It takes a list of filenames as input, executes `shellcheck` with JSON output enabled, and parses the results. Finally, it formats each lint finding into a structured `LintMessage` object and prints it as a JSON string to standard output.",
            "uuid": "4064c173-0ccf-44cd-a922-dd7b766fa5f8"
        },
        "Claude3.7": {
            "file_name": "/tools/linter/adapters/shellcheck_linter.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1051,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.274366,
            "file_summary": "This Python file implements a linter that uses shellcheck to analyze shell scripts for errors and issues. It parses shell script files, runs the shellcheck command on them, and formats the results as structured LintMessage objects that include details like file path, line number, error code, and description. The script can be run as a standalone tool that accepts file paths as arguments and outputs the lint results in JSON format.",
            "uuid": "1723c6fc-89f4-44ac-9fa4-8a79227e209e"
        },
        "mistral-small": {
            "file_name": "/tools/linter/adapters/shellcheck_linter.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1103,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.826155,
            "file_summary": " The main functionality of the file \"shellcheck_linter.py\" is to serve as a linter for shell scripts using the \"shellcheck\" tool. It takes a list of shell script file names as input, runs the \"shellcheck\" command on them, and outputs the linting results in a JSON format. The linting results include the file path, line number, character position, code, severity, name, original text, replacement text, and description of the issue.",
            "uuid": "d340a63d-88e0-4b72-b7fc-189ddd45d3d3"
        },
        "mistral-7b": {
            "file_name": "/tools/linter/adapters/shellcheck_linter.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "total_tokens": 1088,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.33592,
            "file_summary": " The main functionality of the file is to implement a Python script named \"shellcheck\\_linter.py\" which serves as a runner for the Shellcheck linter tool. It accepts one or more filenames as arguments, checks their Shell script syntax using Shellcheck, and outputs the linting results in JSON format. If Shellcheck is not installed, the script will print an error message and exit.",
            "uuid": "62097ed4-337e-40a6-a3f3-2375c1e372c6"
        },
        "cohere-v1": {
            "file_name": "/tools/linter/adapters/shellcheck_linter.py",
            "number_of_lines": 121,
            "project_name": "pytorch",
            "file_summary": "The Python script 'shellcheck_linter.py' appears to be a linter for shell scripts. It uses the 'shellcheck' command-line tool to analyze shell script files and outputs JSON-formatted messages containing information about any linting issues found. The script appears to be designed to facilitate running shellcheck and handle the output in a structured manner.",
            "total_tokens": 893,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.120184,
            "uuid": "3abf30d3-4367-4d8f-8b6a-8761d560a4a1"
        }
    },
    "/torch/distributed/optim/post_localSGD_optimizer.py": {
        "nova-lite-v1": {
            "file_summary": "The `post_localSGD_optimizer.py` file defines a `PostLocalSGDOptimizer` class that wraps an arbitrary PyTorch optimizer to implement the post-local SGD optimization algorithm. This optimizer runs a local optimizer at each step and periodically averages the model parameters after the local optimizer is applied, following a warm-up stage. The class integrates with PyTorch's distributed training capabilities, allowing for efficient model training across multiple devices or nodes.",
            "total_tokens": 1355,
            "latency": 0.980652,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "5265346b-31b1-4cbe-a008-8d423afa8846"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"post_localSGD_optimizer.py\" is to define a class called \"PostLocalSGDOptimizer\" that wraps an arbitrary optimizer and runs post-local SGD algorithm. This optimizer runs the local optimizer at every step and averages parameters periodically after the local optimizer is applied. It also includes methods to save and load the optimizer's state, including the step counter for the model averager. The class is designed to be used with PyTorch's distributed data parallel (DDP) module for distributed training.",
            "total_tokens": 121,
            "latency": 3.004525,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a8299c60-ae58-4043-b4a7-75ad9fc25e40"
        },
        "gpt-4.5": {
            "file_summary": "This Python file implements a \"PostLocalSGDOptimizer\" class in PyTorch, wrapping around a standard optimizer to facilitate training with Post-local SGD, a distributed optimization algorithm. After an initial warm-up period, this optimizer periodically averages model parameters across nodes to achieve faster distributed training convergence. It supports state saving and loading for continuation of training from checkpoints.",
            "total_tokens": 1182,
            "latency": 3.550377,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "415afc61-cb75-4d59-92fd-9d89d3ac4b53"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the \"post_localSGD_optimizer.py\" file is to define a custom optimizer called \"PostLocalSGDOptimizer\" that wraps an existing optimizer and implements the post-local SGD algorithm. This optimizer runs a local optimizer at every step and averages parameters periodically after applying the local optimizer. It also includes methods for saving and loading the optimizer state, performing optimization steps, and managing parameter groups.",
            "total_tokens": 1202,
            "latency": 0.937667,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "4fbc2b28-3e17-4ef5-a662-9ecfbd24cb1c"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"post_localSGD_optimizer.py\" file is to implement a PostLocalSGDOptimizer class, which wraps an arbitrary PyTorch optimizer and applies the post-local SGD algorithm. This optimizer runs a local optimizer at every step and, after a warm-up stage, periodically averages parameters across distributed processes to improve model performance in distributed training scenarios. The file also includes methods for state management, parameter updates, and integration with PyTorch's distributed training ecosystem.",
            "total_tokens": 1528,
            "latency": 3.39965,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "41927f42-b0da-480c-8c9d-9a566f1edba0"
        },
        "nova-pro-v1": {
            "file_summary": "The `post_localSGD_optimizer.py` file defines a `PostLocalSGDOptimizer` class that wraps an existing PyTorch optimizer to implement the post-local SGD algorithm. This optimizer applies the local optimizer at every step and periodically averages parameters after the warm-up stage. The class provides methods to save and load the optimizer's state, including the model averager's step.",
            "total_tokens": 1345,
            "latency": 1.146974,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "5f893a62-6a78-4a28-b1e4-ab7b428e566b"
        },
        "gpt-4o": {
            "file_summary": "The `post_localSGD_optimizer.py` file defines a `PostLocalSGDOptimizer` class, a wrapper around the PyTorch optimizer, to implement the post-local SGD algorithm. It combines a local optimizer with periodic global parameter averaging using a model averager, enabling efficient distributed training during and after a warmup phase. The class also ensures proper state management for checkpointing to preserve and restore the optimizer and averager states.",
            "total_tokens": 1198,
            "latency": 1.932739,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "724a40c9-c205-4aee-bdf0-e5f869ef9638"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to implement a custom PyTorch optimizer called `PostLocalSGDOptimizer` that wraps an arbitrary PyTorch optimizer and runs the post-local SGD algorithm. This algorithm applies local optimization at every step and periodically averages parameters after a warm-up stage. The optimizer is designed to work with distributed data parallelism (DDP) and can be used to train models in a distributed setting.",
            "total_tokens": 1211,
            "latency": 3.496911,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "57b8a81e-a665-4e57-a6e7-4659634e98b8"
        },
        "gpt-4": {
            "file_summary": "The Python file defines a class `PostLocalSGDOptimizer` that extends `torch.optim.Optimizer` to implement the post-local Stochastic Gradient Descent (SGD) algorithm. This optimizer wraps any Torch optimizer and combines local optimization with periodic model averaging after a warm-up stage, based on the `post-local SGD` technique discussed in a published research paper. The class includes methods such as `step`, `zero_grad`, `add_param_group`, `state_dict`, and `load_state_dict` to manage the state and behavior of the optimizer, integrating the additional functionality to handle periodic averaging via an averager provided during initialization.",
            "total_tokens": 1249,
            "latency": 4.981761,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "34d37132-16e5-4607-87f2-053e00fded4c"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement a custom optimizer, `PostLocalSGDOptimizer`, which wraps an arbitrary PyTorch optimizer and applies the post-local SGD algorithm. This algorithm runs local optimization at every step and averages parameters periodically after a warm-up stage. The optimizer is designed to work with distributed data parallelism and can be used to train models in a distributed setting.",
            "total_tokens": 1207,
            "latency": 1.125943,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d6cdb086-cf97-47a5-b76a-2c2990663519"
        },
        "gpt-4.1": {
            "file_summary": "The file defines PostLocalSGDOptimizer, a wrapper around any PyTorch optimizer that implements the post-local SGD algorithm, which performs local parameter updates and periodically averages parameters across workers after a warm-up phase, facilitating efficient distributed training. It manages optimizer state, handles saving/loading of both optimizer and model averager step, and delegates most optimizer methods to the wrapped optimizer while integrating parameter averaging logic. This enables seamless usage of post-local SGD in distributed settings, especially in conjunction with DistributedDataParallel and model averaging modules.",
            "total_tokens": 1214,
            "latency": 1.502515,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 111,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8b755294-bc0d-4080-9665-81901cde8758"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/optim/post_localSGD_optimizer.py",
            "number_of_lines": 111,
            "project_name": "pytorch",
            "total_tokens": 1657,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.087074,
            "file_summary": "This Python file defines `PostLocalSGDOptimizer`, a wrapper around a standard PyTorch optimizer. It implements the post-local SGD algorithm by first applying the wrapped optimizer's step and then periodically averaging model parameters using a provided averager. The optimizer also handles saving and loading the averager's internal step count along with the optimizer's state.",
            "uuid": "9cd5f071-37cc-4e07-9cee-3dc244806089"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/optim/post_localSGD_optimizer.py",
            "number_of_lines": 111,
            "project_name": "pytorch",
            "total_tokens": 1606,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 4.535956,
            "file_summary": "This Python file defines the `PostLocalSGDOptimizer` class, which wraps a standard PyTorch optimizer to implement the Post-Local SGD algorithm for distributed training. It executes the underlying local optimizer at each step. After an optional warm-up phase, it periodically averages the model parameters across distributed processes using a provided model averager.",
            "uuid": "569aa7d9-3b04-4ab6-b492-51adfee88317"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/optim/post_localSGD_optimizer.py",
            "number_of_lines": 111,
            "project_name": "pytorch",
            "total_tokens": 1522,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.164492,
            "file_summary": "The `post_localSGD_optimizer.py` file implements a PyTorch optimizer wrapper called `PostLocalSGDOptimizer` that enhances distributed training by implementing the post-local SGD algorithm. It wraps any standard PyTorch optimizer and periodically averages model parameters across workers after applying local optimization steps, following a configurable warm-up period. This implementation helps reduce communication overhead in distributed training while maintaining model accuracy by combining local optimization with periodic global parameter averaging.",
            "uuid": "f383b13a-4c24-42e2-a4db-e315536cf41a"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/optim/post_localSGD_optimizer.py",
            "number_of_lines": 111,
            "project_name": "pytorch",
            "total_tokens": 1550,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.173167,
            "file_summary": " The main functionality of the file \"post_localSGD_optimizer.py\" is to define a custom PyTorch optimizer called `PostLocalSGDOptimizer`. This optimizer wraps an arbitrary PyTorch optimizer and runs a post-local SGD algorithm, which applies a local optimizer at every step and averages parameters periodically after the warm-up stage. The optimizer takes a local optimizer and a model averager instance as arguments. The file also includes a docstring with an example usage of the optimizer in a distributed data parallel (DDP) setting.",
            "uuid": "d432ff86-eb18-4a36-99e1-c17cfe5fbac6"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/optim/post_localSGD_optimizer.py",
            "number_of_lines": 111,
            "project_name": "pytorch",
            "total_tokens": 1505,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.149648,
            "file_summary": " The given Python file defines a custom optimizer named `PostLocalSGDOptimizer` that wraps an arbitrary `torch.optim.Optimizer` and implements post-local SGD algorithm. After a warm-up stage, it averages parameters periodically after the local optimizer is applied using a provided `averagers.ModelAverager` instance.",
            "uuid": "38993d91-9eaf-4118-91b0-1b2b3386c987"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/optim/post_localSGD_optimizer.py",
            "number_of_lines": 111,
            "project_name": "pytorch",
            "file_summary": "The code defines a PostLocalSGDOptimizer class that wraps any torch.optim.Optimizer to perform post-local Stochastic Gradient Descent (SGD) optimization. It adds functionality to run local optimization steps and then periodically average the parameters using a provided averager instance. This approach is designed for distributed training and includes methods for handling optimizer and averager states.",
            "total_tokens": 1303,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.096501,
            "uuid": "b7485759-04bc-4b2d-af14-22a97f873de3"
        }
    },
    "/torchgen/operator_versions/gen_mobile_upgraders.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `gen_mobile_upgraders.py` file is to generate C++ source code for mobile bytecode upgraders based on the provided bytecode definitions. It reads bytecode information, constructs C++ templates for upgrader functions, and writes the final C++ source code to a specified directory within a PyTorch project. The script also handles version mappings and excludes certain operators during the generation process.",
            "total_tokens": 3629,
            "latency": 1.12652,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "314ccb3b-f78c-4934-8c33-817703b673f5"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to generate a C++ file (upgrader\\_mobile.cpp) that contains a list of bytecode functions and operator strings for mobile devices. The file uses various code templates and constructs to generate the necessary code. It also sorts the upgrader list and excludes certain operators and upgraders from the final output. The generated C++ file is then written to the specified directory.",
            "total_tokens": 87,
            "latency": 2.572564,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "38328500-70cf-4604-b42f-d7527bc81523"
        },
        "gpt-4.5": {
            "file_summary": "This Python script generates C++ source code for managing operator version upgrades in PyTorch mobile, focusing on creating bytecode definitions that can upgrade model operators from older versions to newer ones on mobile devices. It parses upgrade definitions, constructs bytecode instructions, constants, operators, and type lists from the definitions, and then generates a C++ source file for Torch's mobile JIT runtime. The resulting C++ file helps PyTorch mobile applications maintain compatibility by upgrading serialized models' operator implementations dynamically.",
            "total_tokens": 2965,
            "latency": 5.498746,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "3a5c8ef0-2ca9-4db1-89a4-5cc3b0f2e5b9"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided Python script generates bytecodes for upgrading mobile versions of operators in PyTorch. It includes functions to construct instructions, constants, operators, and type lists, as well as to write the generated C++ code into a file named 'upgrader_mobile.cpp' in the specified PyTorch directory. The script processes a list of upgrader dictionaries, sorts them, and generates bytecode functions based on the provided data.",
            "total_tokens": 3032,
            "latency": 1.419613,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "3c1d95c1-5748-42cf-acba-f877bc04f33d"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"gen_mobile_upgraders.py\" file is to generate and write C++ code for mobile upgraders in PyTorch. It processes bytecode information for various upgraders, constructs version maps, and creates a C++ file containing the necessary data structures and functions for mobile upgraders. The script also handles the organization and sorting of upgrader information before writing it to the output file.",
            "total_tokens": 4018,
            "latency": 2.977219,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "7acfe816-b7f8-4cfe-896c-b84da2018579"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the file `gen_mobile_upgraders.py` is to generate C++ source code for mobile upgraders in PyTorch by constructing bytecode functions and version maps, and writing them to a specified path.",
            "total_tokens": 3590,
            "latency": 0.915312,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "abd6ae47-7fdb-4181-868e-965274f62146"
        },
        "gpt-4o": {
            "file_summary": "The script `gen_mobile_upgraders.py` focuses on generating C++ source code for mobile upgrader functionality in PyTorch. It processes data such as instructions, constants, and operator definitions to generate a C++ file (`upgrader_mobile.cpp`) that includes bytecode and version mappings for mobile operator upgrades. The generated file facilitates backward compatibility by defining how older model operators should be upgraded to ensure compatibility with newer runtime environments.",
            "total_tokens": 2953,
            "latency": 2.349103,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "f0ac57ab-ad55-4e50-ae22-ba26c3a0771f"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to generate C++ code for mobile upgraders in the PyTorch framework. It takes a list of upgrader dictionaries as input, constructs the necessary C++ code for each upgrader, and writes the generated code to a file named \"upgrader_mobile.cpp\". The file also defines various helper functions for constructing different parts of the C++ code, such as instructions, constants, operators, and types.",
            "total_tokens": 3045,
            "latency": 4.377408,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "73a734bc-5b4d-4eec-8c86-381d093b1674"
        },
        "gpt-4": {
            "file_summary": "The Python script `gen_mobile_upgraders.py` primarily supports the generation of C++ code related to model upgraders for mobile platforms using PyTorch. The functions included in the file handle the construction and organization of upgrader specifications into structured formats such as bytecode instructions, constants, types, and operators which are later used to generate a C++ source file (`upgrader_mobile.cpp`). It also ensures filtering and sorting of operators and upgraders based on given constraints and criteria, supporting the maintenance and management of operator versioning for mobile deployment.",
            "total_tokens": 3059,
            "latency": 3.110574,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "6eb99e3d-ba66-4dac-9e9d-0ca1a4e4dccd"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `gen_mobile_upgraders.py`, is to generate C++ code for mobile upgraders in the PyTorch framework. It takes a list of upgrader dictionaries as input, constructs the necessary C++ code for each upgrader, and writes the generated code to a file named `upgrader_mobile.cpp`. The file utilizes various templates and functions to construct the C++ code, including instruction lists, constant lists, operator strings, and version maps.",
            "total_tokens": 3056,
            "latency": 1.827399,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "57958092-295a-4c9f-b1db-60bf3e25320d"
        },
        "gpt-4.1": {
            "file_summary": "The gen_mobile_upgraders.py file generates C++ source code required for mobile operator upgraders in PyTorch by processing Python-level operator upgrader definitions and converting them into bytecode representations. It constructs C++ code for upgrader functions, their associated bytecode, and operator version maps, which are then output to upgrader_mobile.cpp for integration into PyTorch Mobile. The script is primarily used to bridge Python upgrader definitions and their deployment in the C++-based mobile runtime of PyTorch.",
            "total_tokens": 2969,
            "latency": 1.73474,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 390,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "12f69eb8-80a7-4a02-aa26-ef4bfdf943c4"
        },
        "gemini-2.5-flash": {
            "file_name": "/torchgen/operator_versions/gen_mobile_upgraders.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 4416,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.141936,
            "file_summary": "This Python script generates bytecode representations for PyTorch mobile operator upgraders and retrieves operator versioning information. It processes this data and uses code templates to construct C++ source code defining the upgraders and their version map. Finally, it writes this generated C++ code to the file `upgrader_mobile.cpp` for use in the mobile runtime.",
            "uuid": "fb4494f0-7599-47ba-af29-ccd56f0092d5"
        },
        "gemini-2.5": {
            "file_name": "/torchgen/operator_versions/gen_mobile_upgraders.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 5112,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 13.395299,
            "file_summary": "This Python script generates a C++ source file (`upgrader_mobile.cpp`) for the PyTorch mobile runtime. It retrieves operator versioning information and bytecode for model upgraders using internal PyTorch functions. The script then formats this data into C++ structures and maps using predefined code templates, enabling the mobile runtime to handle compatibility for older TorchScript models.",
            "uuid": "c2dd171f-3a05-495a-8018-5493b3d39400"
        },
        "Claude3.7": {
            "file_name": "/torchgen/operator_versions/gen_mobile_upgraders.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 4013,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.744716,
            "file_summary": "This Python script generates mobile upgraders for PyTorch by creating C++ code that handles version compatibility in mobile deployments. It processes bytecode information from PyTorch operators, constructs various code components (instructions, constants, types, operators), and outputs an \"upgrader_mobile.cpp\" file containing the necessary functions and data structures to support version upgrades for mobile PyTorch models.",
            "uuid": "bf903132-c5a0-4cf3-a105-265ce084a0eb"
        },
        "mistral-small": {
            "file_name": "/torchgen/operator_versions/gen_mobile_upgraders.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 4247,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.610725,
            "file_summary": " The main functionality of the file \"gen_mobile_upgraders.py\" is to generate C++ source code for mobile upgraders in PyTorch. It uses the bytecode generated from the JIT (Just-In-Time) compiler and converts it into a format that can be used by mobile devices. The generated C++ code includes a list of upgraders, each containing instructions, constants, types, and operators. The upgraders are then registered in a version map for each operator, which is used to upgrade the serialized model during runtime on mobile devices.",
            "uuid": "be975c0b-735a-4ee1-8074-84f1b5f4c071"
        },
        "mistral-7b": {
            "file_name": "/torchgen/operator_versions/gen_mobile_upgraders.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 4218,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.572122,
            "file_summary": " The given Python file, named \"gen\\_mobile\\_upgraders.py\", generates C++ code for PyTorch mobile upgraders. It defines various classes and templates to construct and register upgraders for PyTorch operators based on their instruction lists, constants, types, and operators. The main functionality involves reading upgrader data, constructing C++ code templates, and writing the generated code to a file.",
            "uuid": "4829a87f-179c-486b-a007-305c0f0b9328"
        },
        "cohere-v1": {
            "file_name": "/torchgen/operator_versions/gen_mobile_upgraders.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, gen_mobile_upgraders.py, is responsible for generating C++ source code for mobile upgrader functionality in PyTorch. It processes upgrader information, sorts it, and writes it into a designated C++ file, creating a mapping of operator names to their respective upgraders. The generated C++ file includes functions to upgrade mobile bytecode, improving the compatibility of PyTorch mobile operations.",
            "total_tokens": 3678,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.643719,
            "uuid": "eb0cde76-0e3b-48df-9432-428c819ccae7"
        }
    },
    "/torch/nn/functional.py": {
        "nova-lite-v1": {
            "file_summary": "This file defines various functional operations for tensor manipulation and neural network computations in PyTorch, including convolution, pooling, normalization, activation functions, loss functions, and attention mechanisms. It provides a comprehensive set of utility functions for building and training deep learning models. The functions are designed to handle different input shapes and configurations, and support various optimizations and backends for improved performance.",
            "total_tokens": 69606,
            "latency": 5.937781,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "01025ceb-0d93-47cb-9317-8cbd43269a97"
        },
        "gpt-4.5": {
            "file_summary": "This file provides a collection of PyTorch's functional API implementations, particularly for convolution operations (`conv1d`, `conv2d`, etc.), pooling layers (`max_pool`, `avg_pool`, etc.), normalization layers (`batch_norm`, `layer_norm`, etc.), activation functions (like `relu`, `softmax`), losses (including cross-entropy and MSE), dropout methods, embedding lookups, and multi-head attention mechanisms (`scaled_dot_product_attention` and related functions). Each function contains comprehensive documentation for usage and argument details. Overall, it serves as PyTorch's backend implementations to support neural network modules and custom layers without requiring explicit class definitions.",
            "total_tokens": 61324,
            "latency": 38.340197,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "40b67057-02f4-49fe-96e4-c373573a86e1"
        },
        "Claude3.5": {
            "file_summary": "This Python file contains implementations of various neural network functions and operations, primarily focused on attention mechanisms and related components. Here's a summary of the main functionality:\n\n1. The file defines several attention-related functions, including:\n   - `scaled_dot_product_attention`\n   - `multi_head_attention_forward`\n   - Various utility functions for shape checking, mask handling, and projection operations\n\n2. It implements pooling operations like `max_pool1d`, `max_pool2d`, `max_pool3d`, and their adaptive variants.\n\n3. There are implementations of various loss functions, such as:\n   - `nll_loss`\n   - `cross_entropy`\n   - `binary_cross_entropy`\n   - `kl_div`\n   - `mse_loss`\n   - `l1_loss`\n\n4. The file includes activation functions like `relu`, `gelu`, `silu`, and `softmax`.\n\n5. There are normalization functions like `batch_norm`, `layer_norm`, and `group_norm`.\n\n6. It provides implementations for various dropout techniques, including `dropout`, `alpha_dropout`, and `feature_alpha_dropout`.\n\n7. The file includes utility functions for handling tensors, such as `unfold`, `fold`, and `interpolate`.\n\n8. There are implementations of distance and similarity measures like `cosine_similarity` and `pairwise_distance`.\n\nOverall, this file serves as a comprehensive collection of neural network building blocks, with a particular emphasis on attention mechanisms and related operations used in transformer architectures. It's likely part of a larger deep learning framework or library.",
            "total_tokens": 74334,
            "latency": 15.723343,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "b80617d8-3df8-4b9e-ae6b-f1570c1ecd36"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the file `functional.py` is to provide a collection of functional operations for neural networks in PyTorch, including convolutions, pooling, activation functions, normalization, loss functions, upsampling, padding, and attention mechanisms. These functions are designed to be used directly on tensors without the need for creating module instances, facilitating flexible and modular neural network construction.",
            "total_tokens": 69608,
            "latency": 12.662318,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d51584cb-41bc-4192-9992-bfda9eb066b5"
        },
        "gpt-4o": {
            "file_summary": "The provided Python file defines various functionalities for deep learning operations, primarily focusing on the PyTorch framework. It includes methods for convolutional layers (e.g., `conv1d`, `conv2d`), pooling layers, and various activation functions (e.g., `relu`, `sigmoid`). Advanced functionalities such as multihead attention with `multi_head_attention_forward` and `scaled_dot_product_attention` are provided, along with utilities for handling special data transformations (e.g., `grid_sample`, `pixel_shuffle`, `interpolate`). Additionally, it encapsulates useful loss computations, such as `cross_entropy`, `triplet_margin_loss`, and `ctc_loss`, offering broad utility in building neural network models.",
            "total_tokens": 61336,
            "latency": 35.705794,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8ccbb781-a455-4a87-b47c-67c2bcac0958"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide various functions for neural network operations, including convolutional, pooling, activation, loss, and attention mechanisms. These functions are designed to be used in deep learning models and are implemented using PyTorch, a popular open-source machine learning library.",
            "total_tokens": 60447,
            "latency": 48.251515,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e58dc34c-94c4-42c1-9d9e-a3b41ffdba4f"
        },
        "gpt-4": {
            "file_summary": "The code in the Python file mainly deals with defining multi-head attention mechanisms common in models like transformers. It consists of several functions that handle the calculations for attention mechanisms, including setting up the weight matrices (`in_projection`), conducting the scaled dot product attention (`scaled_dot_product_attention`), and handling multiple heads in attention layers (`multi_head_attention_forward`). These functions are essential components for building complex neural network architectures like Transformer models, which are widely used in natural language processing tasks. Overall, the functions work together to facilitate the attention mechanism, allowing a neural network to focus on different parts of the input data sequentially.",
            "total_tokens": 60507,
            "latency": 11.744846,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "90e7f791-3e9b-41df-9712-c7cbc507844e"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the given Python file is to provide various functions for neural network operations, including convolution, pooling, activation functions, loss functions, and attention mechanisms. These functions are part of the PyTorch library and are used to build and train neural networks. The file contains a large number of functions, each with its own specific purpose, such as `conv1d`, `conv2d`, `conv3d`, `max_pool1d`, `max_pool2d`, `max_pool3d`, `dropout`, `relu`, `sigmoid`, `tanh`, `softmax`, `cross_entropy`, `binary_cross_entropy`, `mean_squared_error`, and many others. The file also includes functions for attention mechanisms, such as `scaled_dot_product_attention` and `multi_head_attention_forward`. Overall, the file provides a wide range of functions for building and training neural networks using PyTorch.",
            "total_tokens": 60574,
            "latency": 15.786967,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "afd864ed-472a-4078-9066-5451759abd9c"
        },
        "gpt-4.1": {
            "file_summary": "This file provides the functional interface for the PyTorch `torch.nn.functional` module, defining a wide range of stateless neural network operations such as convolutions, pooling, normalization, activations, loss functions, distance computations, and utility functions for tensor manipulation (e.g., upsampling and padding). It acts as a bridge to core PyTorch C++/CUDA kernels, adds comprehensive type annotations and docstrings, manages various input options (e.g., handling `__torch_function__` overrides and broadcasting), and includes additional logic for deprecated features, backward compatibility, and expert-level dispatch required for neural network layer implementations. Overall, it enables users and other PyTorch components to compose neural network computations as pure functions without binding them to stateful modules.",
            "total_tokens": 61342,
            "latency": 6.657828,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "49034f9f-3c30-4905-8cd7-ec444b064329"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/functional.py",
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "total_tokens": 70959,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.012917,
            "file_summary": "This Python file provides functional implementations of various neural network operations within the `torch.nn.functional` module. It includes functions for common tasks like convolutions, pooling, activation functions, normalization layers, and loss calculations. These stateless functions operate directly on tensors and serve as the computational backend for their corresponding `torch.nn.Module` counterparts.",
            "uuid": "9ba41e96-0649-446b-87a6-485a1ad8fbae"
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/functional.py",
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "total_tokens": 70999,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.004219,
            "file_summary": "This Python file defines a functional interface for many standard PyTorch neural network operations, acting as the stateless counterpart to layers found in `torch.nn`. It includes a wide range of functions covering convolutions, pooling, activation functions, normalization techniques, loss functions, embeddings, and attention mechanisms. These functions operate directly on tensors, providing building blocks for constructing neural networks without necessarily using the stateful `nn.Module` classes.",
            "uuid": "027a6116-c2b8-4ea8-b365-740267cf98d7"
        },
        "Claude3.7": {
            "file_name": "/torch/nn/functional.py",
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "total_tokens": 74064,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 14.365204,
            "file_summary": "The file \"functional.py\" is a core component of PyTorch's neural network functionality, providing implementations of various neural network operations as functional interfaces. It contains implementations for convolution operations, pooling functions, activation functions, loss functions, attention mechanisms, and other essential neural network building blocks. The file serves as the backbone for PyTorch's nn.functional module, allowing users to apply neural network operations directly to tensors without instantiating module objects.",
            "uuid": "e5a56f88-e02c-4461-8ff5-a7424ee1817a"
        },
        "cohere-v1": {
            "file_name": "/torch/nn/functional.py",
            "number_of_lines": 6423,
            "project_name": "pytorch",
            "file_summary": "The code defines a wide range of functions for various neural network layers and operations, including convolutions, pooling, activations, normalizations, losses, upsampling, and attention mechanisms. These functions enable the construction and training of complex neural networks.",
            "total_tokens": 65944,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 11.658706,
            "uuid": "f46ab8b6-aa2b-45d3-a612-4fbf0dce2a5a"
        }
    },
    "/benchmarks/dynamo/runner.py": {
        "nova-lite-v1": {
            "file_summary": "The `runner.py` script is a command-line tool designed to automate and manage benchmark tests for PyTorch-related projects. It generates shell commands for running benchmarks, executes these commands, and processes the results to produce summaries, visualizations, and reports. The script supports various benchmark configurations, including different compilers, data types, and benchmark suites, and it can output results in CSV format and generate graphs for performance metrics. Additionally, it integrates with a performance dashboard to update it with new benchmark results and detect regressions.",
            "total_tokens": 14487,
            "latency": 1.913131,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0c52f995-b6d5-44c1-9d2f-e470f4a31c47"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given python file is to generate and analyze benchmark results for different models, devices, and compilers. It supports both training and inference modes.\n\nThe script generates commands for running benchmarks, executes them, and then parses the results. It also generates summary statistics, plots graphs, and detects regressions.\n\nThe script uses the following libraries: argparse, dataclasses, datetime, glob, importlib, io, logging, matplotlib.pyplot, numpy, pandas, re, scipy.stats, subprocess, sys, tabulate, and torch.\n\nThe script has several functions, including:\n\n* `generate_commands`: generates bash commands for running benchmarks.\n* `generate_summary`: generates a build summary.\n* `archive_data`: archives the results.\n* `get_archive_name`: generates a unique archive name.\n* `build_summary`: generates a build summary.\n* `archive`: archives the results.\n* `Parser`: a class for parsing benchmark results.\n* `ParsePerformanceLogs`: a subclass of Parser for parsing performance logs.\n* `generate_executive_summary`: generates an executive summary.\n* `generate_warnings`: generates warnings.\n* `prepare_message`: prepares a message for a specific suite.\n* `gen_summary_files`: generates summary files.\n* `SummaryStatDiffer`: a class for comparing summary statistics.\n* `RegressionDetector`: a class for detecting regressions.\n* `RegressionTracker`: a class for tracking regressions.\n* `DashboardUpdater`: a class for updating the dashboard.\n\nThe script uses several environment variables, including:\n\n* `TORCH_CUDA_ARCH_LIST`\n* `CUDA_HOME`\n* `USE_LLVM`\n\nThe script also uses several command-line arguments, including:\n\n* `--devices`: specifies the devices to use.\n* `--dtypes`: specifies the data types to use.\n* `--suites`: specifies the benchmark suites to use.\n* `--compilers`: specifies the compilers to use.\n* `--flag-compilers",
            "total_tokens": 500,
            "latency": 12.909613,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "4b6ce953-12f6-4ad5-8fb4-c8a54954a994"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python script, \"runner.py\", is designed to benchmark PyTorch models by automating the execution of training or inference tasks across various configurations, compilers, and data types. It generates benchmark commands, executes them, collects and parses resulting performance data (such as speedup, accuracy, compilation latency, and memory usage), and can also visualize this data through graphs and CSV outputs. Additionally, it includes functionality for reporting results and regressions systematically through customizable automation and dashboard integration.",
            "total_tokens": 12147,
            "latency": 9.322693,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "9aac8971-8e25-4c76-9f23-c47b5d27c617"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file is a script that serves as a wrapper over a benchmark infrastructure. It allows for generating commands for various benchmark suites, parsing the results, and generating csv/graphs. It supports functionalities such as running specific benchmark suites, visualizing already finished logs, printing commands, and more. Additionally, it includes features to detect regressions, update the performance dashboard on GitHub with graphs and comments, and perform various analyses on the benchmark results such as checking speedup, compilation latency, compression ratio, accuracy, etc.",
            "total_tokens": 12071,
            "latency": 2.324384,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "7847d35d-e78b-4248-b711-8386a8c021c6"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"runner.py\" file can be summarized in three sentences as follows:\n\n1. This script serves as a wrapper over the benchmark infrastructure to generate commonly used commands, parse results, and generate CSV files and graphs for performance benchmarking of PyTorch compilers and backends.\n\n2. It provides options to run benchmarks for different suites (e.g., torchbench, huggingface, timm_models), devices (CPU/GPU), data types (float32/float16), and compilers (e.g., eager, inductor, nvfuser) for both inference and training scenarios.\n\n3. The script can generate commands, execute benchmarks, parse logs, visualize results, detect regressions, and update a performance dashboard with the collected data and analysis.",
            "total_tokens": 15992,
            "latency": 7.246361,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "217a12bb-40ba-4c85-8ff5-49871f0f047b"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `runner.py` file is to serve as a comprehensive wrapper for managing and executing benchmarks, parsing their results, and generating visualizations and reports. It supports generating run commands, visualizing logs, and updating a performance dashboard with detailed summaries and regression detection.",
            "total_tokens": 14436,
            "latency": 1.731109,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "69fa75b6-8b7b-4554-a236-6b63451b7694"
        },
        "gpt-4o": {
            "file_summary": "The `runner.py` file is a benchmarking and profiling tool designed to test and analyze the performance, accuracy, memory footprint, and compilation latency of various PyTorch models and compilers for either inference or training. It allows users to generate benchmarking commands, execute them, parse the resulting logs, and create visualizations and detailed performance reports, including warnings and regressions. The script also integrates with GitHub to facilitate performance dashboards and comparisons over time for ongoing development improvements.",
            "total_tokens": 12143,
            "latency": 2.525488,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "76e71192-c982-40c1-9e43-35d4f1abec27"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to generate commands for running benchmark tests, parse the results, and create visualizations and summaries of the performance metrics. The script can be run in different modes, including generating commands, running the commands and parsing the results, or just visualizing the already finished logs. It also supports updating a dashboard with the results.",
            "total_tokens": 12044,
            "latency": 47.985251,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "73940463-9f24-43f9-b0c2-0fe79b0658b3"
        },
        "gpt-4": {
            "file_summary": "The Python file `runner.py` defines an elaborate benchmark runner script that facilitates the execution and management of performance benchmarks for different backend compilers of PyTorch. The script allows generating benchmark command lines, running those commands, and visualizing the results in terms of logs and graphical outputs. It includes functionality to filter down tests, keep track of performance metrics across multiple runs, and quantify and flag any regressions or anomalous behaviors in the performance data. It utilizes a user-specified configuration table to set up various running conditions for inference and training tasks across different compilers and supports configurations like batch size, data type, and device specifics.",
            "total_tokens": 12092,
            "latency": 4.832093,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "1478072a-f093-4abd-b0af-a43f8eda1592"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to generate and run benchmarking commands for PyTorch models, parse the results, and create visualizations and summaries of the performance metrics. The script can be run in different modes, including generating commands, running benchmarks, and visualizing logs, and it supports various options for customizing the benchmarking process, such as selecting specific models, devices, and data types.",
            "total_tokens": 12057,
            "latency": 3.329082,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "4596cafa-a23b-4e56-bd25-15595694f2bf"
        },
        "gpt-4.1": {
            "file_summary": "The file runner.py is a comprehensive automation and analysis tool for benchmarking PyTorch model performance and accuracy across a variety of suites, devices, data types, and compiler backends. It generates command-line scripts to run detailed benchmarks, parses and aggregates performance/accuracy results, visualizes key metrics (e.g., speedup, memory, compilation latency), and produces both CSV summaries and annotated graphs. Furthermore, it can detect regressions, compare recent runs, package results for dashboards (including uploading images and posting to GitHub), and provides extensive functionality for summarizing, flagging, and tracking benchmark outcomes over time.",
            "total_tokens": 12172,
            "latency": 2.501993,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "a6d4d5b1-4857-4679-b99d-df0e9ad9eca6"
        },
        "gemini-2.5-flash": {
            "file_name": "/benchmarks/dynamo/runner.py",
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "total_tokens": 15394,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.500387,
            "file_summary": "This Python script acts as a wrapper around a PyTorch benchmarking infrastructure. It generates, runs, and parses benchmark commands for various compilers, model suites, devices, and data types for both training and inference modes. The script produces detailed performance and accuracy reports, including executive summaries, warnings, and historical comparisons, with options to visualize results and update a performance dashboard.",
            "uuid": "1170b19c-8c8a-4848-ae7c-c92751e6a952"
        },
        "gemini-2.5": {
            "file_name": "/benchmarks/dynamo/runner.py",
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "total_tokens": 15584,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.317932,
            "file_summary": "This Python script serves as a wrapper for running and analyzing PyTorch benchmarks across various configurations (suites, devices, dtypes, compilers). It can generate benchmark execution commands, run them, parse the resulting log files, and visualize performance metrics like speedup and compilation time as tables and graphs. The script also includes functionality to update a performance dashboard, track metrics over time, and detect regressions compared to previous runs.",
            "uuid": "f3750406-c174-477a-9beb-89910ca8bafc"
        },
        "Claude3.7": {
            "file_name": "/benchmarks/dynamo/runner.py",
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "total_tokens": 15947,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 7.963751,
            "file_summary": "The main functionality of \"runner.py\" is to serve as a wrapper over benchmark infrastructure for PyTorch's dynamic compilation systems (like TorchDynamo), allowing users to generate benchmark commands, run benchmarks for various compilers and models, parse results, and visualize performance metrics through CSV files and graphs. The script supports both inference and training benchmarks across different model suites (torchbench, huggingface, timm_models) and can automatically generate performance reports with metrics like speedup, accuracy, compilation latency, and memory usage, with optional dashboard integration for tracking performance over time.",
            "uuid": "2a009d7a-ccb1-4645-928a-f9e9879e0a30"
        },
        "mistral-small": {
            "file_name": "/benchmarks/dynamo/runner.py",
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "total_tokens": 19589,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 50.910331,
            "file_summary": " This Python script is a wrapper over the benchmark infrastructure to generate commonly used commands, parse results, and generate CSV/graphs. The script works on manually written TABLE (see below). We can add more commands in the future.\n\nThe script works on the following arguments:\n\n* `--devices`: Specify the devices to run the benchmarks on (cpu or cuda)\n* `--dtypes`: Specify the data types to run the benchmarks on (float16/float32/amp)\n* `--suites`: Specify the benchmark suites to run the benchmarks on (huggingface/torchbench/timm\\_models)\n* `--compilers`: Specify the compilers to run the benchmarks on\n* `--flag-compilers`: Specify the compilers to flag issues\n* `--quick`: Run only one model for debugging\n* `--output-dir`: Specify the output directory to save the logs\n* `--keep-output-dir`: Do not cleanup the output directory before running\n* `--print-run-commands`: Generate commands and saves them to run.sh\n* `--visualize-logs`: Pretty print the log files and draw graphs\n* `--run`: Generate commands, run and parses the files\n* `--log-operator-inputs`: Log operator inputs\n* `--include-slowdowns`: Include slowdowns in geomean performance speedup report\n* `--extra-args`: Append commandline with these args\n* `--base-sha`: Commit id for the tested pytorch\n* `--total-partitions`: Total number of partitions, to be passed to the actual benchmark script\n* `--partition-id`: ID of partition, to be passed to the actual benchmark script\n* `--update-dashboard`: Updates to dashboard\n* `--no-graphs`: Do not genenerate and upload metric graphs\n* `--no-update-archive`: Do not update lookup.csv or the log archive\n* `--no-gh-comment`: Do not write a comment to github\n* `--no-detect-regressions`: Do not compare to previous runs for regressions or metric graphs\n* `--update-dashboard-test`: Does all of --no-graphs, --no-update-archive, and --no-gh-comment\n\nThe TABLE in the script contains the commands for training and inference for different compilers. We can add more commands in the future.\n\nThe script generates commands, runs them, and parses the results. It also generates CSV files and graphs for the results. The results are saved in the specified output directory. The script can also update the dashboard with the results.\n\nThe script uses argparse to parse the command line arguments. It imports various modules from the standard library and third-party libraries such as numpy, pandas, and matplotlib. It also imports modules from torch and torch._dynamo.\n\nThe script defines various functions to generate commands, parse results, and generate CSV files and graphs. It also defines a class to update the dashboard with the results.\n\nThe main function of the script is to parse the command line arguments and call the appropriate functions based on the arguments. If the `--run` argument is specified, the script generates commands, runs them, and parses the results. If the `--visualize-logs` argument is specified, the script generates CSV files and graphs for the results. If the `--update-dashboard` argument is specified, the script updates the dashboard with the results.\n\nThe script also defines various constants such as DEFAULT\\_OUTPUT\\_DIR, DEFAULTS, log, TABLE, INFERENCE\\_COMPILERS, TRAINING\\_COMPILERS, and DEFAULTS. These constants are used throughout the script.\n\nThe script uses various logging functions from the logging module to print messages to the console. It also uses various functions from the subprocess module to run external commands.\n\nThe script uses various functions from the os module to manipulate file paths and directories. It also uses various functions from the glob module to find files in a directory.\n\nThe script uses various functions from the functools module to define higher-order functions. It also uses various functions from the itertools module to generate iterators.\n\nThe script uses various functions from the re module to perform regular expression operations. It also uses various functions from the datetime module to manipulate dates and times.\n\nThe script uses various functions from the collections module to create dictionaries and defaultdicts. It also uses various functions from the dataclasses module to define classes with default values.\n\nThe script uses various functions from the math module to perform mathematical operations. It also uses various functions from the random module to generate random numbers.\n\nThe script uses various functions from the shutil module to copy files and directories. It also uses various functions from the tempfile module to create temporary files and directories.\n\nThe script uses various functions from the io module to read and write data to files. It also uses various functions from the sys module to access system-specific information.\n\nThe script uses various functions from the argparse module to parse command line arguments. It also uses various functions from the rcParams module to set matplotlib parameters.\n\nThe script uses various functions from the torch module to perform various operations related to PyTorch. It also uses various functions from the torch._dynamo module to perform various operations related to TorchDynamo.\n\nThe script uses various functions from the torch.nn module to define neural network models. It also uses various functions from the torch.optim module to define optimizers.\n\nThe script uses various functions from the torch.backends.cudnn module to perform various operations related to CUDA. It also uses various functions from the torch.backends.cudnn.benchmark module to perform various benchmarking operations.\n\nThe script uses various functions from the torch.backends.xeon module to perform various operations related to Xeon CPUs. It also uses various functions from the torch.backends.xeon.run\\_cpu module to run benchmarks on Xeon CPUs.\n\nThe script uses various functions from the torch.nn.functional module to perform various functional operations on neural networks. It also uses various functions from the torch.nn.init module to initialize neural network weights.\n\nThe script uses various functions from the torch.utils.data module to load and preprocess data. It also uses various functions from the torch.utils.tensorboard module to log data to TensorBoard.\n\nThe script uses various functions from the torchvision module to perform various operations related to computer vision. It also uses various functions from the torchvision.transforms module to transform images and videos.\n\nThe script uses various functions from the torchvision.datasets module to load various datasets. It also uses various functions from the torchvision.models module to load various pre-trained models.\n\nThe script uses various functions from the torchvision.models.detection module to load various object detection models. It also uses various functions from the torchvision.models.segmentation module to load various semantic segmentation models.\n\nThe script uses various functions from the torchvision.models.feature\\_extraction module to load various feature extraction models. It also uses various functions from the torchvision.models.video.action\\_recognition module to load various action recognition models.\n\nThe script uses various functions from the torchvision.models.vision\\_transformer module to load various vision transformer models. It also uses various functions from the torchvision.models.resnet module to load various ResNet models.\n\nThe script uses various functions from the torchvision.models.efficientnet module to load various EfficientNet models. It also uses various functions from the torchvision.models.mobilenet module to load various MobileNet models.\n\nThe script uses various functions from the torchvision.models.squeezenet module to load various SqueezeNet models. It also uses various functions from the torchvision.models.vgg module to load various VGG models.\n\nThe script uses various functions from the torchvision.models.inception module to load various Inception models. It also uses various functions from the torchvision.models.densenet module to load various DenseNet models.\n\nThe script uses various functions from the torchvision.models.wide\\_resnet module to load various Wide ResNet models. It also uses various functions from the torchvision.models.shufflenet module to load various ShuffleNet models.\n\nThe script uses various functions from the torchvision.models.googlenet module to load various GoogLeNet models. It also uses various functions from the torchvision.models.alexnet module to load various AlexNet models.\n\nThe script uses various functions from the torchvision.models.mnasnet module to load various MNASNet models. It also uses various functions from the torchvision.models.mobilenetv2 module to load various MobileNetV2 models.\n\nThe script uses various functions from the torchvision.models.resnext module to load various ResNeXt models. It also uses various functions from the torchvision.models.regnet module to load various RegNet models.\n\nThe script uses various functions from the torchvision.models.seresnext module to load various SEResNeXt models. It also uses various functions from the torchvision.models.senet module to load various SENet models.\n\nThe script uses various functions from the torchvision.models.efficientnetv2 module to load various EfficientNetV2 models. It also uses various functions from the torchvision.models.convnext module to load various ConvNeXt models.\n\nThe script uses various functions from the torchvision.models.cspnet module to load various CSPNet models. It also uses various functions from the torchvision.models.mixnet module to load various MixNet models.\n\nThe script uses various functions from the torchvision.models.ghostnet module to load various GhostNet models. It also uses various functions from the torchvision.models.repvgg module to load various RepVGG models.\n\nThe script uses various functions from the torchvision.models.swin\\_transformer module to load various Swin Transformer models. It also uses various functions from the torchvision.models.vit module to load various Vision Transformer models.\n\nThe script uses various functions from the torchvision.models.deit module to load various DeiT models. It also uses various functions from the torchvision.models.tnt module to load various TNT models.\n\nThe script uses various functions from the torchvision.models.nest module to load various Nest models. It also uses various functions from the torchvision.models.cait module to load various CaiT models.\n\nThe script uses various functions from the torchvision.models.cfnet module to load various CFNet models. It also uses various functions from the torchvision.models.dla module to load various DLA models.\n\nThe script uses various functions from the torchvision.models.dla\\_v2 module to load various DLA v2 models. It also uses various functions from the torchvision.models.dla\\_xlarge module to load various DLA XLarge models.\n\nThe script uses various functions from the torchvision.models.dla\\_xxlarge module to load various DLA XXLarge models. It also uses various functions from the torchvision.models.dla\\_tiny module to load various DLA Tiny models.\n\nThe script uses various functions from the torchvision.models.dla\\_small module to load various DLA Small models. It also uses various functions from the torchvision.models.dla\\_medium module to load various DLA Medium models.\n\nThe script uses various functions from the torchvision.models.dla\\_large module to load various DLA Large models. It also uses various functions from the torchvision.models.dla\\_xlarge\\_3x3 module to load various DLA XLarge 3x3 models.\n\nThe script uses various functions from the torchvision.models.dla\\_xxlarge\\_3x3 module to load various DLA XXLarge 3x3 models. It also uses various functions from the torchvision.models.dla\\_tiny\\_3x3 module to load various DLA Tiny 3x3 models.\n\nThe script uses various functions from the torchvision.models.dla\\_small\\_3x3 module to load various DLA Small 3x3 models. It also uses various functions from the torchvision.models.dla\\_medium\\_3x3 module to load various DLA Medium 3x3 models.\n\nThe script uses various functions from the torchvision.models.dla\\_large\\_3x3 module to load various DLA Large 3x3 models. It also uses various functions from the torchvision.models.dla\\_xlarge\\_7x7 module to load various DLA XLarge 7x7 models.\n\nThe script uses various functions from the torchvision.models.dla\\_xxlarge\\_7x7 module to load various DLA XXLarge 7x7 models. It also uses various functions from the torchvision.models.dla\\_tiny\\_7x7 module to load various DLA Tiny 7x7 models.\n\nThe script uses various functions from the torchvision.models.dla\\_small\\_7x7 module to load various DLA Small 7x7 models. It also uses various functions from the torchvision.models.dla\\_medium\\_7x7 module to load various DLA Medium 7x7 models.\n\nThe script uses various functions from the torchvision.models.dla\\_large\\_7x7 module to load various DLA Large 7x7 models. It also uses various functions from the torchvision.models.dla\\_xlarge\\_5x5 module to load various D",
            "uuid": "08019658-c5cf-4e43-882d-ce793eea8398"
        },
        "mistral-7b": {
            "file_name": "/benchmarks/dynamo/runner.py",
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "total_tokens": 16643,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 4.310613,
            "file_summary": " The main functionality of the given Python file is to provide a script for running and analyzing benchmarks using PyTorch Dynamo. The script supports both training and inference modes, and it generates commands for different compilers, runs them, and parses the results. The results are then used to generate CSV files, graphs, and comments for GitHub issues. The script also supports options for generating only the commands, visualizing logs, or updating the GitHub dashboard. The file also includes functions for handling command-line arguments, generating dropdown comments, and archiving logs.",
            "uuid": "e4ab97be-b60a-43ee-96f4-8ad74c6ae81e"
        },
        "cohere-v1": {
            "file_name": "/benchmarks/dynamo/runner.py",
            "number_of_lines": 1546,
            "project_name": "pytorch",
            "file_summary": "The given Python script, runner.py, is a wrapper script that facilitates benchmarking and performance analysis for different suites, devices, data types, and compilers. It generates commands for running benchmarks, parses the results, and generates visual summaries in the form of CSV files and graphs. Additionally, it archives the results and provides functionality to detect and comment on regressions on a GitHub issue.",
            "total_tokens": 14040,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 2.863896,
            "uuid": "bfc8e82f-51ab-41d7-a34d-9724e1e5d850"
        }
    },
    "/benchmarks/transformer/sdpa.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"sdpa.py\" file is to benchmark the performance of scaled dot-product attention (SDPA) operations using various configurations, measure the execution time and TFLOPS for both forward and backward passes, and then output the results in a tabular format and save them to a CSV file. It utilizes PyTorch for tensor operations and benchmarking, and the script is designed to run a series of experiments with different parameters to evaluate the performance of SDPA under various conditions.",
            "total_tokens": 2696,
            "latency": 1.18355,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "ebbc1b1f-45f8-4b4c-8cf0-c19112e6e309"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"sdpa.py\" is to benchmark the performance of scaled dot-product attention in PyTorch. It uses the `benchmark_torch_function_in_microseconds` function to measure the execution time of the attention function and the `calculate_tflops` function to calculate the TFLOPS for both forward and backward passes. The file also includes functions to generate experiment configurations, get input tensors, and print and write the results to a CSV file. The benchmarking is performed for different batch sizes, sequence lengths, embedding dimensions, and backends.",
            "total_tokens": 127,
            "latency": 3.098488,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0779f834-c910-4bef-b6b0-d42c09e33f21"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python script benchmarks the performance of the scaled dot product attention function in PyTorch using various configurations (such as batch size, sequence lengths, embedding dimensions, causality settings, data types, and backends). It measures the forward and backward execution times, calculates throughput in TFLOPS, and outputs the results into a readable table and CSV file. It includes functionality for run-time measurement, throughput calculation considering sparsity, and result aggregation.",
            "total_tokens": 2308,
            "latency": 4.879484,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "1bcd7120-72fe-48c7-8fc5-c4aa37f7adef"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file \"sdpa.py\" defines functions and data structures for benchmarking scaled dot product attention with PyTorch. It includes functions to calculate TFLOPS, run experiments with different configurations, and output the results in a readable format. The main functionality of the file is to generate various experiment configurations, run the experiments, display the results using tabulate, and write the results to a CSV file for further analysis.",
            "total_tokens": 2276,
            "latency": 1.297918,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "7bc58e14-6a7f-40ef-afe5-e470985bee59"
        },
        "Claude3.5": {
            "file_summary": "This Python file implements a benchmarking system for scaled dot product attention (SDPA) operations in PyTorch. It defines functions to measure the performance of SDPA in terms of execution time and TFLOPS for both forward and backward passes, considering various configurations such as batch size, sequence length, and data types. The script generates multiple experiment configurations, runs the benchmarks, and outputs the results in both tabular format and CSV files for further analysis.",
            "total_tokens": 2906,
            "latency": 3.299277,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "cabe62c6-2132-4a03-9ef1-ceeade847637"
        },
        "nova-pro-v1": {
            "file_summary": "The `sdpa.py` file is designed to benchmark the performance of scaled dot-product attention (SDPA) in PyTorch, measuring both forward and backward pass times and calculating TFLOPS. It generates various experiment configurations, runs each experiment, and outputs the results in a formatted table and a CSV file.",
            "total_tokens": 2660,
            "latency": 1.045892,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "8eab4f22-ca16-4016-ba76-fec6510b9523"
        },
        "gpt-4o": {
            "file_summary": "The `sdpa.py` file benchmarks the performance of scaled dot-product attention (SDPA) operations in PyTorch across various configurations, including batch size, sequence length, embedding dimensions, and more. It calculates execution times, throughput in TFLOPS, and runs experiments for both forward and backward passes with optional sparsity. Results are displayed in tabular format and optionally written to a CSV file for analysis.",
            "total_tokens": 2301,
            "latency": 1.584655,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "90209ce3-315d-42e6-83a4-bd4a76675b0d"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to benchmark the performance of the scaled dot product attention (SDPA) kernel in PyTorch, specifically on CUDA devices. It generates various experiment configurations, runs each experiment, and measures the execution time and TFLOPS (tera floating-point operations per second) for both forward and backward passes. The results are then printed in a tabular format and written to a CSV file.",
            "total_tokens": 2280,
            "latency": 3.75565,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "8d373ca8-cfd8-4f90-88b3-12d3d20e9a6c"
        },
        "gpt-4": {
            "file_summary": "The Python script defined in 'sdpa.py' is centered around benchmarking scaled dot product attention (SDPA) computational functions, particularly emphasizing GPU execution and measuring performance in terms of throughput (TFLOPS). It supports different configurations for experiments, such as variations on batch size, sequence length, embedding dimensions, and backend processing (e.g., CUDA or inductor). Various functions to aid the process include gathering input data, running single experiments, calculating TFLOPS, and outputting results either as formatted tables or as CSV files, making it a comprehensive toolkit for performance evaluation of attention mechanisms in deep learning frameworks.",
            "total_tokens": 2313,
            "latency": 4.054575,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "db43ea07-077b-4587-b1f0-44c4212128f8"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to benchmark the performance of the scaled dot product attention (SDPA) mechanism in PyTorch, specifically measuring the execution time and TFLOPS for forward and backward passes under various experiment configurations. The script generates a range of experiment configurations with different batch sizes, sequence lengths, and data types, and then runs each experiment to collect results, which are printed in a table and written to a CSV file. The benchmarking process utilizes PyTorch's built-in benchmarking tools and CUDA functionality to measure execution times and calculate TFLOPS.",
            "total_tokens": 2314,
            "latency": 1.951095,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "8ede7392-255a-4b7c-bdfc-769a167d9d3d"
        },
        "gpt-4.1": {
            "file_summary": "The main functionality of sdpa.py is to benchmark the performance (execution time and TFLOPS) of PyTorch's scaled dot product attention (SDPA) operations across various input configurations, using CUDA on GPU. It systematically generates a range of input parameter settings, runs and times both forward and backward SDPA passes, calculates associated TFLOPS, and then prints and saves the results in a CSV file. The code supports consistent, automated performance evaluation of different SDPA backends and settings for large-scale transformer-like workloads.",
            "total_tokens": 2324,
            "latency": 1.59672,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 296,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "0e55119f-b275-46e7-81d0-2af132b226b5"
        },
        "gemini-2.5-flash": {
            "file_name": "/benchmarks/transformer/sdpa.py",
            "number_of_lines": 296,
            "project_name": "pytorch",
            "total_tokens": 3341,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.236465,
            "file_summary": "This Python script is designed to benchmark the performance of PyTorch's `scaled_dot_product_attention` function. It runs experiments across various configurations, including different batch sizes, sequence lengths, data types, and causality settings. For each configuration, it measures the forward and backward pass execution times on CUDA, calculates TFLOPS, and reports the results in a table and saves them to a CSV file.",
            "uuid": "f2414edf-b3af-4cfd-b99f-ad072562c02a"
        },
        "gemini-2.5": {
            "file_name": "/benchmarks/transformer/sdpa.py",
            "number_of_lines": 296,
            "project_name": "pytorch",
            "total_tokens": 3239,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.661991,
            "file_summary": "This Python script benchmarks the performance of PyTorch's `scaled_dot_product_attention` function under various configurations (like batch size, sequence length, dtype, causality). It measures the forward and backward pass execution times in microseconds and calculates the corresponding TFLOPS for each configuration using specified or default SDPA backends. Finally, the script prints the collected benchmark results in a formatted table and saves them to a timestamped CSV file.",
            "uuid": "3db0a2aa-00fe-4eac-9fb1-3d143fb012bd"
        },
        "Claude3.7": {
            "file_name": "/benchmarks/transformer/sdpa.py",
            "number_of_lines": 296,
            "project_name": "pytorch",
            "total_tokens": 2915,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.789777,
            "file_summary": "The file \"sdpa.py\" is a benchmarking tool for measuring the performance of PyTorch's scaled dot product attention (SDPA) operations, calculating both execution time and computational throughput (TFLOPS) for forward and backward passes across various configurations. It defines a comprehensive framework for running experiments with different batch sizes, sequence lengths, attention mechanisms (causal vs non-causal), and data types, then outputs the results in tabular format and saves them to CSV files for analysis.",
            "uuid": "a2a22bb9-f388-4501-a33c-73838eafb99d"
        },
        "mistral-small": {
            "file_name": "/benchmarks/transformer/sdpa.py",
            "number_of_lines": 296,
            "project_name": "pytorch",
            "total_tokens": 3068,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.158508,
            "file_summary": " The main functionality of the file \"sdpa.py\" is to benchmark and analyze the performance of the Scaled Dot Product Attention (SDPA) kernel in PyTorch. It generates various experiment configurations with different parameters such as batch size, number of heads, sequence lengths, embedding dimensions, data types, and SDP backends. For each configuration, it runs the SDPA kernel, measures the execution time, and calculates the throughput in TFLOPS. The results are then printed and written to a CSV file for further analysis.",
            "uuid": "2fdf6914-6988-49b5-a069-70c5f453b325"
        },
        "mistral-7b": {
            "file_name": "/benchmarks/transformer/sdpa.py",
            "number_of_lines": 296,
            "project_name": "pytorch",
            "total_tokens": 3078,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.660322,
            "file_summary": " This Python file is for benchmarking the scaled dot product attention (SDPA) function in PyTorch using the SDPA kernel from the TorchINDUCTOR library. The main functionality includes defining helper functions for benchmarking PyTorch functions, creating experiment configurations, running single experiments, and printing or writing the results to a CSV file. The script generates various experiment configurations based on batch sizes, number of heads, sequence lengths, embedding dimensions, data types, and causality, and then runs each experiment to measure the forward and backward pass times and calculate TFLOPS.",
            "uuid": "a6bb63a1-dd31-4b39-be8c-bd3afa1ecc4e"
        },
        "cohere-v1": {
            "file_name": "/benchmarks/transformer/sdpa.py",
            "number_of_lines": 296,
            "project_name": "pytorch",
            "file_summary": "The Python script 'sdpa.py' is focused on benchmarking the performance of scaled dot product attention, a key component in transformer models. It runs experiments with various configurations, measuring forward and backward pass times and TFLOPS, and presents the results in a tabular format. The script also has the ability to write these results to a CSV file for further analysis.",
            "total_tokens": 2584,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.239675,
            "uuid": "b87c0d98-9082-49e6-8b6c-8bf19fbe08dd"
        }
    },
    "/tools/code_coverage/package/oss/init.py": {
        "nova-lite-v1": {
            "file_summary": "The `init.py` file is designed to initialize and configure a testing environment for both C++ and Python tests. It includes functionalities to parse command-line arguments, create necessary directories, clean up previous test results, and compile a list of tests to run based on user input or default settings. The script also handles specific configurations for different compilers and ensures the environment is properly set up before running the tests.",
            "total_tokens": 1662,
            "latency": 0.702622,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "e4c0666a-6b2b-42a8-a9b0-68852f3f24d1"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to initialize the testing environment for PyTorch. It creates necessary folders, adds command-line arguments, and retrieves the list of tests to run. The file also handles the case when no tests are found and provides information about the testing environment, such as the PyTorch folder, test binaries folder, and compiler type. Additionally, it removes any leftover files from previous runs and exports the necessary environment variables for GCC.",
            "total_tokens": 95,
            "latency": 2.458945,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "e8a3ba65-6e57-4e67-a5ab-70d6601eb611"
        },
        "gpt-4.5": {
            "file_summary": "This Python script initializes and prepares a testing environment for PyTorch by parsing command-line arguments, determining tests (C++ and Python) to run, handling blocked tests based on the compiler type (GCC or Clang), and setting up necessary folders and files. It supports selective test execution through specified options, cleans previous test data if required, and logs essential initialization information. It also manages specific compiler behaviors and restricted test lists to avoid issues with intermediate file processing.",
            "total_tokens": 1384,
            "latency": 4.883882,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8da01908-3c0b-4097-8d0a-e5b5025b05c9"
        },
        "gpt-3.5-turbo": {
            "file_summary": "This Python file initializes test settings and options for running specific tests in a given directory. It defines functions for setting up test environment, parsing command-line arguments, selecting and organizing test cases, and displaying initialization information. Additionally, it includes logic to handle different test types (CPP and PY), compiler types (GCC and clang), and specific test exclusions based on compiler and test file names.",
            "total_tokens": 1352,
            "latency": 1.194149,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "92b3e79e-984c-4086-9891-a5e60ec74152"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of this \"init.py\" file is to initialize and set up the environment for running tests in a Python project, likely related to PyTorch. It defines functions for parsing command-line arguments, creating necessary folders, generating test lists for C++ and Python tests, and handling compiler-specific configurations. The file also includes utility functions for cleaning up, detecting compiler types, and managing test execution options.",
            "total_tokens": 1792,
            "latency": 3.852016,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "43e882a8-208f-49a6-aa72-611b85c7bd5c"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of this Python file is to initialize and configure the environment for running tests, including parsing command-line arguments, setting up necessary folders, and retrieving the list of tests to be executed based on specified criteria.",
            "total_tokens": 1623,
            "latency": 0.780838,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "9bdbb0d1-933e-4078-8f88-72b81cae45aa"
        },
        "gpt-4o": {
            "file_summary": "The provided Python file, likely part of a larger framework or testing suite, handles initialization for running tests, including creating necessary folders, handling command-line arguments, and preparing test lists based on test types (e.g., Python or C++). It also provides utility functions for cleaning up previous logs and temporary files, managing blocked tests, and detecting compiler types (like GCC or Clang). Furthermore, it prints diagnostic or configuration information related to test and compiler settings.",
            "total_tokens": 1384,
            "latency": 1.142612,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "53b8dff2-5da5-42b7-bd30-8325da51d985"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to initialize the environment and settings for running tests, including creating necessary folders, parsing command-line arguments, and determining the list of tests to run based on the specified options. It also sets up the logging and removes any existing log files from previous runs. The file appears to be part of a larger testing framework for PyTorch, with support for both C++ and Python tests.",
            "total_tokens": 1365,
            "latency": 3.414356,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "216c38f3-b703-4f42-bbff-54b3a659e287"
        },
        "gpt-4": {
            "file_summary": "The `init.py` file is primarily focused on initializing and configuring the testing environment for a software project, presumably PyTorch, based on configurations like test type, specific tests to run, and compiler settings. It utilizes Python scripts and C++ binary test lists, handles the argument parsing setup for running particular tests, and cleans up log files from previous runs. Additionally, the script adjusts configurations dynamically based on the compiler type and manages directories for storing JSON and log outputs, ensuring that the test environment is prepared correctly before execution.",
            "total_tokens": 1380,
            "latency": 4.780678,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "f493384e-d09d-4ebf-ac7a-12ba7c8e5d7f"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of this Python file, `init.py`, is to initialize the environment and parse command-line arguments for running tests, including creating necessary folders, cleaning up previous test data, and determining which tests to run based on user input. The file also sets up logging and detects the compiler type being used, such as GCC or Clang. The initialization process returns a tuple containing options, a list of tests to run, and a list of interested folders for reporting purposes.",
            "total_tokens": 1377,
            "latency": 1.676917,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "4b887444-3ab8-4b2b-9944-cd997c58b59e"
        },
        "gpt-4.1": {
            "file_summary": "This file initializes and configures a testing environment for a project that involves both C++ and Python tests, with specialized handling depending on the compiler type (GCC/Clang) and test type. It sets up command-line argument parsing to select which tests to run, manages creation and cleanup of necessary directories and files, and builds a list of tests to run based on user inputs and defaults, excluding known problematic tests where necessary. The module serves as the entry point for managing test selection, cleanup, and logging for the overall testing workflow.",
            "total_tokens": 1400,
            "latency": 1.661608,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 169,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "00d534c6-fda2-4d4f-89c2-10abfec37821"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/code_coverage/package/oss/init.py",
            "number_of_lines": 169,
            "project_name": "pytorch",
            "total_tokens": 2253,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.879911,
            "file_summary": "This Python file handles the initialization process for a testing framework, likely related to PyTorch, by parsing command-line arguments and setting up necessary directories. It identifies the specific C++ and Python tests to be executed based on user input and compiler type, including handling compiler-specific test exclusions. The script also performs initial cleanup tasks and prints relevant setup information before the tests are run.",
            "uuid": "54d01140-8346-497e-87e6-52f3a39001e0"
        },
        "gemini-2.5": {
            "file_name": "/tools/code_coverage/package/oss/init.py",
            "number_of_lines": 169,
            "project_name": "pytorch",
            "total_tokens": 2029,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.181991,
            "file_summary": "This Python script initializes a testing environment by parsing command-line arguments to configure options like which tests to run and whether to clean up previous results. It identifies C++ and Python test files within specified directories, potentially filtering them based on user input or compiler type (GCC/Clang). Finally, it prepares the environment by creating necessary folders, cleaning old artifacts, logging setup information, and returning the configuration options, the list of tests to execute, and folders of interest.",
            "uuid": "baf33a11-1193-413d-a5da-2a84d2649400"
        },
        "Claude3.7": {
            "file_name": "/tools/code_coverage/package/oss/init.py",
            "number_of_lines": 169,
            "project_name": "pytorch",
            "total_tokens": 1793,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.902564,
            "file_summary": "This file provides initialization functionality for a test coverage tool in a PyTorch project, handling command-line arguments, test discovery, and environment setup. It identifies and organizes tests (both C++ and Python) to run, manages folder creation, and configures compiler-specific settings for test coverage analysis. The file serves as the entry point for the coverage tool, preparing the environment before actual test execution and coverage collection begins.",
            "uuid": "598706a8-6071-4522-a999-c1a5a2789e99"
        },
        "mistral-small": {
            "file_name": "/tools/code_coverage/package/oss/init.py",
            "number_of_lines": 169,
            "project_name": "pytorch",
            "total_tokens": 1845,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.76967,
            "file_summary": " The main functionality of the `init.py` file is to initialize the settings and arguments for running tests in a Python project, specifically for PyTorch. It creates necessary folders, parses command-line arguments, and generates a list of tests to be run based on user input or default settings. The file also handles cleaning up previous test results and logs. Additionally, it detects the compiler type and sets up paths for LLVM tools if the compiler is Clang.",
            "uuid": "4de19ca9-b7a1-4998-ab13-d94057a5a7cc"
        },
        "mistral-7b": {
            "file_name": "/tools/code_coverage/package/oss/init.py",
            "number_of_lines": 169,
            "project_name": "pytorch",
            "total_tokens": 1840,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.199338,
            "file_summary": " This Python file, named \"init.py\", is responsible for initializing and parsing command-line arguments for running tests. It imports necessary modules and defines functions for creating folders, parsing arguments, and getting test lists. The main functionality includes creating folders if they don't exist, adding and parsing command-line arguments, cleaning up previous results, and returning the options, test list, and interested folders.",
            "uuid": "719e9faa-6100-4afb-bf30-a59387fae8cd"
        },
        "cohere-v1": {
            "file_name": "/tools/code_coverage/package/oss/init.py",
            "number_of_lines": 169,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, init.py, is responsible for initializing and configuring the testing environment. It sets up the necessary folders, parses command-line arguments, and prepares the test lists for both C++ and Python tests. Additionally, it exports some useful information about the testing environment, such as the PyTorch folder path and compiler type.",
            "total_tokens": 1596,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.220344,
            "uuid": "e9bd5913-e761-4c94-b88c-90811dbd2837"
        }
    },
    "/functorch/notebooks/_src/plot_ensembling.py": {
        "nova-lite-v1": {
            "file_summary": "The file \"plot_ensembling.py\" demonstrates how to implement model ensembling using vectorization in PyTorch. It explains the concept of model ensembling, which combines predictions from multiple models, and shows how to use the `vmap` function to vectorize this process, thereby eliminating for-loops and speeding up computations. The example uses an ensemble of simple CNNs to illustrate the technique.",
            "total_tokens": 1594,
            "latency": 1.007763,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "458b70a7-72a8-4d03-ad0d-3783f019a8d6"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"plot_ensembling.py\" is to demonstrate how to use vmap for vectorizing model ensembling in PyTorch. The file defines a simple CNN model and generates dummy data. It then creates an ensemble of 10 models and uses vmap to speed up the process of generating predictions for different minibatches of data. The file also includes a note about the limitations of using vmap for vectorizing functions with side effects.",
            "total_tokens": 100,
            "latency": 2.430006,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "b277ee94-81e3-45b1-8225-cb2c776a486c"
        },
        "gpt-4.5": {
            "file_summary": "This Python script demonstrates how to efficiently perform model ensembling for multiple neural networks using PyTorch and vectorization techniques provided by the `vmap` function from Functorch. It defines an ensemble of CNN models, combines their parameters and states into a unified batch representation, then vectorizes computations to simultaneously perform inference either with different data batches per model or a common batch. The main goal is eliminating traditional for-loops, thus speeding up computations through efficient tensor operations.",
            "total_tokens": 1386,
            "latency": 6.552079,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "00c4c243-8a11-496e-be2b-0310afb60b55"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the 'plot_ensembling.py' file is to demonstrate how to perform model ensembling using vectorization techniques in PyTorch, specifically utilizing the 'vmap' function transform to map functions across dimensions of input tensors. The file includes the definition of a simple CNN model, generation of dummy data, creation and preparation of multiple models for ensembling, showcasing different options for generating predictions, and using 'vmap' to speed up the for-loop process by combining model states and parameters for efficient prediction computation.",
            "total_tokens": 1392,
            "latency": 1.127568,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "09c3fa42-853a-47e8-8e8c-81b692ae3159"
        },
        "Claude3.5": {
            "file_summary": "This Python file demonstrates the use of model ensembling and how to vectorize it using PyTorch's vmap function. It showcases two approaches for generating predictions with multiple models: using different minibatches for each model and using the same minibatch across all models. The file also illustrates how to use vmap to speed up the ensemble process by eliminating for-loops and leveraging vectorization.",
            "total_tokens": 1590,
            "latency": 2.409433,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "c89e4a4f-0e64-4314-baeb-d675291f0a55"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `plot_ensembling.py` file is to demonstrate how to use PyTorch's `vmap` function to vectorize and speed up the process of model ensembling, specifically by combining the predictions from multiple simple CNNs. It showcases two options for generating predictions: using different minibatches for each model and using the same minibatch for all models.",
            "total_tokens": 1520,
            "latency": 1.115487,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "f4777af8-3ca1-46cf-84c8-2d1f584835be"
        },
        "gpt-4o": {
            "file_summary": "The file `plot_ensembling.py` demonstrates how to use the PyTorch `vmap` function to vectorize model ensembling and eliminate for-loops for combining predictions across multiple models with the same architecture. It constructs a simple CNN, generates synthetic data, presents two possible approaches for obtaining predictions (using separate or shared minibatches), and showcases the efficiency and flexibility of `vmap` in handling stacked model parameters and buffers. Additionally, it highlights the limitations of `vmap` in processing non-pure functions.",
            "total_tokens": 1399,
            "latency": 1.747806,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "dd92d819-4ec1-4e24-a1b6-1f1c05fe4677"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to demonstrate how to use the `vmap` function from the `functorch` library to vectorize model ensembling, specifically for combining the predictions of multiple CNN models. The file shows how to prepare the models for use with `vmap` by stacking their parameters and buffers, and then uses `vmap` to speed up the process of generating predictions from the ensemble. The file also compares the results of using `vmap` with the traditional for-loop approach.",
            "total_tokens": 1396,
            "latency": 4.071495,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "320b137d-0956-462b-a9cd-547bc0d80d3d"
        },
        "gpt-4": {
            "file_summary": "The Python script 'plot_ensembling.py' demonstrates how to perform model ensembling to combine predictions from multiple models, specifically utilizing the vectorization capabilities of `vmap` to expedite this process. The script includes a simple CNN model architecture and explores different approaches to feeding data batches for inference by considering both different and identical minibatches for each model. Additionally, it utilizes the `functorch` library to prepare and optimize the models for efficient ensembling through parameter stacking and vectorized computation using `vmap`.",
            "total_tokens": 1390,
            "latency": 6.66368,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "08c10929-de14-4c29-95ad-b2eac507b2e7"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `plot_ensembling.py`, is to demonstrate model ensembling using PyTorch's `vmap` function, which vectorizes the predictions from multiple models. The script defines a simple CNN model and generates dummy data to simulate an MNIST dataset, then uses `vmap` to combine the predictions from an ensemble of 10 models, showcasing two options: using different minibatches for each model and using the same minibatch for all models. The script verifies the results by comparing the predictions obtained with and without `vmap` for both options.",
            "total_tokens": 1413,
            "latency": 7.025408,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "96e1546d-ec2f-4d3b-92e4-deef78e90614"
        },
        "gpt-4.1": {
            "file_summary": "This file demonstrates how to efficiently ensemble multiple neural network models with the same architecture by leveraging PyTorch's functorch.vmap for vectorization. It illustrates how to stack model parameters across an ensemble, prepare batched data, and use vmap to simultaneously obtain predictions from all models, contrasting this approach with the traditional for-loop method. The code uses a simple CNN and dummy data to show two ensembling scenarios: each model receives a different minibatch or all models share the same minibatch.",
            "total_tokens": 1392,
            "latency": 1.4932,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 124,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "758be855-2cba-4bb0-ba13-ac45b7bf612b"
        },
        "gemini-2.5": {
            "file_name": "/functorch/notebooks/_src/plot_ensembling.py",
            "number_of_lines": 124,
            "project_name": "pytorch",
            "total_tokens": 1946,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 5.484953,
            "file_summary": "This Python script demonstrates model ensembling in PyTorch using multiple instances of a simple CNN. It primarily showcases how to use `functorch.vmap` to vectorize the process of running the ensemble on data, contrasting it with a traditional loop-based approach. The example covers applying `vmap` both when each model in the ensemble processes a different data batch and when all models process the same batch.",
            "uuid": "69ce714a-fcd4-4ec7-8160-ba2310def6b3"
        },
        "gemini-2.5-flash": {
            "file_name": "/functorch/notebooks/_src/plot_ensembling.py",
            "number_of_lines": 124,
            "project_name": "pytorch",
            "total_tokens": 1647,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 1.467103,
            "file_summary": "This Python script demonstrates model ensembling using PyTorch. It shows traditional methods for combining predictions from multiple models, either with different or the same input minibatches. The core functionality is illustrating how to vectorize these ensembling operations efficiently using `functorch.vmap` and `functorch.combine_state_for_ensemble`.",
            "uuid": "11add5a7-7a53-450b-9090-c4f358ff15e6"
        },
        "Claude3.7": {
            "file_name": "/functorch/notebooks/_src/plot_ensembling.py",
            "number_of_lines": 124,
            "project_name": "pytorch",
            "total_tokens": 1610,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.854244,
            "file_summary": "This file demonstrates how to use PyTorch's vmap functionality to vectorize model ensembling, which combines predictions from multiple models. It shows two approaches for generating predictions with an ensemble of CNN models: using different minibatches for each model or using the same minibatch across all models, and then demonstrates how to speed up these approaches by using vmap to eliminate for-loops through vectorization. The example includes code for combining model states and comparing the vectorized implementation against the traditional loop-based approach.",
            "uuid": "e3410a66-fc6d-4d3e-9b99-19d9cae4934d"
        },
        "mistral-small": {
            "file_name": "/functorch/notebooks/_src/plot_ensembling.py",
            "number_of_lines": 124,
            "project_name": "pytorch",
            "total_tokens": 1714,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.164988,
            "file_summary": " The main functionality of the file \"plot_ensembling.py\" is to demonstrate model ensembling using PyTorch and the `vmap` function from the `functorch` library. It defines a simple Convolutional Neural Network (CNN) architecture, generates dummy data, and then combines the predictions from multiple instances of the same model using `vmap` for vectorization. The file illustrates how to use `vmap` to speed up the process of running multiple models on the same data and combining their predictions.",
            "uuid": "9db9b4d7-a077-48fa-9e90-49eeee985c20"
        },
        "mistral-7b": {
            "file_name": "/functorch/notebooks/_src/plot_ensembling.py",
            "number_of_lines": 124,
            "project_name": "pytorch",
            "total_tokens": 1692,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.117956,
            "file_summary": " The main functionality of the file is to demonstrate how to use model ensembling and vectorization with PyTorch's vmap function. The script creates an ensemble of SimpleCNN models, generates some dummy data, and shows how to obtain predictions using both a for-loop and vectorization with vmap. The vmap function is used to speed up the ensemble prediction process by eliminating the need for a for-loop.",
            "uuid": "92dab313-c234-4ebd-98ab-ec57ae797d4d"
        },
        "cohere-v1": {
            "file_name": "/functorch/notebooks/_src/plot_ensembling.py",
            "number_of_lines": 124,
            "project_name": "pytorch",
            "file_summary": ": The code demonstrates model ensembling using PyTorch, combining predictions from multiple instances of a simple CNN model. It showcases two methods for generating predictions: using different minibatches for each model and applying the same minibatch to all models. The code also explains how to utilize the vmap function from the functorch library to vectorize and optimize the ensembling process.",
            "total_tokens": 1530,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.184995,
            "uuid": "477f536b-4143-4848-a88c-87e2a689d08e"
        }
    },
    "/torch/_functorch/_activation_checkpointing/knapsack.py": {
        "nova-lite-v1": {
            "file_summary": "The `knapsack.py` file implements three different algorithms for solving the knapsack problem: a greedy approach, an integer linear programming (ILP) approach using `scipy`, and a dynamic programming (DP) approach using `torch`. Each function takes a list of memory usages, runtimes, and a maximum memory budget, and returns the total runtime, a list of items to save, and a list of items to allow recomputing. The greedy approach sorts items by their runtime-to-memory ratio and selects items until the memory budget is reached. The ILP approach uses `scipy` to solve the problem as an optimization problem. The DP approach quantizes the memory usages and runtimes and uses a DP table to find the optimal solution.",
            "total_tokens": 1467,
            "latency": 1.473946,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "9ef90433-67b9-42bb-b2e9-c3bf0d8de92d"
        },
        "titan_premier": {
            "file_summary": "The Python file \"knapsack.py\" contains three functions: \"greedy_knapsack\", \"ilp_knapsack\", and \"dp_knapsack\". These functions are designed to solve the 0-1 Knapsack problem, which involves selecting a subset of items with the maximum total value while not exceeding a given weight limit. The \"greedy_knapsack\" function uses a greedy approach, the \"ilp_knapsack\" function employs an integer linear programming (ILP) method, and the \"dp_knapsack\" function implements a dynamic programming (DP) solution. The functions take in lists of memory and runtime values for each item, along with a maximum memory constraint, and return the maximum achievable runtime, the indices of the items to save, and the indices of the items to allow recomputing.",
            "total_tokens": 176,
            "latency": 3.830104,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a6016ef5-a22a-40fa-9969-009187b43575"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements three different knapsack algorithm variants\u2014greedy, integer linear programming (ILP), and dynamic programming (DP)\u2014to optimally select a subset of items based on their memory usage and runtimes, such that the total runtime is maximized given a memory usage constraint. Each function returns the selected items to save in memory and those that need recomputing, along with the achieved maximum runtime. While the greedy method offers fast but approximate solutions, the ILP and DP methods produce optimal solutions at the cost of higher computational complexity.",
            "total_tokens": 1218,
            "latency": 5.049594,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "f95fa0e1-356c-4703-8917-13773eb9557e"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided Python file defines three knapsack algorithms: greedy_knapsack, ilp_knapsack, and dp_knapsack. The greedy_knapsack function implements a greedy approach to solving the knapsack problem based on runtime to memory ratio. The ilp_knapsack function uses Integer Linear Programming (ILP) with scipy to solve the problem, and the dp_knapsack function implements a dynamic programming solution for the knapsack problem.",
            "total_tokens": 1207,
            "latency": 1.401182,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "c3336334-4895-4b4c-820e-48cba0ade0af"
        },
        "Claude3.5": {
            "file_summary": "The file \"knapsack.py\" implements three different algorithms (greedy, integer linear programming, and dynamic programming) to solve the knapsack problem, specifically for optimizing memory usage and runtime in a computational context. Each function takes memory requirements, runtimes, and a maximum memory constraint as inputs, and returns the total runtime, items to save, and items to allow recomputing, providing different trade-offs between solution quality and computational complexity.",
            "total_tokens": 1489,
            "latency": 2.990381,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "685b7a1b-4aff-4c85-9aa7-ac3a0482063e"
        },
        "nova-pro-v1": {
            "file_summary": "The `knapsack.py` file implements three different algorithms to solve the knapsack problem with given memory constraints: a greedy approach, an integer linear programming (ILP) approach, and a dynamic programming (DP) approach. Each function returns the maximum achievable runtime and lists of items to save and allow recomputing.",
            "total_tokens": 1376,
            "latency": 0.99918,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "deb9bc6c-37e5-498f-9303-a8bbf585eafc"
        },
        "gpt-4o": {
            "file_summary": "The `knapsack.py` file implements three different algorithms \u2014 greedy, integer linear programming (ILP), and dynamic programming (DP) \u2014 to solve a variation of the 0-1 Knapsack problem. Each function determines an optimal selection of items based on the runtime-to-memory trade-off, ensuring that the total memory used does not exceed a given maximum memory limit. The algorithms output the maximum achievable runtime, the indices of items chosen to save (keep in memory), and the indices of items marked for recomputation.",
            "total_tokens": 1211,
            "latency": 1.909762,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "e82ba5af-a1a1-44b8-9147-100d8d0ffc6f"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to provide three different algorithms (Greedy, Integer Linear Programming (ILP), and Dynamic Programming (DP)) to solve the 0-1 Knapsack problem, which is a classic problem in combinatorial optimization. The goal is to select a subset of items with maximum total runtime that can fit within a given memory constraint. The algorithms return the maximum runtime, the items to save, and the items to allow recomputing.",
            "total_tokens": 1209,
            "latency": 3.843599,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "363bfb8e-cfea-4ada-a8d7-cc7c3098fd66"
        },
        "gpt-4": {
            "file_summary": "The file `knapsack.py` implements three different methods (`greedy_knapsack`, `ilp_knapsack`, and `dp_knapsack`) to solve the knapsack problem, specifically tailored towards optimizing the selection of computational tasks based on memory usage and runtime. The greedy approach sorts items by the ratio of runtime to memory and selects items until the memory limit is reached, ILP (Integer Linear Programming) approach uses SCIPY's MILP solver with memory constraints, and the DP (Dynamic Programming) approach utilizes a quantized pseudopolynomial DP algorithm to handle floating-point memory requirements. Each method returns the total runtime, list of tasks to save (execute), and list to allow recomputing (not execute) within a given memory constraint.",
            "total_tokens": 1265,
            "latency": 6.316073,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "aed7c26a-b034-4ba6-91c6-6dd15eb9ba42"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `knapsack.py`, is to solve the 0/1 knapsack problem using three different algorithms: a greedy approach, integer linear programming (ILP), and dynamic programming (DP). These algorithms aim to maximize the total runtime of items that can be saved within a given memory constraint. The file provides three functions, `greedy_knapsack`, `ilp_knapsack`, and `dp_knapsack`, each implementing one of these approaches to solve the problem.",
            "total_tokens": 1224,
            "latency": 1.56838,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "8ad4386f-3a04-43e9-a4b3-44c46e263139"
        },
        "gpt-4.1": {
            "file_summary": "This file implements three approaches (greedy, integer linear programming, and dynamic programming) to solve a 0-1 knapsack problem where each item has an associated memory cost and runtime value. Each function selects a subset of items to maximize total runtime without exceeding a given memory budget, and returns both the achieved runtime and the indices of items chosen to save versus those to recompute. The code uses Torch for the dynamic programming implementation and optionally Scipy for the ILP method.",
            "total_tokens": 1203,
            "latency": 1.490205,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 122,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "be359b31-9c0d-42e2-bf0a-146fd1ac2c3f"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_functorch/_activation_checkpointing/knapsack.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "total_tokens": 1920,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.727352,
            "file_summary": "This Python file provides three different algorithms (greedy, Integer Linear Programming, and dynamic programming) to solve a 0-1 knapsack problem. The problem involves selecting items, each with a memory cost and an associated runtime benefit, to maximize the total runtime saved while staying within a maximum memory budget. The functions return the maximum achievable runtime and lists indicating which items should be saved and which should be recomputed.",
            "uuid": "85a6eb87-f1c4-42a0-95fb-7fe1112a5e86"
        },
        "gemini-2.5": {
            "file_name": "/torch/_functorch/_activation_checkpointing/knapsack.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "total_tokens": 2017,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.274803,
            "file_summary": "This Python file provides three different algorithms (greedy, integer linear programming, and dynamic programming) for solving the 0/1 knapsack problem. Given lists representing memory costs and runtimes for a set of items, along with a maximum memory budget, the functions aim to select a subset of items that maximizes the total runtime without exceeding the memory limit. Each function returns the maximum achievable runtime, the indices of the selected items (\"saved\"), and the indices of the items not selected (\"recomputed\").",
            "uuid": "35849d3f-6d3e-4b7a-88ab-4fdb5d3414eb"
        },
        "Claude3.7": {
            "file_name": "/torch/_functorch/_activation_checkpointing/knapsack.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "total_tokens": 1486,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.193997,
            "file_summary": "This file implements three different algorithms (greedy, integer linear programming, and dynamic programming) for solving the knapsack problem, specifically in the context of memory optimization. Each function takes memory requirements, runtimes, and a maximum memory constraint as inputs, and returns the total runtime along with lists of items to save in memory and items to allow recomputing, effectively optimizing which computational items to keep in memory versus recompute when needed.",
            "uuid": "52c364b1-43c9-40d3-9684-d99671576444"
        },
        "mistral-small": {
            "file_name": "/torch/_functorch/_activation_checkpointing/knapsack.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "total_tokens": 1537,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.883937,
            "file_summary": " The main functionality of the Python file \"knapsack.py\" is to implement various algorithms for the Knapsack problem, specifically for memory budget checkpointing in machine learning. The file provides three functions: greedy_knapsack, ilp_knapsack, and dp_knapsack, which use Greedy, Integer Linear Programming (ILP), and Dynamic Programming (DP) approaches respectively to determine the optimal set of items to save and recompute within a given memory constraint.",
            "uuid": "784c0b5a-920a-40e1-b379-bba7776cd60d"
        },
        "mistral-7b": {
            "file_name": "/torch/_functorch/_activation_checkpointing/knapsack.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "total_tokens": 1689,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 3.059847,
            "file_summary": " The given Python file, named \"knapsack.py\", contains three functions for solving the Knapsack problem with different algorithms: greedy, integer linear programming (ILP), and dynamic programming (DP). The main functionality of each function is as follows:\n\n1. `greedy_knapsack`: This function implements the greedy algorithm for the Knapsack problem, where items are selected based on the ratio of runtime to memory in descending order until the maximum memory limit is reached.\n\n2. `ilp_knapsack`: This function uses Integer Linear Programming (ILP) to find the optimal solution for the Knapsack problem. It imports the required libraries and sets up the constraints and objective function to find the items that can be saved and those that need to be recomputed.\n\n3. `dp_knapsack`: This function implements the dynamic programming (DP) algorithm for the Knapsack problem. It quantizes the memory and runtimes, initializes the DP table, and updates it iteratively to find the maximum runtime that can be achieved within the given memory constraint. The function then backtracks to find the items included in the knapsack.",
            "uuid": "30855e77-2564-41ca-89b1-b79c59c5298d"
        },
        "cohere-v1": {
            "file_name": "/torch/_functorch/_activation_checkpointing/knapsack.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The code implements three different algorithms for the knapsack problem: a greedy algorithm, an integer linear programming (ILP) approach using SciPy, and a pseudopolynomial dynamic programming (DP) algorithm using PyTorch. It finds the optimal selection of items to maximize runtime or minimize recomputation within a given memory budget.",
            "total_tokens": 1318,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.466271,
            "uuid": "0ebc813c-fcc1-4e35-85fa-d6cd23668d68"
        }
    },
    "/torch/_functorch/pytree_hacks.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"pytree_hacks.py\" file is to provide deprecated utilities `tree_map_` and `treespec_pprint` from `torch.utils._pytree`, with a warning to users to switch to the new module. This file is intended to be removed once the migration of the pytree utility is complete.",
            "total_tokens": 352,
            "latency": 2.642112,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "5424eee5-d6d1-4ba8-ba0e-2c2cdb72d598"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"pytree_hacks.py\" is to provide two functions, \"tree_map_\" and \"treespec_pprint\", which are imported from the \"torch.utils._pytree\" module. The file also includes a deprecation warning message, indicating that the \"torch._functorch.pytree_hacks\" module is deprecated and will be removed in a future release. Users are advised to use the \"torch.utils._pytree\" module instead. The file is licensed under the BSD-style license and is copyrighted by Facebook, Inc. and its affiliates.",
            "total_tokens": 136,
            "latency": 3.000304,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "47425e0f-6fe4-425b-9e6c-a8d4b7723769"
        },
        "gpt-4.5": {
            "file_summary": "This Python file acts as a compatibility layer that imports two pytree utility functions\u2014`tree_map_` and `treespec_pprint`\u2014from `torch.utils._pytree`. It raises a deprecation warning message to inform the user that importing from `torch._functorch.pytree_hacks` is deprecated. The main functionality thus ensures backward compatibility, indicating ongoing migration to the new pytree utilities in `torch.utils._pytree`.",
            "total_tokens": 358,
            "latency": 3.449756,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "488fb830-9c09-4d68-ac89-93b789514a48"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the provided Python file \"pytree_hacks.py\" is to issue a deprecation warning regarding the use of \"`torch._functorch.pytree_hacks`\" and recommend the use of \"`torch.utils._pytree`\" instead. The file also imports and exposes two functions \"`tree_map_`\" and \"`treespec_pprint`\" from \"`torch.utils._pytree`\" as part of the public interface. Additionally, there is a placeholder comment to remove the file once the migration of the pytree utility is completed.",
            "total_tokens": 372,
            "latency": 1.38617,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "dba32deb-6e16-43ad-b935-83e7f6d93b6e"
        },
        "Claude3.5": {
            "file_summary": "The file \"pytree_hacks.py\" serves as a temporary bridge for the migration of pytree utility functions. It imports and re-exports two functions (tree_map_ and treespec_pprint) from torch.utils._pytree, while also issuing a deprecation warning to users, encouraging them to use the new module directly in the future.",
            "total_tokens": 380,
            "latency": 4.036235,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "26e0549f-01ca-4ba6-b403-dbabcb823e59"
        },
        "nova-pro-v1": {
            "file_summary": "The file `pytree_hacks.py` serves as a transitional module that imports specific functions from `torch.utils._pytree` and issues a deprecation warning, signaling users to migrate to the new module before eventual removal.",
            "total_tokens": 327,
            "latency": 0.991994,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "4e95ec5f-7efc-4934-b90a-e74969b1257f"
        },
        "gpt-4o": {
            "file_summary": "This Python file acts as a temporary compatibility layer for `torch.utils._pytree`, re-exporting its `tree_map_` and `treespec_pprint` utilities while issuing a deprecation warning. It notifies users that the file is slated for removal in favor of directly using `torch.utils._pytree`. The functionality is specific to aiding migration during the transition phase for pytree utilities.",
            "total_tokens": 347,
            "latency": 1.357438,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "da1994df-602f-4d8e-a454-6287559b12a2"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide a deprecated interface to pytree utility functions, specifically `tree_map_` and `treespec_pprint`, while warning users to migrate to the new location `torch.utils._pytree` as this file will be removed in a future release.",
            "total_tokens": 323,
            "latency": 2.406513,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "798a0391-2a7c-4441-b4c2-48a97813da87"
        },
        "gpt-4": {
            "file_summary": "The Python file \"pytree_hacks.py\" primarily serves as a deprecation warning mechanism. It imports specific functions from \"`torch.utils._pytree`\" such as `tree_map_` and `tree_spec_pprint`, which seems to be utilities for handling complex data structures in PyTorch. The file advises users that \"pytree_hacks.py\" is deprecated and instructs to use \"`torch.utils._pytree`\" directly instead, signaling a migration in utility usage within PyTorch's ecosystem.",
            "total_tokens": 363,
            "latency": 4.496783,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "2b32250d-00e8-4c07-9fa5-c7f04d7a6150"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `pytree_hacks.py` file is to provide a deprecated interface to pytree utilities, specifically `tree_map_` and `treespec_pprint`, which are now recommended to be accessed through `torch.utils._pytree`. The file raises a deprecation warning, indicating that it will be removed in a future release. The file's content is primarily a placeholder for backwards compatibility, with a TODO comment indicating that it should be removed once the migration of the pytree utility is complete.",
            "total_tokens": 370,
            "latency": 2.721433,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "87b812fb-0dfc-48c4-ae16-ddf3e398a035"
        },
        "gpt-4.1": {
            "file_summary": "The file pytree_hacks.py serves as a transitional compatibility layer that re-exports the tree_map_ and treespec_pprint utilities from torch.utils._pytree. It issues a deprecation warning to inform users that this module will be removed in the future and advises using torch.utils._pytree directly. The file's main functionality is to provide temporary access to these pytree utilities during a migration period.",
            "total_tokens": 349,
            "latency": 1.359955,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 24,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "551dd9b3-f8d5-4320-b293-0c57e6a160f2"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_functorch/pytree_hacks.py",
            "number_of_lines": 24,
            "project_name": "pytorch",
            "total_tokens": 708,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.702606,
            "file_summary": "This file re-exports the `tree_map_` and `treespec_pprint` functions, which are imported from `torch.utils._pytree`. It serves as a temporary file during a migration process. The file itself is deprecated, and users are warned to use `torch.utils._pytree` directly instead.",
            "uuid": "77deb449-134b-4856-a1b6-3762151df4aa"
        },
        "gemini-2.5": {
            "file_name": "/torch/_functorch/pytree_hacks.py",
            "number_of_lines": 24,
            "project_name": "pytorch",
            "total_tokens": 985,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.358605,
            "file_summary": "This Python file re-exports the `tree_map_` and `treespec_pprint` functions from PyTorch's internal `_pytree` utility module. Its primary function is to serve as a temporary alias while issuing a `DeprecationWarning` upon import. The warning advises users to switch to importing these functions directly from `torch.utils._pytree` as this compatibility layer will be removed in the future.",
            "uuid": "12f338ad-0ef9-420b-82d1-0b37e85b6895"
        },
        "Claude3.7": {
            "file_name": "/torch/_functorch/pytree_hacks.py",
            "number_of_lines": 24,
            "project_name": "pytorch",
            "total_tokens": 384,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.151368,
            "file_summary": "This file provides a deprecated interface for PyTree utility functions, specifically re-exporting `tree_map_` and `treespec_pprint` from `torch.utils._pytree`. It issues a deprecation warning to users, instructing them to use `torch.utils._pytree` directly instead of the `torch._functorch.pytree_hacks` module.",
            "uuid": "c24146fe-7cd0-46c1-9bf8-15b253d40747"
        },
        "mistral-small": {
            "file_name": "/torch/_functorch/pytree_hacks.py",
            "number_of_lines": 24,
            "project_name": "pytorch",
            "total_tokens": 406,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.865608,
            "file_summary": " The main functionality of the file \"pytree_hacks.py\" is to provide utility functions for PyTorch's pytree data structure. Specifically, it includes the functions `tree_map_` for applying a function recursively to every leaf in a pytree, and `treespec_pprint` for printing the pytree structure. The file is marked as deprecated and users are advised to use `torch.utils._pytree` instead.",
            "uuid": "e0147ab4-29ec-4b8e-9784-264e1146358f"
        },
        "mistral-7b": {
            "file_name": "/torch/_functorch/pytree_hacks.py",
            "number_of_lines": 24,
            "project_name": "pytorch",
            "total_tokens": 416,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.310805,
            "file_summary": " The Python file \"pytree\\_hacks.py\" is a deprecated module from the Torch library, which includes two functions: \"tree\\_map\\_\" and \"treespec\\_pprint\". These functions are used for recursively applying a function to all the leaves of a nested data structure and pretty-printing the tree structure, respectively. However, the use of this module is discouraged, and users are advised to use the equivalent functions in the \"torch.utils._pytree\" module instead.",
            "uuid": "23bbfd9d-483f-44f7-8428-6f74a95c7b32"
        },
        "cohere-v1": {
            "file_name": "/torch/_functorch/pytree_hacks.py",
            "number_of_lines": 24,
            "project_name": "pytorch",
            "file_summary": "The file pytree_hacks.py imports functions from torch.utils._pytree and re-exports them with a deprecation warning, advising users to use torch.utils._pytree directly instead. These functions, tree_map_ and treespec_pprint, are part of the module's interface and are related to mapping operations and tree specifications.",
            "total_tokens": 348,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.095286,
            "uuid": "6e734c72-f8a1-4ccb-8526-c0935c8f192d"
        }
    },
    "/torch/distributed/elastic/agent/server/health_check_server.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"health_check_server.py\" file is to provide an interface for a health check monitoring server, which can be extended to start a TCP/HTTP server on a specified port. The `HealthCheckServer` class allows for the configuration of a callback function to track the last progress time of an agent, a port number, and a timeout value to determine if the agent is alive or dead. However, the provided implementation does not actually start any health check server, as indicated by the warning message in the `start` method.",
            "total_tokens": 631,
            "latency": 1.198627,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "7300353a-97e0-4d95-b90a-dea68964bda1"
        },
        "titan_premier": {
            "file_summary": "The file \"health_check_server.py\" contains a class \"HealthCheckServer\" that serves as an interface for health check monitoring servers. This class has methods to start and stop the server, but the start method is currently unsupported in Pytorch. The file also includes a function \"create_healthcheck_server\" that creates an instance of the \"HealthCheckServer\" class. The class and function are part of a larger project licensed under the BSD-style license.",
            "total_tokens": 99,
            "latency": 2.447957,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "980bfa06-4de8-455c-988e-420b56487301"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines a class called `HealthCheckServer`, serving as an interface intended for health check monitoring of agents, primarily used within PyTorch's distributed elastic training environment. The class mainly provides methods to initiate and stop a health check server, although the actual server start functionality is currently not supported and simply logs a warning stating this. The file also includes a factory function `create_healthcheck_server` which instantiates and returns an instance of `HealthCheckServer`.",
            "total_tokens": 560,
            "latency": 3.884604,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "6873b531-0e15-4fb1-a991-3a5ed9951c17"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the \"health_check_server.py\" file is to define a class `HealthCheckServer` and a function `create_healthcheck_server` for creating a health check monitor server. The `HealthCheckServer` class includes methods to start and stop the health check server, although the start method does not actually start any server. The `create_healthcheck_server` function is used to instantiate a `HealthCheckServer` object with the provided parameters: alive_callback, port, and timeout.",
            "total_tokens": 567,
            "latency": 1.153836,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "359b3375-6e6e-43fd-a575-cea00f58222b"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"health_check_server.py\" file is to define a HealthCheckServer class that serves as an interface for health check monitoring. This class is designed to be extended for implementing actual health check servers, but in its current form, it doesn't start any real server and only provides a placeholder implementation. The file also includes a function to create instances of the HealthCheckServer class.",
            "total_tokens": 629,
            "latency": 2.977614,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "5aaf278a-c9cb-4ed8-8175-76b46da33a34"
        },
        "nova-pro-v1": {
            "file_summary": "The `health_check_server.py` file defines an interface for a health check monitoring server, specifically the `HealthCheckServer` class, which is intended to be extended for starting a TCP/HTTP server on a specified port. It includes methods to start and stop the server, though the current implementation does not actually start any server and logs a warning instead. The file also provides a factory function `create_healthcheck_server` to instantiate the `HealthCheckServer` class.",
            "total_tokens": 618,
            "latency": 1.283182,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "5290f372-eba7-4e29-b887-37870dc84fbb"
        },
        "gpt-4o": {
            "file_summary": "The `health_check_server.py` file defines a `HealthCheckServer` class that serves as an interface for a health check monitoring server, with methods to start and stop the server, though the actual server functionality is not implemented and only logs warnings. It includes configurable parameters such as an `alive_callback`, `port`, and `timeout` to monitor agent health. Additionally, a helper function, `create_healthcheck_server`, is provided to instantiate the server object.",
            "total_tokens": 559,
            "latency": 1.406171,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "c2ca67c1-c4ce-4fc0-bdcb-31d5f3d1d320"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a `HealthCheckServer` class that provides an interface for health check monitoring, allowing users to create a server that can be extended to start a TCP/HTTP server on a specified port. The class has methods to start and stop the health check server, although the start method is currently unsupported and does not start any server. The file also includes a function to create a `HealthCheckServer` object.",
            "total_tokens": 564,
            "latency": 3.924399,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "5800ac3b-07c4-4f02-8d57-76b432aa9b4f"
        },
        "gpt-4": {
            "file_summary": "The file defines a class `HealthCheckServer` and a function `create_healthcheck_server` for creating a monitoring server in a Python environment, specifically designed for checking the health of a server running PyTorch distributed applications. The `HealthCheckServer` class takes callbacks, port, and timeout parameters but lacks the implementation to effectively start or stop a real server \u2014 it only logs placeholder messages. Essentially, the server acts as a stub for a health checking mechanism potentially for development or testing purposes.",
            "total_tokens": 566,
            "latency": 2.424112,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "5ce1f3f0-fcd9-42bb-9438-9b813d7794f5"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `health_check_server.py` file is to define a `HealthCheckServer` class that provides an interface for health check monitoring, allowing for the creation of a server that can be extended to start a TCP/HTTP server on a specified port. The class has methods to start and stop the health check server, although the `start` method is currently unsupported and does not start any server. The file also includes a `create_healthcheck_server` function to create an instance of the `HealthCheckServer` class.",
            "total_tokens": 582,
            "latency": 2.158533,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "6d1b9353-b1e4-45c8-b9a8-592d1a513379"
        },
        "gpt-4.1": {
            "file_summary": "The file defines a basic HealthCheckServer class meant as an interface for a health monitoring server, providing methods to start and stop such a server, though actual start/stop functionality is not implemented for PyTorch and only logs warnings or info. The server is initialized with a callback, port, and timeout for potential health checks. Additionally, a factory function create_healthcheck_server is provided to instantiate the HealthCheckServer with the specified parameters.",
            "total_tokens": 553,
            "latency": 1.386944,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 66,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "2933275f-e33f-4701-a515-3c350af14470"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/elastic/agent/server/health_check_server.py",
            "number_of_lines": 66,
            "project_name": "pytorch",
            "total_tokens": 1054,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.313971,
            "file_summary": "This Python file defines a `HealthCheckServer` class intended as an interface for health check monitoring, along with a factory function `create_healthcheck_server`. The current implementation of the `HealthCheckServer` is a placeholder (\"noop\") that does not actually start any server or perform health checks. It serves as a base or a non-functional stub for potential future extensions.",
            "uuid": "c9cca8e2-7c78-4724-8c3d-690da9e52bca"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/elastic/agent/server/health_check_server.py",
            "number_of_lines": 66,
            "project_name": "pytorch",
            "total_tokens": 1104,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.231409,
            "file_summary": "This Python file defines a `HealthCheckServer` class intended as an interface for a health monitoring server, using a callback to check agent liveness based on a specified timeout. However, the provided implementation is a no-op, as the `start` method explicitly states it doesn't actually initiate any server process. A factory function `create_healthcheck_server` is also included to instantiate this placeholder server class.",
            "uuid": "981032f0-c69e-46da-8807-8ecdf8bf2539"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/elastic/agent/server/health_check_server.py",
            "number_of_lines": 66,
            "project_name": "pytorch",
            "total_tokens": 625,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.397902,
            "file_summary": "The file defines a `HealthCheckServer` class that serves as an interface for health check monitoring but doesn't actually implement server functionality (it's a no-operation implementation). It provides a structure for health monitoring with callbacks to check if an agent is alive, but explicitly warns that no actual health check server is started when the `start()` method is called, making this essentially a placeholder implementation.",
            "uuid": "630bf299-3be8-43ef-82ac-d2b582f876d7"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/elastic/agent/server/health_check_server.py",
            "number_of_lines": 66,
            "project_name": "pytorch",
            "total_tokens": 662,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.328672,
            "file_summary": " The main functionality of the file \"health_check_server.py\" is to define an interface for a health check monitoring server, which can be extended to start a TCP/HTTP server on a specified port. The file includes a class named \"HealthCheckServer\" and a function \"create_healthcheck_server\" to create an instance of the class. However, the start method of the class is unsupported in PyTorch and does not start any health check server.",
            "uuid": "5cb0c224-a91a-4fe1-a405-71a9de476971"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/elastic/agent/server/health_check_server.py",
            "number_of_lines": 66,
            "project_name": "pytorch",
            "total_tokens": 675,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 3.109397,
            "file_summary": " The given Python file, named \"health_check_server.py\", defines a class named \"HealthCheckServer\" that acts as an interface for a health check monitoring server. Users can create an instance of this class by providing a callback function, a port number, and a timeout value. The main functionality of the file is to create and manage a health check server object, although the provided code does not actually start or stop any server, as indicated by the \"start\" and \"stop\" methods' empty implementations.",
            "uuid": "c31f978c-d3b6-4b0a-aa5b-b9cc8b4923ee"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/elastic/agent/server/health_check_server.py",
            "number_of_lines": 66,
            "project_name": "pytorch",
            "file_summary": "The provided Python script defines a HealthCheckServer class and a create_healthcheck_server function. The class represents a health check monitoring server with an unsupported start method and a stop method. The function creates an instance of HealthCheckServer with the specified alive_callback, port, and timeout arguments. The main functionality seems to be related to health checking and server management.",
            "total_tokens": 561,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 2.447164,
            "uuid": "517f1819-8cf1-454e-96b2-cd1f8963c907"
        }
    },
    "/torch/distributed/algorithms/model_averaging/utils.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"utils.py\" file is to provide utility functions for averaging model parameters in a distributed training setup using PyTorch. The file includes three key functions: `average_parameters`, `get_params_to_average`, and `average_parameters_or_parameter_groups`. These functions facilitate the averaging of parameters across different processes in a distributed environment, ensuring efficient and synchronized updates during training.",
            "total_tokens": 977,
            "latency": 0.935674,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "5ddc441c-6505-4cf2-9ae3-3ec77e6a48b0"
        },
        "titan_premier": {
            "file_summary": "The Python file \"utils.py\" contains three functions:\n\n1. \"average_parameters\" - This function averages the given parameters across all processes in a specified process group. It first checks if the current process is part of the process group. If it is, it creates two iterators from the input parameters, flattens the parameters into a contiguous buffer, divides the buffer by the number of processes, and performs an all-reduce operation to average the parameters across all processes. Finally, it updates the original parameters with the averaged values.\n\n2. \"get_params_to_average\" - This function filters out the parameters that do not contain any gradients from the input parameters. It supports two types of input: a list of model parameters or a list of parameter groups from an optimizer. It returns a list of filtered parameters.\n\n3. \"average_parameters_or_parameter_groups\" - This function is a wrapper function that calls \"average_parameters\" with the filtered parameters obtained from \"get_params_to_average\".\n\nThe file also contains some import statements and type hints for the functions. It is designed to work with PyTorch and distributed computing.",
            "total_tokens": 255,
            "latency": 5.359052,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "11d92e05-4ac0-4696-859b-bb021f803b6b"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements utility functions to average torch parameters (weights of a neural network) across multiple distributed processes, typically for distributed training. It handles efficient all-reduce operations by flattening and unflattening model parameters and supports inputs both as raw parameters and optimizer parameter groups. It includes logic to filter out parameters that do not have gradients, and properly synchronizes data across the distributed process group.",
            "total_tokens": 843,
            "latency": 5.587718,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "61092778-43a2-47cc-9408-9c3f6855ef6a"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `utils.py` file is to provide functions for averaging parameters of a model or parameter groups of an optimizer using distributed training in PyTorch. The file includes functions for averaging parameters, getting parameters to average by filtering out those without gradients, and a high-level function that effectively averages parameters or parameter groups utilizing the previously defined functions. The file uses torch.distributed for communication and synchronization during the parameter averaging process.",
            "total_tokens": 846,
            "latency": 1.241477,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "889c2b3c-edcc-455c-a22e-8e8786d15a47"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"utils.py\" file is to provide utility functions for averaging parameters in distributed machine learning environments. It includes functions to average parameters across multiple processes, filter parameters that need averaging, and handle both model parameters and optimizer parameter groups. The file is designed to work with PyTorch's distributed computing capabilities and supports efficient parameter averaging using allreduce operations.",
            "total_tokens": 1048,
            "latency": 2.473735,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3cae6bc8-56d6-46b4-9d7b-004ae70b60ac"
        },
        "nova-pro-v1": {
            "file_summary": "The `utils.py` file provides utility functions for averaging model parameters in a distributed PyTorch environment. It includes functions to average parameters across a process group, filter parameters that require gradients, and handle both model parameters and optimizer parameter groups. The main functionality revolves around facilitating parameter synchronization in distributed training setups.",
            "total_tokens": 954,
            "latency": 0.92463,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "671057e1-c3d2-48a5-bbbb-a8a5982a6ba2"
        },
        "gpt-4o": {
            "file_summary": "This Python file, `utils.py`, primarily provides functionality to average neural network parameters across multiple processes in a distributed training setup using PyTorch's distributed utilities. It includes methods to filter (retrieve) parameters with gradients for averaging, flatten them into a contiguous buffer for efficient allreduce operations, and restore their original shapes and types after averaging. The file ensures compatibility with PyTorch's distributed group settings and handles both model parameters and optimizer parameter groups.",
            "total_tokens": 851,
            "latency": 1.703519,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "5f361974-9054-4202-929a-d510cd951f24"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to provide utility functions for averaging model parameters or parameter groups in a distributed training setting using PyTorch. The functions `average_parameters` and `average_parameters_or_parameter_groups` allow for the averaging of parameters across different processes in a distributed group, while `get_params_to_average` filters out parameters that do not contain any gradients.",
            "total_tokens": 839,
            "latency": 3.147915,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d76c2406-823e-4ef7-8c36-b3cb106ce0b2"
        },
        "gpt-4": {
            "file_summary": "The file `utils.py` primarily deals with operations related to averaging parameters in PyTorch, tailored for distributed computing environments. It has functions to average parameters directly, fetch parameters which need averaging (while filtering out those without gradients), and handle averaging across different data types efficiently to accommodate data parallelism during model training. Additionally, it ensures synchronization and reduction of parameters across different processes in a group to maintain consistency and accurate averages in a distributed setup.",
            "total_tokens": 848,
            "latency": 2.832739,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b4770158-ce43-4633-971d-67aa4cb5e137"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `utils.py`, is to provide utility functions for averaging model parameters in a distributed training setting using PyTorch. The key functions include `average_parameters`, which averages a list of parameters across a process group, and `get_params_to_average`, which filters out parameters without gradients. These functions can be used to average model parameters or parameter groups of an optimizer in a distributed training environment.",
            "total_tokens": 852,
            "latency": 1.249249,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "51124648-921e-43c1-aacd-8db79f6be130"
        },
        "gpt-4.1": {
            "file_summary": "The provided utils.py file defines utility functions to average PyTorch model parameters across distributed processes using torch.distributed groups, typically for synchronizing parameters during distributed training. It includes functions to (1) aggregate parameters into a flat buffer for efficient all-reduce operations (average_parameters), (2) filter out parameters that require averaging based on gradient presence (get_params_to_average), and (3) provide a convenience wrapper to average either model or optimizer parameters (average_parameters_or_parameter_groups). Overall, the file streamlines the parameter averaging process in distributed machine learning workflows.",
            "total_tokens": 875,
            "latency": 1.500213,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 94,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "47b81bef-cfe3-48ab-a346-b8b15bbe03f6"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/algorithms/model_averaging/utils.py",
            "number_of_lines": 94,
            "project_name": "pytorch",
            "total_tokens": 1541,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.717206,
            "file_summary": "This Python file provides utility functions for averaging PyTorch model parameters or optimizer parameter groups in a distributed setting. It includes a function to filter parameters that have gradients and a core function that flattens, averages using `torch.distributed.all_reduce`, and unpacks the parameters. The main entry point `average_parameters_or_parameter_groups` orchestrates this process for various input types.",
            "uuid": "f6b3843e-9bca-4017-9f8e-301315520c41"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/algorithms/model_averaging/utils.py",
            "number_of_lines": 94,
            "project_name": "pytorch",
            "total_tokens": 1626,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.828636,
            "file_summary": "This Python file provides utility functions for PyTorch distributed training, specifically focusing on averaging parameters across multiple processes. It defines functions to select parameters (optionally filtering for those with gradients) and then performs an efficient `all_reduce` operation to compute and apply the average values to the parameters within a given process group. The main entry point handles both direct model parameters and optimizer parameter groups.",
            "uuid": "1bf9ef00-9110-49b4-ab6b-9e737b75f034"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/algorithms/model_averaging/utils.py",
            "number_of_lines": 94,
            "project_name": "pytorch",
            "total_tokens": 1064,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.420518,
            "file_summary": "The utils.py file provides functionality for averaging parameters across distributed processes in PyTorch, primarily through three main functions: average_parameters, get_params_to_average, and average_parameters_or_parameter_groups. It handles the efficient averaging of model parameters or optimizer parameter groups by flattening parameters into a contiguous buffer for allreduce operations, filtering out parameters without gradients, and supporting different process groups for distributed training scenarios.",
            "uuid": "38dce25b-5a1f-4b56-91dd-aac2f8f6f26d"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/algorithms/model_averaging/utils.py",
            "number_of_lines": 94,
            "project_name": "pytorch",
            "total_tokens": 1141,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.340664,
            "file_summary": " The main functionality of the file \"utils.py\" is to provide functions for averaging parameters in a distributed training setup. The file contains three functions: \"average_parameters\", \"get_params_to_average\", and \"average_parameters_or_parameter_groups\". The \"average_parameters\" function averages all given parameters in a process group, while \"get_params_to_average\" returns a list of parameters that need to be averaged, filtering out parameters without gradients. The \"average_parameters_or_parameter_groups\" function averages parameters of a model or parameter groups of an optimizer.",
            "uuid": "ae5e5d7a-3072-4c28-8060-3c1956dd1dec"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/algorithms/model_averaging/utils.py",
            "number_of_lines": 94,
            "project_name": "pytorch",
            "total_tokens": 1152,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.751336,
            "file_summary": " The given Python file, named \"utils.py\", defines functions for averaging parameters of a PyTorch model or optimizer across multiple processes using distributed computing. The \"average\\_parameters\" function takes an iterator of PyTorch parameters and a process group, averages the parameters by flattening them into a contiguous buffer, and then synchronizes the allreduce operation to ensure consistency among processes. The \"get\\_params\\_to\\_average\" function filters out the parameters that do not contain any gradients. The \"average\\_parameters\\_or\\_parameter\\_groups\" function is a wrapper that calls \"average\\_parameters\" with either the model parameters or optimizer parameter groups.",
            "uuid": "5d42ea46-3d98-4ad5-a34f-b51dba7568f8"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/algorithms/model_averaging/utils.py",
            "number_of_lines": 94,
            "project_name": "pytorch",
            "file_summary": "The Python script performs distributed training parameter averaging. It averages the parameters of a model or optimizer's parameter groups across multiple processes and supports different parameter types. The averaging function synchronizes parameters efficiently using torch.distributed and considers different data types, ensuring proper handling.",
            "total_tokens": 921,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 0.957509,
            "uuid": "455851b8-c28b-4659-8b35-8dfe2f684545"
        }
    },
    "/torch/_inductor/autotune_process.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `autotune_process.py` file is to manage and execute benchmarking tasks for various computational kernels in parallel across multiple devices, primarily GPUs. It achieves this by launching multiple subprocesses, each dedicated to benchmarking a specific kernel, and distributing the workload among these subprocesses using a thread pool. The file includes classes for managing the subprocesses, handling benchmark requests, and performing the actual benchmarking in separate processes to avoid blocking the main program.",
            "total_tokens": 7854,
            "latency": 1.501945,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0f0597c1-62f4-4ce0-b06a-3fabed15e011"
        },
        "titan_premier": {
            "file_summary": "The file `autotune_process.py` is a Python module that contains classes and functions for managing and interacting with a pool of subprocesses for benchmarking kernels in parallel across devices.\n\nThe main functionality of the file includes:\n\n1. `TuningProcess`: A class that represents a benchmarking subprocess. It has methods to start, check if it's alive, put work items, get results, shut down, and kill the subprocess.\n\n2. `TuningProcessPool`: A class that manages a pool of `TuningProcess` instances. It has methods to start the child processes, distribute work items to the subprocesses using a thread pool, and shut down the child processes.\n\n3. `BenchmarkRequest`: An abstract base class that represents a benchmarking request. It has methods to create a run function, clean up the run function, and do the benchmarking.\n\n4. `TritonBenchmarkRequest`: A class that extends `BenchmarkRequest` for benchmarking Triton kernels. It has additional attributes and methods to load the module, precompile the kernel, and create a run function for the kernel.\n\n5. `CUDABenchmarkRequest`: A class that extends `BenchmarkRequest` for benchmarking CUDA kernels. It has additional attributes and methods to compile the source code, load the DLL, and create a run function for the kernel.\n\n6. `CppBenchmarkRequest`: A class that extends `BenchmarkRequest` for benchmarking C++ kernels. It has additional attributes and methods to load the DLL and create a run function for the kernel.\n\n7. `benchmark_in_sub_process`: A function that uses the `TuningProcessPool` to benchmark a list of `TritonTemplateCaller` instances in separate subprocesses and returns the performance numbers (latency).\n\nOverall, the file provides a framework for parallel benchmarking of kernels across devices using a pool of subprocesses.",
            "total_tokens": 446,
            "latency": 10.401103,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "23278e85-2581-4d2c-81a8-0aaf5202a1a9"
        },
        "gpt-4.5": {
            "file_summary": "The Python script implements a subprocess-based autotuning framework for benchmarking kernel performance in PyTorch, specifically targeting GPU and CPU kernels. It involves launching subprocesses to run isolated benchmarks across available devices (typically GPUs), measuring their execution times, and selecting optimal kernels based on these measurements. The file supports Triton, CUDA, and C++ kernel benchmarking, leveraging a combination of multiprocessing, subprocess management, and serialization (pickle) for robust and parallel performance evaluation.",
            "total_tokens": 6604,
            "latency": 9.285382,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "e9882762-9293-40cb-baa2-f30ed4cd7a83"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the provided python file `autotune_process.py` is to implement a system for benchmarking and optimizing kernels in parallel across multiple devices. It includes classes like `TuningProcess` for launching subprocesses, `TuningProcessPool` for managing multiple subprocesses, and various `BenchmarkRequest` subclasses for defining benchmark tasks. The file handles subprocess communication, launching benchmarking tasks, and managing resources like CUDA devices, kernel code cache, and optimizing performance across different devices.",
            "total_tokens": 6573,
            "latency": 1.896022,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "46afd356-be86-4073-a61a-9131d0e72728"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"autotune_process.py\" file is to implement a system for autotuning and benchmarking CUDA, Triton, and C++ kernels in separate subprocesses. It defines classes for managing tuning processes, handling benchmark requests, and executing benchmarks across multiple devices. The file also includes utilities for inter-process communication, tensor metadata handling, and code caching to optimize the autotuning process.",
            "total_tokens": 8794,
            "latency": 3.86819,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "457481b2-9854-407e-9b1c-fb01363135e8"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `autotune_process.py` file is to manage and execute benchmarking tasks for various kernels in separate subprocesses, utilizing a pool of `TuningProcess` instances to parallelize the work across available devices. It defines classes and methods for handling different types of benchmark requests, including Triton, CUDA, and CPU benchmarks, and provides mechanisms for serializing and executing these requests in isolated processes to ensure stability and performance.",
            "total_tokens": 7849,
            "latency": 1.58718,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "9f8d4dcf-5a89-4d0e-b843-fc29392fb87e"
        },
        "gpt-4o": {
            "file_summary": "The provided Python script, `autotune_process.py`, is a highly specialized module dedicated to managing and executing benchmark operations for optimizing computational kernels, particularly in GPU and CPU execution environments. It handles the creation and management of subprocesses (`TuningProcess` and `TuningProcessPool`) to execute benchmarks in isolation, ensuring resource allocation is streamlined and parallelism across devices is utilized. Additionally, it defines multiple benchmark request classes tailored to different programming frameworks (e.g., Triton, CUDA, C++), facilitating kernel performance evaluation within various execution contexts efficiently.",
            "total_tokens": 6626,
            "latency": 2.839386,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "6594462e-fd25-4218-b85c-98f18e0a20e1"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to provide a framework for benchmarking and autotuning of PyTorch kernels, particularly those generated by the Triton compiler. The file defines several classes and functions that enable the creation of benchmarking requests, the execution of these requests in separate processes, and the collection of results. The framework supports both CPU and GPU devices and allows for the benchmarking of different kernel configurations.",
            "total_tokens": 6563,
            "latency": 4.972151,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "ac6e8f0d-0d3d-452d-a9b3-53bd0907c1b9"
        },
        "gpt-4": {
            "file_summary": "The Python file `autotune_process.py` handles automated performance tuning for Tensor Processing through benchmarking in subprocesses. It provides classes and methods to launch subprocesses specifically configured with distinct device environments, run benchmark tests on various Tensor processing operations using the Torch library, and maintain pool of tuning processes to manage multiple benchmark subprocesses. The file mainly concentrates on setting up the environment for subprocesses, executing benchmark tests, handling inter-process communication, and maintaining a process pool for tuning various kernel operations on different hardware devices.",
            "total_tokens": 6577,
            "latency": 3.384907,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "2e0a3692-14d7-491f-bb66-27c751d97422"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `autotune_process.py`, is to manage a pool of subprocesses for benchmarking and autotuning of PyTorch kernels, particularly those using the Triton library. It creates a pool of subprocesses, each of which can execute benchmarking tasks independently, allowing for parallelization of the autotuning process across multiple devices (e.g., GPUs). The file defines various classes and functions to handle the creation, management, and communication with these subprocesses, as well as the execution of benchmarking tasks within them.",
            "total_tokens": 6596,
            "latency": 2.711676,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "59d7e501-3f4d-4824-92ee-ddaff61a773f"
        },
        "gpt-4.1": {
            "file_summary": "The main functionality of **autotune_process.py** is to orchestrate autotuning and benchmarking of kernel implementations (such as Triton, CUDA, or C++ kernels) in subprocesses, isolating each benchmark in its own process or across multiple devices for robustness and parallelism. It provides a mechanism to serialize requests and results, distribute them to worker subprocesses, and coordinate resource management, including workspace allocation and kernel precompilation. The file defines abstractions for specifying benchmark requests, handling device specifics (CPU/GPU), and ensuring safe and efficient benchmarking in a way that tolerates potential crashes or timeouts during kernel evaluation.",
            "total_tokens": 6641,
            "latency": 2.416959,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 896,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "9bd811be-9cd0-43cd-a8e6-e63edb566eb2"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/autotune_process.py",
            "number_of_lines": 896,
            "project_name": "pytorch",
            "total_tokens": 8929,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 5.256646,
            "file_summary": "This Python file implements a system for benchmarking kernel code (like Triton, CUDA, and C++) in isolated subprocesses. It manages a pool of these subprocesses, typically one per available device, to execute benchmarking tasks concurrently. Kernel details and tensor metadata are serialized and sent to the subprocesses, which run the kernels and return performance results to the parent process, providing isolation and parallel execution for autotuning.",
            "uuid": "02032188-1b71-4c93-bd8f-d442d5cc8879"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/autotune_process.py",
            "number_of_lines": 896,
            "project_name": "pytorch",
            "total_tokens": 8824,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.370931,
            "file_summary": "This Python file defines infrastructure for benchmarking PyTorch kernels (Triton, CUDA, C++) by running them in isolated subprocesses to prevent crashes and ensure accurate timing. It manages a pool of these subprocesses, often one per available device, allowing for parallel benchmarking across multiple GPUs or CPU cores. The code handles serializing benchmark requests (including kernel details and tensor metadata), sending them to worker processes, and retrieving the timing results.",
            "uuid": "f358f770-4201-45b6-b800-4b33f2880622"
        },
        "Claude3.7": {
            "file_name": "/torch/_inductor/autotune_process.py",
            "number_of_lines": 896,
            "project_name": "pytorch",
            "total_tokens": 8779,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.406539,
            "file_summary": "The file \"autotune_process.py\" implements a framework for autotuning GPU and CPU kernels by benchmarking different implementations in separate subprocesses. It defines classes for managing tuning processes, handling benchmark requests for different backends (Triton, CUDA, C++), and safely executing potentially unstable code in isolated environments to prevent crashes in the main process.",
            "uuid": "02c3e1b1-a049-4cb8-abdd-a5e0f9ec7b59"
        },
        "mistral-small": {
            "file_name": "/torch/_inductor/autotune_process.py",
            "number_of_lines": 896,
            "project_name": "pytorch",
            "total_tokens": 9362,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 8.661978,
            "file_summary": " The given Python file is related to autotuning in PyTorch. It defines several classes and functions for benchmarking and autotuning operations on different devices (CPU and GPU).\n\nThe main functionality of the file is to analyze a given Python file containing operations to be benchmarked, create appropriate benchmark requests for each operation, and then benchmark these operations in a separate process using a `TuningProcessPool`. The results of the benchmarking are returned as a dictionary mapping each operation to its corresponding performance metric (latency).\n\nThe file supports benchmarking of operations implemented using PyTorch's Triton and CUDA backends. It also includes support for unit testing using a `_TestBenchmarkRequest` class.\n\nHere are the main classes and functions defined in the file:\n\n* `TuningProcess`: A class that manages communication with a subprocess for benchmarking operations.\n* `TuningProcessPool`: A class that manages a pool of `TuningProcess` instances for parallel benchmarking of operations.\n* `BenchmarkRequest`: An abstract base class for defining benchmark requests.\n* `_TestBenchmarkRequest`: A class for unit testing.\n* `TritonBenchmarkRequest`: A class for benchmarking operations implemented using PyTorch's Triton backend.\n* `TritonGPUBenchmarkRequest`: A class for benchmarking operations implemented using PyTorch's Triton backend on GPU devices.\n* `TritonCPUBenchmarkRequest`: A class for benchmarking operations implemented using PyTorch's Triton backend on CPU devices.\n* `CUDABenchmarkRequest`: A class for benchmarking operations implemented using PyTorch's CUDA backend.\n* `CppBenchmarkRequest`: A class for benchmarking operations implemented using PyTorch's C++ backend.\n* `get_tuning_process_pool`: A function that returns a `TuningProcessPool` instance.\n* `benchmark_in_sub_process`: A function that benchmarks a given list of operations in a separate process using a `TuningProcessPool`.\n\nThe file also includes several imports and helper functions for handling different types of operations and devices.",
            "uuid": "70bb8cf3-2790-41ba-bf9e-b4811b008ae4"
        },
        "mistral-7b": {
            "file_name": "/torch/_inductor/autotune_process.py",
            "number_of_lines": 896,
            "project_name": "pytorch",
            "total_tokens": 8984,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 3.490656,
            "file_summary": " The given Python file defines classes and functions for benchmarking various types of kernels, including GPU and CPU kernels written in CUDA, Cpp, and Triton. The main functionality of the file is to launch and interact with a benchmarking subprocess, which runs the benchmarking logic in a separate process. The subprocess receives work items from the main process, executes the benchmark, and returns the results back to the main process. The file also includes utilities for managing the subprocess pool, handling different device types, and serializing objects for communication between processes.",
            "uuid": "ef7f6d90-abba-4c64-bb6e-532044ca64fa"
        },
        "cohere-v1": {
            "file_name": "/torch/_inductor/autotune_process.py",
            "number_of_lines": 896,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, autotune_process.py, is a module for automated kernel tuning. It defines classes for managing benchmarking subprocesses, such as TuningProcess and TuningProcessPool, which allow for parallel kernel benchmarking across devices. Additionally, it contains classes for requesting benchmarks, like TritonBenchmarkRequest and CUDABenchmarkRequest, which facilitate the benchmarking of Triton kernels and CUDA kernels, respectively, by creating functions for kernel execution. The script also includes utility functions for obtaining tuning process pools and performing benchmarks in separate processes.",
            "total_tokens": 7543,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 2.497892,
            "uuid": "125498a3-df77-4d90-8cd0-be00527f81e3"
        }
    },
    "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py": {
        "nova-lite-v1": {
            "file_summary": "The file `rocm_benchmark_request.py` defines a class `ROCmBenchmarkRequest` that handles the precompilation and execution of ROCm (Radeon Open Compute) kernel benchmarks within the PyTorch framework. It manages the compilation of kernel source code into shared object files, initializes necessary workspaces, and creates callable functions to execute the compiled kernels with specified input and output tensors. The class integrates with PyTorch's internal mechanisms for device-specific code caching and execution.",
            "total_tokens": 1639,
            "latency": 1.068551,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e1a711e3-a4f5-44c4-8f16-66507964b815"
        },
        "titan_premier": {
            "file_summary": "The file \"rocm_benchmark_request.py\" is a Python class that extends the \"GPUDeviceBenchmarkMixin\" and \"BenchmarkRequest\" classes. It is used to manage the compilation and execution of a kernel function on a GPU device using the ROCm platform. The class includes methods for precompiling the kernel code, generating a function to run the kernel, updating the workspace size, loading the DLL, and cleaning up after running the kernel. The class also includes methods for serialization and deserialization of the object across process boundaries.",
            "total_tokens": 119,
            "latency": 3.065521,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "35d06311-29c3-48bf-81c4-0f074ba5efdc"
        },
        "gpt-4.5": {
            "file_summary": "This Python file defines the `ROCmBenchmarkRequest` class, which is tailored for benchmarking ROCm GPU kernels in the context of PyTorch's inductor compiler backend. The class compiles and executes ROCm kernels from provided source code, dynamically manages workspace memory, and caches compiled source using `ROCmCodeCache`. It provides methods for creating executable functions (`make_run_fn`), estimating workspace requirements (`update_workspace_size`), and handling dynamic loading/unloading of compiled DLLs.",
            "total_tokens": 1358,
            "latency": 4.179292,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8b9cd829-a492-4c41-8503-a5b909f74ab6"
        },
        "gpt-3.5-turbo": {
            "file_summary": "This Python file, named 'rocm_benchmark_request.py', defines a class called ROCmBenchmarkRequest that extends GPUDeviceBenchmarkMixin and BenchmarkRequest classes. It handles benchmark requests for ROCm by precompiling source code, creating run functions, updating workspace size, loading dynamic link libraries (DLLs), and cleaning up after execution, in order to facilitate efficient GPU device benchmarking with TensorFlow code.",
            "total_tokens": 1333,
            "latency": 1.109838,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "f75beb2f-9925-4676-b56b-1544e3bc6406"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"rocm_benchmark_request.py\" file is to define a ROCmBenchmarkRequest class for benchmarking ROCm (Radeon Open Compute) kernels. This class handles the compilation, loading, and execution of ROCm kernels, including managing workspace memory and interfacing with the ROCm code cache. It provides methods for precompiling, creating run functions, updating workspace sizes, and cleaning up resources associated with the benchmark requests.",
            "total_tokens": 1788,
            "latency": 3.12977,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "94d6af6c-5cb7-44af-b001-ed0ff2c133d9"
        },
        "nova-pro-v1": {
            "file_summary": "The `rocm_benchmark_request.py` file defines a `ROCmBenchmarkRequest` class for handling benchmarking requests specific to ROCm (Radeon Open Compute) within the PyTorch framework. It manages the precompilation of ROCm kernel code, dynamic loading of compiled libraries, and execution of kernel functions with appropriate workspace management.",
            "total_tokens": 1606,
            "latency": 1.020834,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "bd69fb3a-7fbe-4a53-8ced-6b18744de373"
        },
        "gpt-4o": {
            "file_summary": "The `rocm_benchmark_request.py` file defines a `ROCmBenchmarkRequest` class that facilitates benchmarking and running GPU kernels on AMD ROCm architecture using PyTorch's Inductor framework. It manages kernel metadata, code caching, and dynamic compilation, while also generating callable functions to execute compiled kernels with support for handling workspace memory allocation. Additionally, it ensures thread-safe precompilation and cleanup procedures to efficiently utilize GPU resources.",
            "total_tokens": 1347,
            "latency": 1.567887,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "ad32c65d-42cf-40c5-b7b8-2be41c671e84"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a class `ROCMBenchmarkRequest` that represents a request to benchmark a kernel on a ROCm (Radeon Open Compute) device. The class provides methods to precompile the kernel, create a run function for the kernel, update the workspace size, and ensure the DLL is loaded. The class is designed to be used in a multi-threaded environment and is responsible for managing the kernel's compilation, execution, and cleanup.",
            "total_tokens": 1354,
            "latency": 4.408685,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3b1585ac-9a02-421b-ae8e-de9eded502e7"
        },
        "gpt-4": {
            "file_summary": "The Python file `rocm_benchmark_request.py` defines a class `ROCmBenchmarkRequest` which inherits behavior from `GPUDeviceBenchmarkMixin` and `BenchmarkRequest`. This class is utilized primarily for handling kernel benchmarking requests in a ROCm (Radeon Open Compute) environment. The class mainly focuses on managing the DAL\u00cd wrapping for compiled GPU kernels, updating and managing workspace sizes for tensor operations, and precompiling source code for ROCm execution, ensuring all instances are serializable and ready for cross-process utility.\n",
            "total_tokens": 1358,
            "latency": 3.774992,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "08d5def0-f115-47c6-aab5-58013604efba"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `rocm_benchmark_request.py`, is to handle benchmarking requests for ROCm (Radeon Open Compute) kernels, which are used for GPU acceleration. The `ROCmBenchmarkRequest` class manages the compilation, execution, and cleanup of these kernels, utilizing the `torch` library and `ROCmCodeCache` for code caching and compilation. This class enables the creation of a run function for a given kernel, which can be executed with input tensors and an output tensor, allowing for efficient benchmarking and testing of ROCm kernels.",
            "total_tokens": 1377,
            "latency": 2.705526,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a7ba9a0f-cf6c-4e34-9326-6815345ba01f"
        },
        "gpt-4.1": {
            "file_summary": "The file defines ROCmBenchmarkRequest, a class for managing the compilation, caching, and execution of ROCm (AMD GPU) kernels for benchmarking purposes in PyTorch's Inductor backend. It handles kernel code compilation (and caching), workspace memory management, and dynamically creates callable run functions for executing the GPU kernels with specified input/output tensor metadata and arguments. The class ensures resources are properly managed and is designed to be serializable for use across process boundaries.",
            "total_tokens": 1352,
            "latency": 1.597793,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 147,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "427ba42a-8c3a-4924-b25e-5a3544a4132f"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py",
            "number_of_lines": 147,
            "project_name": "pytorch",
            "total_tokens": 2263,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 7.850517,
            "file_summary": "This file defines the `ROCmBenchmarkRequest` class, responsible for managing the benchmarking of compiled ROCm kernels within PyTorch's inductor framework. It handles the compilation of ROCm source code into a shared library using `ROCmCodeCache` and manages the loading of this library. The class prepares a callable function to execute the kernel with specified input/output tensors and arguments, including determining and allocating necessary workspace memory.",
            "uuid": "17a1a077-72e4-4285-843b-5d33801d5d2e"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py",
            "number_of_lines": 147,
            "project_name": "pytorch",
            "total_tokens": 2076,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.603211,
            "file_summary": "This Python file defines the `ROCmBenchmarkRequest` class, which extends `BenchmarkRequest` for ROCm (AMD GPU) devices. It encapsulates the source code and metadata for a specific kernel to be benchmarked. The class handles compiling the kernel source code, loading the compiled library, determining the required workspace size, and generating a callable function to execute the kernel with provided tensors.",
            "uuid": "be1535bb-e00f-41eb-b95a-4e89fdab84ed"
        },
        "Claude3.7": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py",
            "number_of_lines": 147,
            "project_name": "pytorch",
            "total_tokens": 1799,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.613813,
            "file_summary": "The ROCmBenchmarkRequest class in rocm_benchmark_request.py implements functionality for benchmarking ROCm GPU kernels in PyTorch's inductor framework. It handles compiling GPU kernel source code, loading the compiled code as a DLL, managing workspace memory, and creating callable functions that execute the kernels with specified input and output tensors. The class extends BenchmarkRequest with ROCm-specific functionality, including methods to precompile kernels, update workspace sizes, and clean up resources after benchmarking.",
            "uuid": "f77399c7-1480-494c-ab00-7d9adb3d92db"
        },
        "mistral-small": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py",
            "number_of_lines": 147,
            "project_name": "pytorch",
            "total_tokens": 1839,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 4.250436,
            "file_summary": " The main functionality of the file \"rocm_benchmark_request.py\" is to create a request for benchmarking a specific kernel on a ROCm (Radeon Open Compute Platform) device. This request includes the kernel name, input and output tensor metadata, extra arguments, and the source code of the kernel. The request also handles precompilation of the code, workspace size calculation, and creation of a run function for the kernel. The file uses the PyTorch library for tensor operations and the ROCmCodeCache class for code compilation and loading.",
            "uuid": "066dd3f6-978b-4712-b64f-5e7b3ad2bf6d"
        },
        "mistral-7b": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py",
            "number_of_lines": 147,
            "project_name": "pytorch",
            "total_tokens": 1950,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 4.381631,
            "file_summary": " This Python file defines a class named `ROCmBenchmarkRequest` that is used for benchmarking ROCm (Radeon Open Compute) kernels in PyTorch. The main functionality of the file includes:\n\n1. Defining the `ROCmBenchmarkRequest` class that inherits from `GPUDeviceBenchmarkMixin` and `BenchmarkRequest`. This class is responsible for managing the kernel name, input and output tensor metadata, extra arguments, source code, and workspace size for ROCm benchmarking.\n2. Implementing methods such as `precompile()`, `make_run_fn()`, `update_workspace_size()`, `ensure_dll_loaded()`, and `cleanup_run_fn()` to handle precompilation, creating a run function, updating workspace size, loading the DLL, and cleaning up the run function, respectively.\n3. Utilizing the `ROCmCodeCache` to write, compile, and load the source code from the cache as needed.",
            "uuid": "55bd9977-7792-4113-bf43-246727b938c2"
        },
        "cohere-v1": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_benchmark_request.py",
            "number_of_lines": 147,
            "project_name": "pytorch",
            "file_summary": "The python script defines a class ROCmBenchmarkRequest that facilitates the benchmarking of kernels on ROCm devices. It prepares the necessary artifacts for kernel execution, including precompiling the kernel, and provides a method to create a runnable function for the kernel with a specific workspace size. Additionally, it handles the loading and cleanup of the kernel DLL.",
            "total_tokens": 1548,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 2.358161,
            "uuid": "867cb860-e33d-4a88-bc7e-b7cc59d7fb51"
        }
    },
    "/torch/_higher_order_ops/invoke_subgraph.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"invoke_subgraph.py\" file is to facilitate the invocation of a subgraph within PyTorch's autograd and functionalization mechanisms. It provides a way to trace and compile subgraphs, enabling efficient execution and gradient computation. The file defines a custom Higher Order Operator (HOP) called \"InvokeSubgraphHOP\" that allows the subgraph to be identified and compiled, and it includes utility functions for marking regions to be compiled, tracing joint graphs, and creating forward and backward graphs for the subgraph.",
            "total_tokens": 3639,
            "latency": 1.379098,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "cf5ccad6-90c8-4d8d-9a94-b6824245a786"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to define a class `InvokeSubgraphHOP` that extends `HigherOrderOperator` and provides a method `__call__` to handle the `invoke_subgraph` operation. It also defines a function `invoke_subgraph_placeholder` that serves as a placeholder for the `invoke_subgraph` operation when used with `torch.compile`. Additionally, the file includes functions for marking a region for compilation, getting the cache for `invoke_subgraph`, and creating forward and backward graphs for the `invoke_subgraph` operation. The file also includes a class `InvokeSubgraphAutogradOp` that extends `torch.autograd.Function` and provides methods for the forward and backward passes of the `invoke_subgraph` operation. The file also includes various helper functions and decorators to support the main functionality.",
            "total_tokens": 182,
            "latency": 4.642178,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "1c404c55-f343-4dac-a9c9-1575af3201af"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements advanced internal mechanisms within PyTorch to manage, invoke, and cache computations of computational subgraphs, a component specifically used by torch.compile and Dynamo operations. It defines a HigherOrderOperator named \"invoke_subgraph,\" facilitating subgraph execution and integration with PyTorch's auto-differentiation (autograd) system, tracing routines, and optimization tooling. Primarily, it includes logic to handle both forward-backward graph execution, caching of traced subgraphs, and integrating cleanly with functionalization, autograd, tracing, and fake tensor proxy modes.",
            "total_tokens": 3067,
            "latency": 6.215747,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "679d6f53-ca15-421d-ac1d-38e0c1fab632"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `invoke_subgraph.py` file is to define and implement a higher-order operator (`InvokeSubgraphHOP`) for invoking subgraphs in PyTorch. This file contains functions for invoking subgraphs, tracing forward and backward graphs, handling autograd operations, and managing cache for subgraph invocations. Additionally, it includes implementations for different dispatch keys like `CompositeExplicitAutograd` and `Autograd`, as well as proxy tensor dispatch modes.",
            "total_tokens": 3033,
            "latency": 1.604888,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d0ca1828-a6a0-49ab-a13f-543f9b9b7b2a"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"invoke_subgraph.py\" file is to implement a higher-order operator called \"invoke_subgraph\" that allows for the execution of subgraphs within a larger computational graph. It provides mechanisms for tracing, caching, and executing these subgraphs, including support for autograd and functionalization. The file also includes implementations for various dispatch keys and proxy modes, enabling integration with PyTorch's compilation and tracing systems.",
            "total_tokens": 4040,
            "latency": 3.5362,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "ce157381-986b-483f-9f11-42669657d45a"
        },
        "nova-pro-v1": {
            "file_summary": "The `invoke_subgraph.py` file defines a mechanism for invoking subgraphs within PyTorch, utilizing higher-order operators (HOPs) to manage both forward and backward computations. It includes functionality for tracing and caching subgraphs, handling autograd operations, and integrating with TorchDynamo for compilation optimizations.",
            "total_tokens": 3597,
            "latency": 1.249014,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "7a7ac6ec-e9a1-4d6d-a00b-7628d38bf0c4"
        },
        "gpt-4o": {
            "file_summary": "The `invoke_subgraph.py` file implements functionality for executing, tracing, and functionalizing computational subgraphs within PyTorch's higher-order operations and automatic differentiation framework. It introduces a custom `InvokeSubgraphHOP` operator to handle the invocation and caching of subgraphs, which are either PyTorch `GraphModule` objects or functionalization context wrappers. The file also includes integration with PyTorch's `torch.compile`, caching mechanisms for dispatch modes, and autograd hooks for forward and backward graph tracing to optimize computation and support efficient automatic differentiation.",
            "total_tokens": 3061,
            "latency": 2.011862,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "40a12438-4341-4819-8e56-59c4c812d1ba"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to implement the `invoke_subgraph` function, which is a higher-order operator (HOP) that allows for the invocation of a subgraph within a larger graph. The subgraph can be a PyTorch `GraphModule` or a `FunctionalizeCtxWrapper`, and the `invoke_subgraph` function takes care of tracing the forward and backward graphs, as well as caching the results to avoid redundant computations. The file also defines several helper functions and classes to support the `invoke_subgraph` function, including autograd functions, tracing functions, and caching mechanisms.",
            "total_tokens": 3066,
            "latency": 5.668458,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "61244c03-0cc8-4ad7-bab3-b67ccdf98589"
        },
        "gpt-4": {
            "file_summary": "The Python file \"invoke_subgraph.py\" defines and handles the execution of subgraphs in the PyTorch framework, particularly for purposes like autograd operations, TorchDynamo compilation, and handling tensors in different contexts (such as functionalization and proxy modes). Central to the file is `InvokeSubgraphHOP`, a higher order operator class for invoking subgraphs, supported by various utility functions and class methods to manage the invocation in various compute environments (e.g., autograd, functionalization). It also features methods to trace and optimize joint forward and backward computational graphs, integrate with TorchDynamo to ensure efficient execution, and handle specialized tensor operations suitable for dynamic code transformation and graph-level optimizations in PyTorch.",
            "total_tokens": 3082,
            "latency": 5.108322,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "2b90d499-a66e-4dfa-9d17-8deba5cb193d"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `invoke_subgraph.py`, is to implement a higher-order operator (HOP) called `InvokeSubgraph` that enables the invocation of subgraphs within PyTorch models, allowing for more efficient and flexible computation graphs. This operator is designed to work with PyTorch's dynamic computation graph and autograd system, enabling features like just-in-time (JIT) compilation and automatic differentiation. The file defines various implementations of the `InvokeSubgraph` operator for different dispatch keys, including autograd, composite explicit autograd, and proxy torch dispatch mode, to handle different scenarios and use cases.",
            "total_tokens": 3074,
            "latency": 5.238344,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "fc480a1d-2d1c-4019-ad21-38780ac9c634"
        },
        "gpt-4.1": {
            "file_summary": "The file defines and implements the \"invoke_subgraph\" Higher Order Operator (HOP) in PyTorch, facilitating the encapsulation, invocation, and autodiff support for a compiled subgraph within larger computation graphs. It provides integration points for tracing, functionalization, autograd, and proxy dispatch backends, as well as caching logic for repeated subgraph execution, enabling efficient re-use and correct gradient computation. Additionally, the file exposes utilities and decorators (such as mark_compile_region) allowing users or the Torch compiler to demarcate subregions for isolated tracing, compilation, or special execution.",
            "total_tokens": 3071,
            "latency": 2.005836,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 354,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "e5440851-d928-4775-8624-ba79ba2ac185"
        },
        "gemini-2.5": {
            "file_name": "/torch/_higher_order_ops/invoke_subgraph.py",
            "number_of_lines": 354,
            "project_name": "pytorch",
            "total_tokens": 4086,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.20629,
            "file_summary": "This Python file defines a PyTorch Higher Order Operator (HOP) called `invoke_subgraph` used primarily by `torch.compile`. Its main purpose is to encapsulate a region of code (often marked by the `@mark_compile_region` decorator) into a distinct subgraph within the larger computation graph. This allows `torch.compile` to compile the subgraph once and reuse the compiled artifact, preventing aggressive inlining and enabling modular compilation and differentiation through the subgraph via custom autograd handling.",
            "uuid": "b49eb3fa-395f-4a91-8b47-44233cfb1a73"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_higher_order_ops/invoke_subgraph.py",
            "number_of_lines": 354,
            "project_name": "pytorch",
            "total_tokens": 3893,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.547777,
            "file_summary": "This Python file defines a PyTorch `HigherOrderOperator` called `invoke_subgraph`. This operator is used by `torch.compile` to represent and manage compiled or traced subgraphs within a larger computation. It handles dispatching for various modes like Autograd, Functionalization, and Proxy tracing, including tracing joint forward/backward graphs and caching results.",
            "uuid": "66dea07d-b02d-42ee-a898-afa8fb88e6d1"
        },
        "Claude3.7": {
            "file_name": "/torch/_higher_order_ops/invoke_subgraph.py",
            "number_of_lines": 354,
            "project_name": "pytorch",
            "total_tokens": 4043,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 5.332254,
            "file_summary": "The file \"invoke_subgraph.py\" implements a higher-order operator called \"invoke_subgraph\" that enables the compilation and reuse of subgraphs within PyTorch's computation framework. It provides functionality for tracing, caching, and executing subgraphs with proper handling of autograd, allowing for efficient reuse of compiled code regions through the \"mark_compile_region\" decorator and supporting PyTorch's various dispatch modes including functionalization and autograd.",
            "uuid": "908c6fa3-fd10-4b97-ad09-aeb73a54f24a"
        },
        "mistral-small": {
            "file_name": "/torch/_higher_order_ops/invoke_subgraph.py",
            "number_of_lines": 354,
            "project_name": "pytorch",
            "total_tokens": 4095,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 3.487002,
            "file_summary": " The main functionality of the file `invoke_subgraph.py` is to define a Higher Order Operator (HOP) called `InvokeSubgraphHOP` in PyTorch. This operator allows for the creation of a subgraph in a computation graph, which can then be executed as a single unit. The file also includes various helper functions and classes to support the use of this operator, such as `invoke_subgraph_placeholder`, `mark_compile_region`, and `InvokeSubgraphAutogradOp`. The overall purpose of this file is to enable more efficient and flexible computation graph construction and execution in PyTorch.",
            "uuid": "7d363bb5-b7cc-4261-9da9-f3355893d8e8"
        },
        "mistral-7b": {
            "file_name": "/torch/_higher_order_ops/invoke_subgraph.py",
            "number_of_lines": 354,
            "project_name": "pytorch",
            "total_tokens": 4187,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 4.256182,
            "file_summary": " The given Python file is a part of PyTorch implementation, specifically for the InvokeSubgraph higher-order operator (HOP). The main functionality of the file can be summarized as follows:\n\n1. The file defines the InvokeSubgraphHOP class, which is a custom higher-order operator that handles the invocation of a subgraph with a given identifier and operands.\n2. The file also includes functions for creating and tracing the forward and backward graphs of the subgraph, as well as functions for registering and caching the subgraph in different modes.\n3. The file also includes the InvokeSubgraphAutogradOp class, which is an autograd function op that stashes the backward graph in the context while running forward.\n\nIn summary, the main functionality of the file is to define and implement the InvokeSubgraph higher-order operator, which is used to invoke a subgraph with a given identifier and operands, and to trace the forward and backward graphs of the subgraph for autograd purposes.",
            "uuid": "5ff24d12-6feb-4941-942d-c14302b26c3a"
        },
        "cohere-v1": {
            "file_name": "/torch/_higher_order_ops/invoke_subgraph.py",
            "number_of_lines": 354,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, named invoke_subgraph.py, defines a higher-order operator (HOP) called InvokeSubgraphHOP and its corresponding functions for invoking a subgraph in a neural network. This file enables compiling and reusing the subgraph, which is a portion of a larger graph, to optimize the network's performance. It also includes autograd functionality for automatic differentiation.",
            "total_tokens": 3418,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 12.699837,
            "uuid": "3494060f-b2ad-4c2a-bb20-40cfa8fafbe5"
        }
    },
    "/tools/code_coverage/package/oss/utils.py": {
        "nova-lite-v1": {
            "file_summary": "The `utils.py` file provides utility functions for managing and interacting with binary files, environment variables, and subprocesses, specifically tailored for a PyTorch-related project. It includes functions for locating binary files, detecting compiler types, and running Python tests. The file also handles environment-specific configurations and cleanup tasks, such as removing `.gcda` files.",
            "total_tokens": 1144,
            "latency": 0.945472,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "bf5e8469-461a-4152-82fe-051dc3028d3a"
        },
        "titan_premier": {
            "file_summary": "The 'utils.py' file contains various utility functions for managing files and directories, as well as interacting with the operating system. Specifically, it includes functions for:\n\n1. Retrieving the path to the PyTorch folder and its subdirectories, such as the binary folder and shared library.\n2. Detecting the compiler type based on user input or by automatically detecting it from the system.\n3. Cleaning up gcda files in the build directory.\n4. Running Python test scripts and handling any errors that may occur.\n\nThe file also includes functions for removing files and printing error messages, which are likely used by other functions in the file. Overall, the main functionality of the file is to provide a set of tools for managing and running tests in the PyTorch codebase.",
            "total_tokens": 173,
            "latency": 3.783641,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "eb41d1e1-7240-4a86-9e66-8efdc0afcf53"
        },
        "gpt-4.5": {
            "file_summary": "This Python file contains utility functions related to environment setup, file paths, and compiler detection for a PyTorch project's code coverage tools. It handles finding test binaries, shared libraries, LLVM tools, cleaning generated coverage files (\".gcda\"), and executing OSS Python tests. Additionally, it includes functionality to auto-detect or validate the compiler type (clang or gcc) based on environment variables or the system settings.",
            "total_tokens": 965,
            "latency": 3.840876,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "366ff38d-bc2f-437b-bdfe-1dce97b8e86e"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given `utils.py` file contains various utility functions related to managing binary files, shared libraries, compiler types, and executing Python tests in a PyTorch project. It includes functions to get paths for binary files and LLVM tools, detect compiler types, clean up generated files, and run Python tests. Additionally, there are functions to handle environmental variables, such as `PYTORCH_FOLDER` and `CXX`, and interactions with the system using subprocess and os modules.",
            "total_tokens": 970,
            "latency": 1.027177,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8e24895a-3f48-46c4-adc5-0c4a54b51940"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"utils.py\" file is to provide utility functions for managing and executing tests in a PyTorch development environment. It includes functions for locating binary files and folders, detecting compiler types, handling gcda files, and running Python tests. The file also contains helper functions for retrieving paths and cleaning up test-related files.",
            "total_tokens": 1207,
            "latency": 2.453213,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "a2232e87-3420-483a-a063-042e2ddb6670"
        },
        "nova-pro-v1": {
            "file_summary": "The `utils.py` file provides utility functions for locating and managing binaries, libraries, and configuration settings related to testing and compilation within a PyTorch development environment. It includes functions for determining paths to binaries and shared libraries, detecting the compiler type, and running Python tests. Additionally, it offers cleanup functionality for test-related files.",
            "total_tokens": 1137,
            "latency": 0.980909,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "6eaea079-6ce6-412f-b8f5-5218ae6ddad1"
        },
        "gpt-4o": {
            "file_summary": "The `utils.py` file provides utility functions to support testing and build processes for a codebase, including managing and retrieving paths for binaries, shared libraries, and test files related to PyTorch. It also includes functionalities for detecting the compiler type, cleaning up `.gcda` coverage files, and running Python test scripts. The file leverages environment variables and subprocess commands for dynamic setup and operations.",
            "total_tokens": 963,
            "latency": 1.405053,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "589c188c-d268-4bc0-8290-cd7f88b90c26"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to provide utility functions for working with the PyTorch project, including getting binary folders and files, detecting compiler types, cleaning up gcda files, and running OSS Python tests. The file appears to be part of a larger testing or build framework for PyTorch. It provides various helper functions to interact with the PyTorch project structure and tools.",
            "total_tokens": 960,
            "latency": 3.140232,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "5414bb5f-da2b-4152-a4ca-a0bbdde563ae"
        },
        "gpt-4": {
            "file_summary": "The Python file `utils.py` provides several utility functions related to managing paths and processes for a project centered around the use of the PyTorch library. Functions in the file are used to get directory paths, detect compiler types, handle Python and C++ test binaries, manage shared libraries, and clean up coverage files (`*.gcda`). Additionally, it accommodates different compilers (clang, gcc) and handles subprocesses for invoking shell commands, including detecting compilers or running Python scripts directly.",
            "total_tokens": 973,
            "latency": 4.575037,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "1c1db321-9b14-4e35-8311-4201d5446217"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `utils.py`, is to provide utility functions for working with the PyTorch project, including getting binary folders and files, detecting compiler types, and running tests. These functions appear to be designed to support testing and development workflows, such as running Python tests and cleaning up intermediate files. The file also includes functions for interacting with the file system, running subprocesses, and handling environment variables.",
            "total_tokens": 969,
            "latency": 1.502182,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "8c2e7816-7a1c-43d6-8a47-936784e88f0b"
        },
        "gpt-4.1": {
            "file_summary": "This file provides utility functions for managing and interacting with a PyTorch project build and test environment, particularly for code coverage and testing workflows. It includes functions to locate PyTorch build directories, binaries, and shared libraries, as well as to detect the system's compiler, clean up coverage-related files, and run Python-based test scripts. The utilities rely on environment variables, subprocesses, and file system operations to abstract away the complexities of handling different test types (C++ and Python) and build setups.",
            "total_tokens": 984,
            "latency": 2.309779,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 97,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "79793cae-2768-405b-8dc6-43fcb6a19d76"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/code_coverage/package/oss/utils.py",
            "number_of_lines": 97,
            "project_name": "pytorch",
            "total_tokens": 1386,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.293384,
            "file_summary": "This Python file `utils.py` provides utility functions primarily for interacting with a PyTorch project's build and test environment. It includes functions to locate build directories, binaries, shared libraries, and external tools like LLVM. Additionally, it offers functionalities for detecting the C++ compiler type, managing code coverage `.gcda` files, and running Python tests.",
            "uuid": "3586adb7-697a-4269-afe7-8c65ff2102ba"
        },
        "gemini-2.5": {
            "file_name": "/tools/code_coverage/package/oss/utils.py",
            "number_of_lines": 97,
            "project_name": "pytorch",
            "total_tokens": 1595,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 6.609058,
            "file_summary": "This Python file provides utility functions primarily for supporting the PyTorch build and testing environment, particularly related to code coverage analysis. It includes functions to locate PyTorch source/build directories, test binaries (C++ and Python), shared libraries, and LLVM tools, as well as detecting the compiler type (Clang/GCC). Additionally, the file offers helpers for finding and cleaning up code coverage data files (`.gcda`) and executing Python tests using subprocess.",
            "uuid": "a8303f69-3903-4f62-8324-2bf681da7c6d"
        },
        "Claude3.7": {
            "file_name": "/tools/code_coverage/package/oss/utils.py",
            "number_of_lines": 97,
            "project_name": "pytorch",
            "total_tokens": 1201,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.749934,
            "file_summary": "This utility file provides functions for managing PyTorch testing and code coverage, particularly for locating binary files, detecting compiler types, and handling code coverage data files (.gcda). It contains helper functions to find PyTorch directories, run Python tests, and clean up coverage data, supporting both C++ and Python test environments.",
            "uuid": "8704c7b0-b5a7-43b9-bf40-9aee7565289d"
        },
        "mistral-small": {
            "file_name": "/tools/code_coverage/package/oss/utils.py",
            "number_of_lines": 97,
            "project_name": "pytorch",
            "total_tokens": 1251,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.537665,
            "file_summary": " The main functionality of the file \"utils.py\" is to provide utility functions for interacting with the PyTorch source code and its build system. This includes functions to get the paths of various PyTorch folders, binary files, and shared libraries, as well as functions to detect the type of compiler being used, clean up certain files, and run Python tests.",
            "uuid": "764b91fc-101d-4b54-b6c5-e1534352fcad"
        },
        "mistral-7b": {
            "file_name": "/tools/code_coverage/package/oss/utils.py",
            "number_of_lines": 97,
            "project_name": "pytorch",
            "total_tokens": 1260,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.215177,
            "file_summary": " This Python file, named \"utils.py\", provides utility functions for obtaining the paths to various folders and files related to a project, including binary folders, shared libraries, and Python scripts. It also includes functions for detecting the compiler type and cleaning up generated files. The main functionality is to support the execution of tests for a project by locating and running the appropriate binary files or scripts.",
            "uuid": "592d3f3f-685a-410d-a5f4-09f0403858dd"
        },
        "cohere-v1": {
            "file_name": "/tools/code_coverage/package/oss/utils.py",
            "number_of_lines": 97,
            "project_name": "pytorch",
            "file_summary": "The Python script provides several utility functions for managing test binaries and shared libraries in a PyTorch development environment. It helps with tasks like retrieving binary folders, detecting compiler types, running Python tests, and cleaning up GCDA files. Additionally, it offers functions to get shared library paths and LLVM tool paths, aiding in test execution and code coverage analysis.",
            "total_tokens": 1086,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.044797,
            "uuid": "27efa09c-831a-41e7-912b-bf267123c90c"
        }
    },
    "/benchmarks/inference/server.py": {
        "nova-lite-v1": {
            "file_summary": "This Python script implements a multi-process system for benchmarking the performance of a deep learning model, specifically a ResNet18 model, in a server-client architecture. The FrontendWorker process sends requests to the BackendWorker process, which performs computations on the input data and returns the results. The script measures various metrics such as latency, throughput, and GPU utilization during the process. The results are logged and can be optionally profiled using PyTorch's profiler.",
            "total_tokens": 3605,
            "latency": 1.192143,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "87f65292-753f-42a2-b746-8c9077ecde5a"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to measure the throughput and latency of requests sent to a backend process, as well as the GPU utilization. It does this by creating two worker processes: a FrontendWorker and a BackendWorker. The FrontendWorker sends requests to the BackendWorker, measures the latency and throughput of the requests, and records the GPU utilization. The BackendWorker receives requests, performs some computation, and returns the results. The results are then recorded in a CSV file. The file also includes options to download a pre-trained model checkpoint and to profile the computation using PyTorch's profiler.",
            "total_tokens": 132,
            "latency": 3.513627,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "34482dbe-a278-437b-9f2b-fb0e8d77f9cd"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file implements a performance benchmarking server using PyTorch which consists of two primary components: a FrontendWorker sending simulated inference requests and measuring metrics like latency, throughput, and GPU utilization, and a BackendWorker that processes these requests using a ResNet-18 model loaded on a GPU asynchronously. The script can profile computation, handle model compilation, and is capable of downloading the required model checkpoint automatically. It records the computed metrics to a CSV file for later analysis.",
            "total_tokens": 2824,
            "latency": 5.428997,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "5f8b3a30-4b84-4dd8-b797-bf2ac105913a"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `server.py` file is to implement a system with a frontend worker and a backend worker that interact with each other to send requests, measure latency and throughput, and utilize the GPU for processing data using PyTorch. The frontend worker sends requests to the backend worker, measures response times, and records metrics like latency and throughput. The backend worker processes requests by running computations on a model (ResNet18), returns results, and records metrics related to model loading, compilation, and GPU utilization.",
            "total_tokens": 2819,
            "latency": 1.56274,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "46bd403e-624d-4a37-870b-f2d280c13b23"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of this Python file is to implement a client-server architecture for benchmarking a ResNet18 model's performance. It consists of a FrontendWorker that sends requests and measures throughput, latency, and GPU utilization, and a BackendWorker that processes these requests using the ResNet18 model. The script also includes functionality for profiling, data handling, and metrics collection, with the results being saved to a CSV file.",
            "total_tokens": 3838,
            "latency": 2.983913,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "456f1731-210c-4122-a0ea-f5ddfb65d364"
        },
        "nova-pro-v1": {
            "file_summary": "The `server.py` file implements a client-server model using multiprocessing and multithreading to measure the performance of a machine learning model. It sends requests to a backend process, measures throughput, latency, and GPU utilization, and saves the results to a CSV file.",
            "total_tokens": 3567,
            "latency": 0.991925,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3c28f6a5-8bd7-40cb-b14f-4bc9cd531e2c"
        },
        "gpt-4o": {
            "file_summary": "The main functionality of the `server.py` file is to implement a performance benchmarking system that measures the latency, throughput, and GPU utilization for processing tensor data using a pretrained ResNet-18 model on a CUDA-enabled GPU. It uses two multiprocessing workers: a `FrontendWorker` to simulate sending requests and collect metrics, and a `BackendWorker` to load the model, perform computations, and process requests. The script integrates PyTorch, asynchronous processing, and profiling features, exporting gathered metrics in CSV format for analysis.",
            "total_tokens": 2834,
            "latency": 1.808945,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "60731035-df38-4802-ac58-72a57bdebf7b"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to simulate a client-server architecture where a frontend worker sends requests to a backend worker, which processes the requests using a ResNet18 model and returns the results. The frontend worker measures the throughput and latency of the requests, as well as the GPU utilization, and stores the metrics in a dictionary. The backend worker uses multiple threads to handle the requests concurrently.",
            "total_tokens": 2800,
            "latency": 3.85693,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e11c9295-9c46-4cf7-b7f2-260931bab2d9"
        },
        "gpt-4": {
            "file_summary": "The Python script `server.py` is designed to execute a distributed computing system involving front-end and back-end workers processing deep learning tasks. The front-end worker sends tensors to a backend worker which processes them using a ResNet model, benchmarking performance metrics such as latency and GPU utilization. The system leverages Python's multiprocessing capabilities, including queues and events for inter-process communication, as well as threading and asynchronous IO for concurrent operations, integrating numerous libraries like PyTorch and Pandas for data manipulation and model computations.",
            "total_tokens": 2818,
            "latency": 2.984128,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "510bd568-85f3-4141-8062-b041d116335b"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `server.py`, is to simulate a client-server architecture where a frontend worker sends requests to a backend worker, which processes the requests using a ResNet18 model and returns the responses. The script measures the throughput, latency, and GPU utilization of the backend worker and logs the results to a CSV file. The backend worker utilizes multi-threading and asynchronous execution to handle the requests concurrently, allowing for efficient processing of multiple requests in parallel.",
            "total_tokens": 2818,
            "latency": 1.858587,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "33deccd0-2153-46ea-8188-25bb67807db4"
        },
        "gpt-4.1": {
            "file_summary": "This file implements a benchmarking server for measuring the performance of a GPU-based PyTorch model, specifically ResNet18. It orchestrates frontend and backend worker processes: the frontend sends batches of fake image data to the backend, measures throughput, latency, and GPU utilization, while the backend loads the model, performs inference, and returns results. The script collects and saves detailed performance metrics (including optional profiling) to a CSV file after running the specified number of iterations and batch size.",
            "total_tokens": 2825,
            "latency": 1.972349,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 399,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8d93f8f5-9150-4fc8-aa92-b68e81ad3549"
        },
        "gemini-2.5-flash": {
            "file_name": "/benchmarks/inference/server.py",
            "number_of_lines": 399,
            "project_name": "pytorch",
            "total_tokens": 4005,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.699115,
            "file_summary": "This Python script simulates a client-server system to benchmark the performance of a deep learning inference task using a ResNet18 model. A `FrontendWorker` process sends requests and measures metrics like latency, throughput, and GPU utilization. The `BackendWorker` processes these requests using the model, managing data transfer and computation with threads and asyncio, and the collected performance data is saved to a CSV file.",
            "uuid": "383cd308-2551-4e34-a89f-1afa8961349d"
        },
        "gemini-2.5": {
            "file_name": "/benchmarks/inference/server.py",
            "number_of_lines": 399,
            "project_name": "pytorch",
            "total_tokens": 4356,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.522589,
            "file_summary": "This Python script implements a multi-process benchmarking system for a PyTorch ResNet model, separating tasks into a `FrontendWorker` and a `BackendWorker`. The `FrontendWorker` sends batches of inference requests, measures performance metrics like latency, throughput, and GPU utilization. The `BackendWorker` receives these requests, performs model inference asynchronously on a GPU using multiple threads and CUDA streams for efficiency, and sends results back.",
            "uuid": "7344ca1d-786f-42d9-a94f-68d3fde406e7"
        },
        "Claude3.7": {
            "file_name": "/benchmarks/inference/server.py",
            "number_of_lines": 399,
            "project_name": "pytorch",
            "total_tokens": 3826,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.573136,
            "file_summary": "This Python file implements a server for benchmarking deep learning model inference, featuring a frontend-backend architecture where FrontendWorker sends requests to BackendWorker which processes them using a ResNet18 model. The system measures performance metrics including latency, throughput, and GPU utilization while supporting features like model compilation, multi-threading, and CUDA stream management for optimized data transfer and computation.",
            "uuid": "e5c64e69-712b-4da1-8e43-bce3e27a9d75"
        },
        "mistral-small": {
            "file_name": "/benchmarks/inference/server.py",
            "number_of_lines": 399,
            "project_name": "pytorch",
            "total_tokens": 4095,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 3.574546,
            "file_summary": " The main functionality of the file \"server.py\" is to measure the throughput and latency of requests to a backend process, as well as GPU utilization. It does this by creating a FrontendWorker and BackendWorker, which communicate through multiprocessing Queues. The FrontendWorker sends requests to the BackendWorker, which processes them using a ResNet18 model, and then returns the results. The FrontendWorker measures the startup latency, average, max, and min latency, as well as throughput of the requests, and also polls nvidia-smi for GPU utilization every 100ms to record the average GPU utilization. The results are stored in a dictionary and can be written to a csv file.",
            "uuid": "01efa78a-3792-4c03-8edd-4005b01030df"
        },
        "mistral-7b": {
            "file_name": "/benchmarks/inference/server.py",
            "number_of_lines": 399,
            "project_name": "pytorch",
            "total_tokens": 4054,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.390819,
            "file_summary": " This Python file contains the implementation of two processes, FrontendWorker and BackendWorker, for measuring the performance of a deep learning model using multiple threads and GPU utilization. The FrontendWorker sends requests to the BackendWorker, measures the latency and throughput of those requests, and records GPU utilization. The BackendWorker receives tensors from the request queue, performs computation, and returns the results back to the response queue. The main functionality of the file is to test and measure the performance of a deep learning model using multiple threads and GPU utilization.",
            "uuid": "1aeef398-da1e-4369-9dd9-2b441ef5fef0"
        },
        "cohere-v1": {
            "file_name": "/benchmarks/inference/server.py",
            "number_of_lines": 399,
            "project_name": "pytorch",
            "file_summary": "The provided Python script appears to be a performance testing framework for a GPU-based deep learning model. It employs two worker processes, a frontend worker responsible for sending data requests and measuring metrics like throughput and latency, and a backend worker that processes the requests, performs computations using the model, and returns responses. The script also collects metrics like GPU utilization and execution times, and eventually saves the results in a CSV file.",
            "total_tokens": 3407,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.611011,
            "uuid": "f773f609-e454-4760-a0ba-2a21dad77783"
        }
    },
    "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `cache_debug_microbenchmarks.py` file is to measure the performance of iterating over a large number of FX (Function for eXpressive) nodes in PyTorch. It creates a large symbolic trace graph, computes its hash details, and then benchmarks the time taken to debug and print the lines of this graph using the `FxGraphHashDetails` class. The script outputs the time taken for the operation and the rate of nodes processed per second.",
            "total_tokens": 392,
            "latency": 1.047966,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "ce354316-a26e-4399-a57a-6be0b39409cd"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"cache_debug_microbenchmarks.py\" is to measure the performance of iterating over a large number of FX nodes. It creates a huge graph using the torch.fx.symbolic_trace function, and then uses the FxGraphHashDetails class to get the debug lines of the graph. The performance is measured by timing how long it takes to iterate over the debug lines of the graph a certain number of times. The results are then printed to the console.",
            "total_tokens": 105,
            "latency": 2.386932,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "dc47e968-f484-411d-887b-9d2487f0ea34"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file benchmarks the performance of iterating over nodes in a large PyTorch FX computational graph. It constructs a huge graph consisting of repeated sine operations and uses `FxGraphHashDetails` to iterate through its nodes, measuring and printing how efficiently (in nodes per second) this iteration occurs. This tool is intended primarily for debugging or analyzing PyTorch's internal FX graph handling performance.",
            "total_tokens": 338,
            "latency": 4.202449,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "f57674ca-50f7-4e41-b204-8455545857b4"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The Python file \"cache_debug_microbenchmarks.py\" measures the performance of iterating over a large symbolic graph created using Torch FX. It defines a function to create a huge graph, traces it symbolically, calculates hash details, and then measures the time taken to iterate over the graph nodes. The script outputs the time taken to iterate over the nodes and the nodes processed per second.",
            "total_tokens": 337,
            "latency": 0.786206,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "81567ada-d614-4692-ac31-ce4d4538174e"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the file \"cache_debug_microbenchmarks.py\" is to perform a microbenchmark on the performance of iterating over a large number of FX nodes in a PyTorch graph. It creates a huge graph with many repeated operations, measures the time taken to generate debug lines for this graph using FxGraphHashDetails, and reports the performance in terms of nodes processed per second.",
            "total_tokens": 395,
            "latency": 2.21454,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "98a739c0-bacf-434a-8f7c-4849c1c45d61"
        },
        "nova-pro-v1": {
            "file_summary": "The file `cache_debug_microbenchmarks.py` is designed to benchmark the performance of iterating over a large number of FX nodes generated by PyTorch's symbolic tracing, specifically measuring the time taken to process these nodes and printing the results.",
            "total_tokens": 343,
            "latency": 0.762317,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e140b212-6785-4698-84f2-e02f869044ac"
        },
        "gpt-4o": {
            "file_summary": "The file `cache_debug_microbenchmarks.py` evaluates the performance of iterating over a large number of FX graph nodes created using PyTorch's `torch.fx` symbolic tracing mechanism. It defines a function generating a computational graph with repetitive sine operations, measures the processing speed using the `FxGraphHashDetails.debug_lines()` method, and reports the time and nodes-per-second rate for benchmarking purposes. This is primarily aimed at profiling and debugging PyTorch's FX graph handling in terms of performance.",
            "total_tokens": 358,
            "latency": 1.401668,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "ab1481e0-64a2-4e0d-a1ed-fb344e3a817c"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to measure the performance of iterating over a large PyTorch FX graph, specifically the time it takes to iterate over a graph with 10,000 nodes, repeated 100 times. The file uses the `timeit` module to benchmark the execution time of the `debug_lines` method of the `FxGraphHashDetails` class, which is responsible for iterating over the graph nodes. The results are printed to the console, showing the total time taken and the number of nodes iterated per second.",
            "total_tokens": 376,
            "latency": 4.267466,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "741fad86-1a01-447b-b3d3-b1970a273931"
        },
        "gpt-4": {
            "file_summary": "The Python script \"cache_debug_microbenchmarks.py\" benchmarks the performance related to iterating and processing nodes in a large FX-graph (built using PyTorch FX) by employing the `FxGraphHashDetails.debug_lines()` method. It creates a huge computation graph where a trigonometric function (sine) is applied repeatedly (N times) to a tensor, symbolically traces it into an FX graph, and then measures the time it takes to process debug information from the graph K times, reporting the results in seconds and nodes per second. This helps in evaluating the efficiency of debug-logging mechanisms in handling large FX graphs.",
            "total_tokens": 387,
            "latency": 11.337669,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "eed912c9-ce44-4936-9da1-766f7c9f57b3"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `cache_debug_microbenchmarks.py` file is to measure the performance of iterating over a large number of FX nodes in a PyTorch graph. The script creates a huge graph with 10,000 nodes, and then uses the `timeit` module to benchmark the time it takes to iterate over these nodes 100 times. The results are printed to the console, showing the total time taken and the number of nodes processed per second.",
            "total_tokens": 362,
            "latency": 1.274868,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a9146745-3f01-4c4c-a984-bdecae065c82"
        },
        "gpt-4.1": {
            "file_summary": "The file cache_debug_microbenchmarks.py defines a microbenchmark that constructs a large torch.fx computational graph by repeatedly applying the sine operation, then measures the performance of the debug_lines() method from the FxGraphHashDetails class over many iterations. It uses timeit to record how long it takes to process a large number (N * K) of FX nodes, printing the total time taken and throughput. The primary purpose is to benchmark and debug the performance of code related to torch._inductor's handling of FX graph details.",
            "total_tokens": 365,
            "latency": 2.359589,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 33,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "54ee7a03-42e5-49d8-b1ac-4ac95cecac86"
        },
        "gemini-2.5-flash": {
            "file_name": "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py",
            "number_of_lines": 33,
            "project_name": "pytorch",
            "total_tokens": 857,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.403948,
            "file_summary": "This Python script defines a microbenchmark to measure the performance of iterating over nodes in a large `torch.fx.Graph`. It creates a graph with many operations and uses `timeit` to time the execution of generating debug lines for this graph via `FxGraphHashDetails.debug_lines()`. The script then prints the total number of nodes processed, the time taken, and the processing rate in nodes per second.",
            "uuid": "d7c6604b-1694-438c-ab3f-f7fe3025d536"
        },
        "gemini-2.5": {
            "file_name": "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py",
            "number_of_lines": 33,
            "project_name": "pytorch",
            "total_tokens": 994,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 8.038948,
            "file_summary": "This Python script benchmarks the performance of generating debug information for a large PyTorch FX graph. It specifically measures the time taken by the `debug_lines()` method of the `FxGraphHashDetails` class, which is part of PyTorch Inductor's code caching system. The script uses `timeit` to repeatedly execute this method on a synthetically generated large graph and reports the execution speed in nodes per second.",
            "uuid": "db1c8108-7615-4dc8-8a50-3167788d9c88"
        },
        "Claude3.7": {
            "file_name": "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py",
            "number_of_lines": 33,
            "project_name": "pytorch",
            "total_tokens": 385,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.7934,
            "file_summary": "This file contains a microbenchmark for measuring the performance of debug line generation in PyTorch's FX graph hashing functionality. It creates a large FX graph with 10,000 nodes, then times how quickly the `debug_lines()` method of `FxGraphHashDetails` can process these nodes, reporting the throughput in nodes per second.",
            "uuid": "ff12b37a-8bf7-4b3b-b8f1-a91ad2070a6c"
        },
        "mistral-small": {
            "file_name": "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py",
            "number_of_lines": 33,
            "project_name": "pytorch",
            "total_tokens": 410,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.473509,
            "file_summary": " The main functionality of the file \"cache_debug_microbenchmarks.py\" is to measure the performance of iterating over a large number of PyTorch FX nodes. It creates a large FX graph with a sin function applied N times, then measures the time taken to iterate over the debug lines of the graph details. The results are printed in terms of nodes per second.",
            "uuid": "ed5719b5-8ee7-4a20-9681-eb434ed3deef"
        },
        "mistral-7b": {
            "file_name": "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py",
            "number_of_lines": 33,
            "project_name": "pytorch",
            "total_tokens": 435,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.424413,
            "file_summary": " The Python file \"cache\\_debug\\_microbenchmarks.py\" defines a function \"huge\\_graph\" that uses symbolic tracing with PyTorch FX to create a large graph, and then measures the time it takes to iterate over the graph's debug lines using the FxGraphHashDetails class from PyTorch's code cache module. The main functionality of the script is to perform microbenchmarks on the debug lines of the large graph for better cache debugging.",
            "uuid": "2383c298-47e0-45a3-a1b8-beba78da2433"
        },
        "cohere-v1": {
            "file_name": "/benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py",
            "number_of_lines": 33,
            "project_name": "pytorch",
            "file_summary": "The Python script cache_debug_microbenchmarks.py benchmarks the performance of iterating over a large number of FX nodes in a graph by creating a synthetic huge graph and measuring the time it takes to execute it. The script calculates and prints the time in seconds to iterate over all the nodes and provides the performance in terms of nodes/second. This is achieved by utilizing the torch.fx and timeit modules.",
            "total_tokens": 366,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.218102,
            "uuid": "58b08541-8617-4a7b-ad40-0ff2fa51fcc2"
        }
    },
    "/torch/distributions/independent.py": {
        "nova-lite-v1": {
            "file_summary": "The `independent.py` file defines a custom `Independent` class that inherits from `torch.distributions.distribution.Distribution`. This class allows for reinterpreting some of the batch dimensions of a base distribution as event dimensions, which can be useful for reshaping the output of methods like `log_prob`. The primary purpose of this class is to facilitate the transformation of distributions, such as converting a diagonal Normal distribution to have the same shape as a Multivariate Normal distribution.",
            "total_tokens": 1461,
            "latency": 1.046534,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "8daa03ee-049f-4b5a-b2dd-c645d4e2a720"
        },
        "titan_premier": {
            "file_summary": "The file \"independent.py\" is a Python file that defines a class called \"Independent\" which is a subclass of the \"Distribution\" class from the \"torch.distributions\" module. The main functionality of the \"Independent\" class is to reinterpret some of the batch dimensions of a distribution as event dimensions, which is useful for changing the shape of the result of the \"log_prob\" method. The class takes two arguments: \"base_distribution\", which is the base distribution to reinterpret, and \"reinterpreted_batch_ndims\", which is the number of batch dimensions to reinterpret as event dimensions. The class also overrides several methods from the \"Distribution\" class, such as \"sample\", \"rsample\", \"log_prob\", \"entropy\", and \"enumerate_support\", to apply the reinterpretation of batch dimensions as event dimensions.",
            "total_tokens": 180,
            "latency": 3.97886,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "cfc7a10c-d36c-4a01-9c0c-d6d20e1844fc"
        },
        "gpt-4.5": {
            "file_summary": "This Python file defines the \"Independent\" class, a wrapper around PyTorch distributions that reinterprets some of the batch dimensions of a given distribution as event dimensions. It adjusts shape semantics (especially for methods like `log_prob`, `entropy`) to allow interchangeability with multivariate distributions. Essentially, it helps manage the dimensions and behavior of probability distributions in PyTorch for flexible distribution reshaping.",
            "total_tokens": 1214,
            "latency": 5.243362,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "7df67ff5-c8ab-45c2-9705-305ef8d3228e"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The file defines a class called \"Independent\" that extends the \"Distribution\" class in PyTorch. It allows reinterpreting some batch dimensions of a distribution as event dimensions, which is useful for changing the shape of the result of log probabilities. The class provides methods for sampling, calculating log probabilities, and handling the distribution's properties like mean, mode, and variance.",
            "total_tokens": 1185,
            "latency": 2.218926,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "58b92a4d-2554-4bde-903c-c63654adf4ce"
        },
        "Claude3.5": {
            "file_summary": "The \"independent.py\" file defines an Independent class that extends PyTorch's Distribution class. This class reinterprets some of the batch dimensions of a base distribution as event dimensions, primarily to modify the shape of the log probability output. It allows for creating distributions with different batch and event shapes while maintaining the underlying distribution's properties, which is useful for making certain distributions interchangeable in probabilistic models.",
            "total_tokens": 1566,
            "latency": 2.824686,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "c28d2148-c7a6-4ccf-b2b4-09d4bbef5276"
        },
        "nova-pro-v1": {
            "file_summary": "The `independent.py` file defines a class `Independent` that inherits from `Distribution` in PyTorch. It reinterprets some batch dimensions of a base distribution as event dimensions, primarily to change the shape of the result of `log_prob`. The class includes methods to handle sampling, log probability calculation, entropy, and other distribution properties.",
            "total_tokens": 1437,
            "latency": 1.11533,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "24c2c1c6-63cc-4990-b682-1aad99b920a3"
        },
        "gpt-4o": {
            "file_summary": "The Python file defines a custom `Independent` class that extends PyTorch's `Distribution` class. Its main functionality is to reinterpret certain batch dimensions of a base distribution as event dimensions, enabling operations like `log_prob` or `entropy` to sum over selected dimensions while maintaining compatibility with PyTorch's probabilistic modeling framework. This is particularly useful when working with distributions that require reshaping or reinterpreting batch-event relationships for advanced modeling scenarios.",
            "total_tokens": 1224,
            "latency": 1.40402,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "9074d582-c3e2-4ff3-a25e-58b5341799d7"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a custom PyTorch distribution class called `Independent`, which reinterprets some of the batch dimensions of a base distribution as event dimensions. This class allows for changing the shape of the result of the `log_prob` method, making it useful for creating diagonal distributions with the same shape as multivariate distributions. The class inherits from PyTorch's `Distribution` class and overrides several methods to accommodate the reinterpreted batch dimensions.",
            "total_tokens": 1212,
            "latency": 3.822538,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "7a217970-80d9-4bf5-b223-3fa397acd2c6"
        },
        "gpt-4": {
            "file_summary": "The Python file `independent.py` defines a class named `Independent`, which extends the functionality of a given PyTorch distribution by allowing the reinterpretation of certain batch dimensions as event dimensions, primarily to modify the output shape of the `log_prob` method. The class includes methods for calculating properties like mean, mode, variance, and entropy by leveraging analogous methods from the base distribution, while modifying returns such as log probabilities to consider new event dimensions. Essential redefinitions and validations are contained within the constructor, and methods for sampling and expansion support such adaptations to navigate between batch and event dimensions seamlessly.",
            "total_tokens": 1230,
            "latency": 2.936114,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "c2d7d38e-9f14-444a-a6b0-9a889626ed2d"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to define a class `Independent` that reinterprets some of the batch dimensions of a distribution as event dimensions, allowing for changes in the shape of the result of `log_prob`. This class is a subclass of `torch.distributions.distribution.Distribution` and is designed to work with PyTorch distributions. The `Independent` class takes a base distribution and the number of batch dimensions to reinterpret as event dimensions, and provides various methods for working with the reinterpreted distribution, including `log_prob`, `sample`, and `entropy`.",
            "total_tokens": 1234,
            "latency": 10.707086,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a20939d0-8cd7-4cba-8f44-77aea2155d77"
        },
        "gpt-4.1": {
            "file_summary": "The file defines the Independent distribution class, which is a PyTorch distribution wrapper that reinterprets a specified number of batch dimensions of a base distribution as event dimensions. This is primarily useful for adjusting how methods like log_prob sum across dimensions, making independent distributions behave structurally like multivariate ones. The class properly handles sampling, shape management, and probability computation by delegating to the base distribution and summing or reshaping results as needed.",
            "total_tokens": 1223,
            "latency": 1.392579,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 130,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "bf49c0a5-559a-4f72-b5fc-1f2329fbc1a4"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributions/independent.py",
            "number_of_lines": 130,
            "project_name": "pytorch",
            "total_tokens": 1735,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.882877,
            "file_summary": "This Python file defines the `Independent` distribution class from PyTorch's distributions module. It acts as a wrapper around a base distribution, reinterpreting a specified number of the base distribution's batch dimensions as event dimensions. This modification primarily affects the shape of the output from methods like `log_prob` and `entropy` by summing over the newly designated event dimensions.",
            "uuid": "e5e0d908-22ce-4a8f-a74c-21c3cd377963"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributions/independent.py",
            "number_of_lines": 130,
            "project_name": "pytorch",
            "total_tokens": 1696,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.391357,
            "file_summary": "This Python file defines the `Independent` distribution class within the `torch.distributions` library. The `Independent` distribution wraps a base distribution and reinterprets a specified number of its rightmost batch dimensions as event dimensions. This transformation primarily impacts methods like `log_prob` and `entropy`, which then sum over these reinterpreted dimensions.",
            "uuid": "fa54bcfb-b0f6-4366-984a-d67342ca2f75"
        },
        "Claude3.7": {
            "file_name": "/torch/distributions/independent.py",
            "number_of_lines": 130,
            "project_name": "pytorch",
            "total_tokens": 1572,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.025642,
            "file_summary": "The `independent.py` file defines the `Independent` distribution class in PyTorch that reinterprets some batch dimensions of a base distribution as event dimensions. This class is primarily useful for changing the shape of probability distributions, allowing distributions with different shapes to be used interchangeably. It wraps another distribution and modifies its batch and event shapes by moving specified dimensions from batch to event, which affects operations like log probability calculations.",
            "uuid": "a900d132-31a2-48c0-aac5-c0128426556a"
        },
        "mistral-small": {
            "file_name": "/torch/distributions/independent.py",
            "number_of_lines": 130,
            "project_name": "pytorch",
            "total_tokens": 1610,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.737352,
            "file_summary": " The main functionality of the file \"independent.py\" is to define a class named \"Independent\" which is a subclass of \"Distribution\" in PyTorch. This class reinterprets some batch dimensions of a distribution as event dimensions, which is useful for changing the shape of the result of the log_prob method. The Independent class takes a base distribution and the number of batch dimensions to be reinterpreted as event dimensions as arguments during initialization.",
            "uuid": "0026c910-967d-4b78-b7a1-11b300f797ad"
        },
        "mistral-7b": {
            "file_name": "/torch/distributions/independent.py",
            "number_of_lines": 130,
            "project_name": "pytorch",
            "total_tokens": 1603,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.126793,
            "file_summary": " The given Python file defines a custom distribution named \"Independent\" that reinterprets some batch dimensions of a base distribution as event dimensions. This allows changing the shape of the result of methods like \"log\\_prob\" and is useful for creating diagonal distributions with the same shape as multivariate distributions. The Independent distribution takes a base distribution and a number of batch dimensions to reinterpret as event dimensions as arguments.",
            "uuid": "baa04a12-e6d5-44f0-a6c5-6ca1c9e3b78a"
        },
        "cohere-v1": {
            "file_name": "/torch/distributions/independent.py",
            "number_of_lines": 130,
            "project_name": "pytorch",
            "file_summary": "The Python script defines a class called Independent that's inherited from the Distribution class. It's main functionality is to reinterpret some of the batch dimensions of a given distribution object as event dimensions, essentially altering the shape of the distribution's log probability. This is useful for tasks like changing the shape of multivariate distributions to make them interchangeable with other shapes.",
            "total_tokens": 1361,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 30,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.079486,
            "uuid": "5b0f11cd-30b1-4842-b9e0-e05fc2e94ffb"
        }
    },
    "/torch/_export/db/examples/specialized_attribute.py": {
        "nova-lite-v1": {
            "file_summary": "The given Python file defines an enumeration `Animal` and a PyTorch module `SpecializedAttribute`. The `SpecializedAttribute` class has a `forward` method that squares its input and adds a constant if the attribute `a` matches the value of `Animal.COW`; otherwise, it raises a `ValueError`. This file is primarily focused on creating a specialized neural network module with conditional behavior based on an enumerated attribute.",
            "total_tokens": 327,
            "latency": 0.926584,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "fc533905-1c30-45a3-a54d-3cb40f30d572"
        },
        "titan_premier": {
            "file_summary": "The file \"specialized_attribute.py\" is a Python file that defines an Enum class called \"Animal\" and a class called \"SpecializedAttribute\" that inherits from \"torch.nn.Module\". The \"SpecializedAttribute\" class has two attributes, \"a\" and \"b\", and a method called \"forward\" that takes an input \"x\" and performs a calculation based on the value of \"a\". The file also includes an example usage of the \"SpecializedAttribute\" class with a random input.\n\nThe main functionality of the file is to define a specialized attribute model using the PyTorch library and perform calculations based on the value of a specific attribute. The file also includes an example usage of the model with a random input.",
            "total_tokens": 157,
            "latency": 3.394791,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "aea8d237-f7b6-44fa-94cc-b3cff93a05bd"
        },
        "gpt-4.5": {
            "file_summary": "The Python file defines a PyTorch neural network module named \"SpecializedAttribute\" that includes specialized attributes (\"a\" with value \"moo\" and \"b\" with value 4). Its forward pass squares its input tensor and adds the attribute \"b\" when attribute \"a\" matches the value of \"Animal.COW\". Otherwise, it raises a ValueError.",
            "total_tokens": 298,
            "latency": 3.404345,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "24efd62a-0156-4053-b127-c6da2a18a26e"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The python file named specialized_attribute.py defines a class SpecializedAttribute which is a subclass of torch.nn.Module. This class has an __init__ method initializing attributes 'a' and 'b', and a forward method that performs a specific calculation based on the value of 'a'. An example of creating an instance of SpecializedAttribute and providing it with input data is also included in the file.",
            "total_tokens": 306,
            "latency": 1.273287,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d0cbabf1-0034-48f9-99c1-99ab679caf61"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"specialized_attribute.py\" file is to define a custom PyTorch module called SpecializedAttribute. This module performs a conditional operation in its forward method, multiplying the input by itself and adding a constant if a specific attribute matches a predefined value from an Animal enum. The file also includes an example of creating an instance of the module and preparing example input arguments for demonstration purposes.",
            "total_tokens": 362,
            "latency": 2.583491,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "81ee44ca-1682-4129-9c17-b3b781823844"
        },
        "nova-pro-v1": {
            "file_summary": "The `specialized_attribute.py` file defines a PyTorch neural network module called `SpecializedAttribute` with specialized attributes. The module performs a specific forward computation based on the value of its attribute `a`, which is compared against an enumeration value from the `Animal` enum. An example usage of the module is provided at the end of the file.",
            "total_tokens": 313,
            "latency": 0.931597,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "bff8c272-897e-40ee-9d0d-f9dd64aa65aa"
        },
        "gpt-4o": {
            "file_summary": "The file defines a PyTorch `nn.Module` named `SpecializedAttribute` that performs a simple computation in its `forward` method, squaring the input tensor and adding a constant if a class attribute matches the value of a specific `Animal` enum. The file also includes an example input tensor and instantiates the model as `model`. It demonstrates the use of enums and conditionally specialized behaviors in a neural network module.",
            "total_tokens": 311,
            "latency": 1.873342,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "69ead35e-c6a4-4904-898a-1d56093c8476"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to define a custom PyTorch module called `SpecializedAttribute` that inherits from `torch.nn.Module`. This module has a specialized attribute `a` that is compared to an enum value from the `Animal` enum, and if they match, it performs a specific operation on the input `x` in the `forward` method. The file also creates an instance of this module and passes a random tensor to its `forward` method.",
            "total_tokens": 330,
            "latency": 3.588364,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "11e56633-7077-467e-a42e-0770cc488978"
        },
        "gpt-4": {
            "file_summary": "The Python file \"specialized_attribute.py\" defines a simple PyTorch model named `SpecializedAttribute` which inherits from `torch.nn.Module`. The model checks if an internal attribute `a` matches the value assigned to an animal (in this case, \"COW\" with value \"moo\") in an enumeration; if the check passes, it computes and returns `(x*x) + b` during the forward pass, otherwise, it raises a `ValueError`. The primary purpose of this file seems to be demonstrating a PyTorch model that processes input differently based on a predefined enum condition.",
            "total_tokens": 349,
            "latency": 3.280786,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "39383193-f380-4933-8251-b93803a0ac82"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `specialized_attribute.py` file is to define a custom PyTorch module called `SpecializedAttribute` that inherits from `torch.nn.Module`. This module has a specialized attribute `a` that is compared to an enum value from the `Animal` class, and based on this comparison, it either performs a mathematical operation on the input `x` or raises a `ValueError`. The module is then instantiated and an example input is prepared using PyTorch's `torch.randn` function.",
            "total_tokens": 339,
            "latency": 4.226153,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "99de6209-745d-49fe-a386-4bb5a49ba0c0"
        },
        "gpt-4.1": {
            "file_summary": "The file defines a PyTorch neural network module named SpecializedAttribute, which includes custom attributes and logic based on an Animal enum. The module's forward method performs mathematical operations on input tensors, using the 'a' attribute and the Animal enum for conditional behavior. Additionally, the file includes sample input data and instantiates the model for potential use in testing or demonstration.",
            "total_tokens": 297,
            "latency": 1.032716,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 27,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "0bea4407-0892-419b-a81a-a19f52109543"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_export/db/examples/specialized_attribute.py",
            "number_of_lines": 27,
            "project_name": "pytorch",
            "total_tokens": 615,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.111158,
            "file_summary": "This Python file defines a PyTorch module `SpecializedAttribute`. The module's `forward` method checks if a string attribute `a` matches the value of the `Animal.COW` enum member. Based on this check, it either performs a specific tensor operation on the input or raises a `ValueError`.",
            "uuid": "eb817866-2909-4218-94c8-5644c9d41362"
        },
        "gemini-2.5": {
            "file_name": "/torch/_export/db/examples/specialized_attribute.py",
            "number_of_lines": 27,
            "project_name": "pytorch",
            "total_tokens": 658,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.595114,
            "file_summary": "This Python file defines a simple PyTorch module `SpecializedAttribute` with two attributes, `a` (string) and `b` (integer). The module's `forward` method performs a calculation (`x*x + b`) only if the attribute `a` matches the value of an `Animal` enum member (`COW`), otherwise raising an error. An example instantiation is also provided.",
            "uuid": "bc5703fd-3b79-4be3-bf33-f6a01332df5d"
        },
        "Claude3.7": {
            "file_name": "/torch/_export/db/examples/specialized_attribute.py",
            "number_of_lines": 27,
            "project_name": "pytorch",
            "total_tokens": 354,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.787917,
            "file_summary": "The file defines a PyTorch neural network module called `SpecializedAttribute` that performs a conditional operation in its forward pass, multiplying the input by itself and adding a constant value if a specific attribute matches the value of an enum. The module demonstrates specialization through attribute values, with a simple example of creating the model and preparing example arguments at the end of the file.",
            "uuid": "80721212-df45-4d39-a829-94ea6a8bfc36"
        },
        "mistral-small": {
            "file_name": "/torch/_export/db/examples/specialized_attribute.py",
            "number_of_lines": 27,
            "project_name": "pytorch",
            "total_tokens": 402,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.068464,
            "file_summary": " The main functionality of the file \"specialized_attribute.py\" is to define a custom PyTorch module named `SpecializedAttribute`. This module has two specialized attributes, `a` and `b`, where `a` is a string that can take the value of an enumerated type `Animal` and `b` is an integer. In the forward pass, the module performs a simple mathematical operation on the input tensor `x` based on the value of attribute `a`. If the value of `a` is not the value of the `COW` enumerated type, a ValueError is raised.",
            "uuid": "c0368595-2433-41da-b556-968a967e57be"
        },
        "mistral-7b": {
            "file_name": "/torch/_export/db/examples/specialized_attribute.py",
            "number_of_lines": 27,
            "project_name": "pytorch",
            "total_tokens": 401,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.416515,
            "file_summary": " The given Python file, \"specialized_attribute.py\", defines a custom PyTorch module named `SpecializedAttribute` where its attributes are represented by an Enum class `Animal`. The main functionality of this file is to create a PyTorch model with a specialized attribute (represented by the Enum class `Animal.COW`) that affects the forward pass behavior of the model. If the specialized attribute is set to `Animal.COW`, the forward pass will perform a squaring operation with an additional constant value. Otherwise, a `ValueError` is raised.",
            "uuid": "5f052d38-f2d9-4c3d-82d1-5d75c904baee"
        },
        "cohere-v1": {
            "file_name": "/torch/_export/db/examples/specialized_attribute.py",
            "number_of_lines": 27,
            "project_name": "pytorch",
            "file_summary": "The Python script defines an enumerated type Animal with one attribute COW, and a class SpecializedAttribute which inherits from torch.nn.Module. The SpecializedAttribute class has two attributes a and b, and an x parameter in its constructor. The forward method defines the computation performed by the class, squaring x and adding a bias b if a equals COW, otherwise throwing an error.",
            "total_tokens": 317,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.131671,
            "uuid": "8de1da8c-bfa4-4147-8d7d-721067bb8c19"
        }
    },
    "/torch/fx/passes/backends/cudagraphs.py": {
        "nova-lite-v1": {
            "file_summary": "The `cudagraphs.py` file provides functionality to partition a PyTorch FX graph into sub-GraphModules that can be executed within CUDA graphs, ensuring that all operations within a subgraph involve CUDA tensors only. It defines a `CudaGraphsSupport` class to determine if a node in the graph is supported for CUDA graph execution and a `partition_cudagraphs` function to partition the graph accordingly. This is useful for optimizing the execution of PyTorch models on CUDA-enabled hardware by leveraging CUDA graph capture and replay.",
            "total_tokens": 771,
            "latency": 1.39439,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "d06416ec-bef9-4c0c-9f46-39e3179eb967"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"cudagraphs.py\" is to provide support for partitioning an FX graph into sub-GraphModules that can be validly run under CUDA graphs. This is achieved through the \"partition_cudagraphs\" function, which takes a GraphModule and a list of inputs as arguments. The function first propagates fake tensors through the inputs, then creates a CudaGraphsSupport object to determine if a node is supported for CUDA graphs. It then uses a CapabilityBasedPartitioner to propose partitions based on the supported operations and fuses the partitions into a single fused graph. The file also includes imports from various torch modules and defines a CudaGraphsSupport class that extends the OperatorSupport class.",
            "total_tokens": 156,
            "latency": 3.509733,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "6a5888d7-97af-4647-b549-2facc00fc64a"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python code defines functionality for partitioning PyTorch FX computational graphs into subgraphs that can be efficiently executed as CUDA graphs. It checks node compatibility by verifying the presence of CUDA-specific operations and ensures all operands and results involve CUDA tensors. The `partition_cudagraphs` function performs partitioning based on identified CUDA-compatible nodes using the FX graph partitioning utilities provided by PyTorch.",
            "total_tokens": 644,
            "latency": 3.961588,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "aff5b770-76e9-4279-992b-c45ac9ba8991"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The provided Python file named 'cudagraphs.py' defines a class 'CudaGraphsSupport' that checks if certain nodes in a PyTorch FX graph support CUDA operations based on predefined conditions. Additionally, it includes a function 'partition_cudagraphs' that partitions an FX graph into sub-GraphModules that can be run under CUDA graphs only if all operations involve CUDA tensors. The code aims to optimize the execution of PyTorch graphs for CUDA compatibility by utilizing capabilities of GPU acceleration.",
            "total_tokens": 667,
            "latency": 1.166227,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b00fa1dc-0c37-41b5-ac4c-e828d9aa7276"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"cudagraphs.py\" file is to partition an FX graph into sub-GraphModules that can be run under CUDA graphs. It defines a CudaGraphsSupport class to determine which nodes are supported for CUDA graph execution, and provides a partition_cudagraphs function that uses this support class to propose and fuse partitions of the graph that are compatible with CUDA graph execution.",
            "total_tokens": 824,
            "latency": 3.168677,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "0553c4d9-6c50-4304-ae37-7e39420007c5"
        },
        "nova-pro-v1": {
            "file_summary": "The `cudagraphs.py` file defines a class `CudaGraphsSupport` to check if a given node in a Torch FX graph is compatible with CUDA graphs, and a function `partition_cudagraphs` to partition an FX graph into sub-GraphModules that can be run under CUDA graphs, ensuring all operations involve CUDA tensors only.",
            "total_tokens": 733,
            "latency": 1.253375,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "347d7b31-25bf-46d1-ac96-23c1eabe2d90"
        },
        "gpt-4o": {
            "file_summary": "The Python file `cudagraphs.py` defines functionality for partitioning PyTorch FX graphs into subgraphs that are compatible with CUDA graph execution. It does this by validating nodes for CUDA compatibility and ensuring that all operations within a subgraph involve CUDA tensors. The file provides the class `CudaGraphsSupport` for defining compatibility checks and the function `partition_cudagraphs` to apply these checks and generate fused graph modules.",
            "total_tokens": 652,
            "latency": 1.607722,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "22917a95-5b33-4162-9784-c4cf66f9fa11"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to support the creation of CUDA graphs in PyTorch by partitioning a given FX graph into subgraphs that can be executed on a CUDA device. The `CudaGraphsSupport` class checks if a node in the graph is supported for CUDA execution, and the `partition_cudagraphs` function uses this class to partition the graph into valid CUDA subgraphs. The partitioned subgraphs can then be executed on a CUDA device, potentially improving performance.",
            "total_tokens": 671,
            "latency": 4.891373,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "b135ea31-1d1e-4c3b-b535-c3265a5c7dd7"
        },
        "gpt-4": {
            "file_summary": "The Python file 'cudagraphs.py' defines functionality to manipulate CUDA-compatible graphs within PyTorch, specifically focusing on determining support for CUDA graph execution and partitioning FX graphs based on CUDA tensor operations. The class `CudaGraphsSupport` provides methods to check if nodes in a graph are supported based on the operations involved and whether they use CUDA tensors. The function `partition_cudagraphs` then utilizes this class to partition a given PyTorch FX graph into subgraphs that are exclusively composed of operations involving CUDA tensors, preparing them for optimized CUDA graph execution.",
            "total_tokens": 682,
            "latency": 3.760483,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "ac8bdc40-55fa-4df3-ab2d-9bbef3400d3a"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `cudagraphs.py` file is to partition a PyTorch FX graph into sub-graphs that can be executed on a CUDA device, ensuring that all operations within each sub-graph only involve CUDA tensors. The `CudaGraphsSupport` class checks if a node in the graph is supported for CUDA execution, and the `partition_cudagraphs` function uses this class to partition the graph into valid CUDA sub-graphs. The partitioning process involves propagating fake tensor information, identifying supported operations, and fusing the resulting partitions into a single graph.",
            "total_tokens": 692,
            "latency": 2.76638,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "50394444-b1c1-4eab-b7bc-37e775ec2835"
        },
        "gpt-4.1": {
            "file_summary": "The file defines functionality to partition a PyTorch FX computation graph into subgraphs that are safe to execute under CUDA graphs, ensuring that all operations within each subgraph operate only on CUDA tensors. It introduces a custom OperatorSupport class (`CudaGraphsSupport`) to determine CUDA compatibility of nodes, and provides a `partition_cudagraphs` function that uses this support logic to partition and fuse FX GraphModules accordingly. This enables safe and efficient execution of PyTorch graphs using CUDA's graph capture and replay mechanism.",
            "total_tokens": 667,
            "latency": 1.494062,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 62,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "fe7821d9-48a9-4bdd-b8ee-38c9988fb3f7"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/fx/passes/backends/cudagraphs.py",
            "number_of_lines": 62,
            "project_name": "pytorch",
            "total_tokens": 1280,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.034867,
            "file_summary": "This Python file defines utilities for partitioning PyTorch FX graphs to identify subgraphs suitable for execution under CUDA graphs. It includes a `CudaGraphsSupport` class to determine if an operation and its associated tensors are exclusively on the CUDA device. The `partition_cudagraphs` function uses this support logic to split an input graph into submodules containing only CUDA-compatible operations.",
            "uuid": "87830af2-05df-468a-bb55-2a0bac2e2720"
        },
        "gemini-2.5": {
            "file_name": "/torch/fx/passes/backends/cudagraphs.py",
            "number_of_lines": 62,
            "project_name": "pytorch",
            "total_tokens": 1523,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 10.302643,
            "file_summary": "This Python file defines logic to partition a PyTorch FX graph based on CUDA compatibility. It introduces a `CudaGraphsSupport` class to check if individual graph nodes operate exclusively on CUDA tensors. The `partition_cudagraphs` function then uses this check with FX's `CapabilityBasedPartitioner` to group consecutive CUDA-compatible operations into subgraphs, preparing the model for potential acceleration with CUDA graphs.",
            "uuid": "7114d71c-6318-43e1-98bb-91b41922dbc4"
        },
        "Claude3.7": {
            "file_name": "/torch/fx/passes/backends/cudagraphs.py",
            "number_of_lines": 62,
            "project_name": "pytorch",
            "total_tokens": 869,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.792677,
            "file_summary": "The file \"cudagraphs.py\" implements functionality for partitioning PyTorch FX graphs into sub-GraphModules that can be executed under CUDA graphs, with the main `partition_cudagraphs` function analyzing and splitting graphs based on CUDA tensor compatibility. It defines a `CudaGraphsSupport` class that determines whether nodes in the graph can be supported in CUDA graphs by checking if all tensors involved are CUDA tensors. The partitioning process uses FakeTensorProp for tensor metadata propagation and CapabilityBasedPartitioner to create valid CUDA-compatible subgraphs.",
            "uuid": "e5615b7c-0578-451a-8d7f-6f726fcd7736"
        },
        "mistral-small": {
            "file_name": "/torch/fx/passes/backends/cudagraphs.py",
            "number_of_lines": 62,
            "project_name": "pytorch",
            "total_tokens": 841,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.70145,
            "file_summary": " The main functionality of the file \"cudagraphs.py\" is to partition a PyTorch FX graph into sub-GraphModules that can be run under CUDA graphs. This is achieved by using a custom OperatorSupport class called CudaGraphsSupport to check if all operations in a subgraph involve CUDA tensors only, and then applying a CapabilityBasedPartitioner to propose partitions and fuse them together.",
            "uuid": "0f193e24-37eb-4514-8e94-9c7188b85cbb"
        },
        "mistral-7b": {
            "file_name": "/torch/fx/passes/backends/cudagraphs.py",
            "number_of_lines": 62,
            "project_name": "pytorch",
            "total_tokens": 873,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.383342,
            "file_summary": " The given Python file, named \"cudagraphs.py\", defines a custom OperatorSupport class named `CudaGraphsSupport` for the Torch FX framework. This class is used to check if a given node in a Torch FX graph can be executed under CUDA. The `partition_cudagraphs` function takes an FX graph and its inputs as arguments and partitions the graph into sub-GraphModules that can be validly run under CUDA graphs by checking if all operations in the subgraph involve only CUDA tensors.",
            "uuid": "dd35b547-7ebc-48f2-8f4a-6099e229e286"
        },
        "cohere-v1": {
            "file_name": "/torch/fx/passes/backends/cudagraphs.py",
            "number_of_lines": 62,
            "project_name": "pytorch",
            "file_summary": "The Python script cudagraphs.py is designed to partition a Torch FX graph into sub-GraphModules that can run under CUDA graphs. It utilizes the CapabilityBasedPartitioner class and a custom OperatorSupport class to determine which nodes are supported. The partition_cudagraphs function performs the graph partitioning based on CUDA capability and returns the fused graph.",
            "total_tokens": 689,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.029425,
            "uuid": "6ddb61ff-93ec-4ba1-903e-69c7bed439ae"
        }
    },
    "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the `handlers.py` file is to provide a function `get_subprocess_handler` that creates an instance of `SubprocessHandler` from the `torch.distributed.elastic.multiprocessing` module. This function is used to configure and initialize a subprocess handler for distributed training, allowing for the specification of the entry point, arguments, environment variables, and standard output/error paths.",
            "total_tokens": 363,
            "latency": 0.863668,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "7f0b3ae2-0a5f-4790-aa2b-546b214f1ea3"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the \"handlers.py\" file is to provide a function called \"get_subprocess_handler\" that returns an instance of the \"SubprocessHandler\" class. This function takes in parameters such as the entrypoint, arguments, environment variables, stdout, stderr, and local rank id, and uses them to initialize a \"SubprocessHandler\" object. The \"SubprocessHandler\" class is imported from the \"torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler\" module.",
            "total_tokens": 113,
            "latency": 2.64282,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a2b04200-014e-4b28-929b-97cb849fb212"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file defines a function `get_subprocess_handler` that returns a configured instance of the `SubprocessHandler` class imported from `torch.distributed.elastic`. This handler facilitates managing subprocesses by specifying an entry point, arguments, environment variables, standard output/error redirections, and a local rank ID. Overall, the primary purpose of this file is to abstract and simplify subprocess creation and management specifically tailored for distributed multiprocessing tasks.",
            "total_tokens": 351,
            "latency": 3.37534,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "4928c280-97d3-42cb-be91-b1110415a9c5"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The `handlers.py` file defines a function `get_subprocess_handler` that creates an instance of `SubprocessHandler` from the `torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler` module. The function takes parameters like entrypoint, arguments, environment variables, stdout and stderr paths, and local rank id, and returns an initialized SubprocessHandler. The main functionality of this file is to provide a wrapper function for creating and configuring subprocess handlers for distributed processing using PyTorch.",
            "total_tokens": 366,
            "latency": 1.279778,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "a0e7a5c3-ad98-4ecd-8359-af6f9618ff28"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"handlers.py\" file is to provide a function called `get_subprocess_handler` that creates and returns a `SubprocessHandler` object. This function serves as a wrapper or factory method for initializing a `SubprocessHandler` with specified parameters, likely used for managing subprocesses in a distributed computing environment.",
            "total_tokens": 385,
            "latency": 2.206299,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3a531dfa-a45f-4eee-a14c-d9662c6a6469"
        },
        "nova-pro-v1": {
            "file_summary": "The `handlers.py` file provides a function to instantiate a `SubprocessHandler` object from the `torch.distributed.elastic.multiprocessing.subprocess_handler` module, which is likely used for managing subprocesses in a distributed computing environment. The main functionality is encapsulated in the `get_subprocess_handler` function, which takes several parameters to configure the subprocess handler.",
            "total_tokens": 359,
            "latency": 1.126139,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3ca3f7f2-e057-45d4-a093-480bd70c4a71"
        },
        "gpt-4o": {
            "file_summary": "The file `handlers.py` defines a utility function `get_subprocess_handler` that initializes and returns an instance of `SubprocessHandler` from PyTorch's distributed elastic module. This handler is designed to manage subprocess execution with specified entrypoints, arguments, environmental variables, and I/O configurations. Its purpose is likely to support distributed or parallel computation in machine learning workflows.",
            "total_tokens": 338,
            "latency": 1.297442,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "9bd59dfc-1200-4eee-8196-4f25e70cebb8"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file, `handlers.py`, is to provide a function `get_subprocess_handler` that returns an instance of `SubprocessHandler` from the `torch.distributed.elastic.multiprocessing.subprocess_handler` module. This function takes several parameters, including the entry point, arguments, environment variables, and output streams, to configure the subprocess handler. The returned `SubprocessHandler` instance is likely used to manage a subprocess in a distributed computing environment.",
            "total_tokens": 367,
            "latency": 3.762742,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "4420c1f3-16bc-4ab2-9cd6-84ffb50696b8"
        },
        "gpt-4": {
            "file_summary": "The 'handlers.py' file defines a function `get_subprocess_handler` used to set up and return a `SubprocessHandler` instance, which is responsible for handling subprocesses in a parallel computing environment. This function specifies several parameters such as the entrypoint, arguments, environment variables, outputs for `stdout` and `stderr`, and a `local_rank_id`, which are then utilized to initialize the `SubprocessHandler` object. The main purpose of this module is to facilitate managing subprocesses specifically tailored for distributed computing tasks, likely within the context of using the PyTorch framework.",
            "total_tokens": 383,
            "latency": 2.70314,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "73654639-b5da-4626-bf69-95021a766951"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `handlers.py` file is to provide a function `get_subprocess_handler` that returns a `SubprocessHandler` object, which is used to manage subprocesses in a distributed computing environment. This function takes in several parameters, including the entrypoint, arguments, environment variables, and output streams, to configure the subprocess handler. The file appears to be part of a larger distributed computing framework, likely PyTorch, and is designed to facilitate the creation and management of subprocesses in a distributed setting.",
            "total_tokens": 377,
            "latency": 1.445281,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "29496fa8-3cde-4c3c-a0c3-c8fa200e8f91"
        },
        "gpt-4.1": {
            "file_summary": "The file defines a utility function get_subprocess_handler that creates and returns an instance of SubprocessHandler, given parameters such as the entrypoint, arguments, environment variables, output/error streams, and a local rank ID. It serves as a thin wrapper over the SubprocessHandler class imported from torch.distributed.elastic, likely to standardize or simplify subprocess handler instantiation. The file is part of a distributed or multiprocessing framework, potentially used for launching and managing distributed worker processes.",
            "total_tokens": 359,
            "latency": 1.49869,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 31,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "43834595-f473-4075-a176-69ae10edd8b8"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py",
            "number_of_lines": 31,
            "project_name": "pytorch",
            "total_tokens": 881,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.497721,
            "file_summary": "This Python file defines a factory function `get_subprocess_handler` which simplifies the creation of `SubprocessHandler` objects. It takes configuration parameters like entrypoint, arguments, environment variables, and standard stream redirection paths as input. The primary purpose is to instantiate and return a `SubprocessHandler` configured for managing a worker subprocess within the PyTorch Elastic distributed training framework.",
            "uuid": "3c6fc4e3-edf1-417a-864c-e78feddf6154"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py",
            "number_of_lines": 31,
            "project_name": "pytorch",
            "total_tokens": 912,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.509651,
            "file_summary": "This Python file defines a factory function `get_subprocess_handler`. This function creates and returns an instance of the `SubprocessHandler` class. It initializes the handler with parameters specifying the subprocess to manage, such as its entrypoint, arguments, environment, and standard I/O paths.",
            "uuid": "3975d9cf-d30c-48a8-81fd-63e580aa173d"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py",
            "number_of_lines": 31,
            "project_name": "pytorch",
            "total_tokens": 382,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.246432,
            "file_summary": "This file provides a utility function `get_subprocess_handler` that creates and returns a `SubprocessHandler` instance from PyTorch's distributed elastic multiprocessing module. The function serves as a wrapper around the `SubprocessHandler` constructor, making it easier to create handlers for managing subprocesses in a distributed computing environment.",
            "uuid": "223dbf12-258f-4dfe-9787-c03b6967040d"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py",
            "number_of_lines": 31,
            "project_name": "pytorch",
            "total_tokens": 424,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.648295,
            "file_summary": " The main functionality of the file \"handlers.py\" is to provide a single function `get_subprocess_handler` which returns an instance of `SubprocessHandler` from the `torch.distributed.elastic.multiprocessing.subprocess_handler` module. This function takes in various parameters such as entrypoint, arguments, environment variables, standard output and error paths, and local rank ID, and sets them for the `SubprocessHandler` instance.",
            "uuid": "37ad55d8-63fd-4cd6-b9b3-134e70b8adb4"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py",
            "number_of_lines": 31,
            "project_name": "pytorch",
            "total_tokens": 484,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.762431,
            "file_summary": " This Python file, named \"handlers.py\", exports a function `get_subprocess_handler` that creates and returns an instance of `SubprocessHandler`. The `SubprocessHandler` is a class imported from `torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler` module. The function takes as arguments an entrypoint, a tuple of arguments, a dictionary of environment variables, and strings for standard output and error streams. It initializes a new `SubprocessHandler` object with these arguments and returns it. The main functionality of this file is to provide a way to spawn new subprocesses with customized configurations for distributed processing using Torch's elastic multiprocessing.",
            "uuid": "280c0266-42e6-4ea7-84f1-8c3832c0ec13"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py",
            "number_of_lines": 31,
            "project_name": "pytorch",
            "file_summary": "The handlers.py file appears to be a module that provides a function called get_subprocess_handler, which returns an instance of SubprocessHandler, taking several arguments such as entrypoint, args, env, stdout, stderr, and local_rank_id. This function likely facilitates the creation of subprocesses with specified configurations.",
            "total_tokens": 347,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 0.972949,
            "uuid": "94f6c8d4-6273-4733-a04b-01e1a846bc54"
        }
    },
    "/torch/_inductor/autoheuristic/autoheuristic.py": {
        "nova-lite-v1": {
            "file_summary": "The `autoheuristic.py` file defines a framework for collecting data, learning heuristics, and generating code based on heuristics for optimizing kernel choices in PyTorch. It includes classes for handling feedback, managing metadata, and implementing heuristic selection algorithms. The main functionality is to facilitate the collection of performance data for different kernel choices and use this data to inform and improve heuristic decision-making for selecting the most efficient kernel operations.",
            "total_tokens": 3071,
            "latency": 1.091094,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "f808fcad-b4f4-41cb-9e47-51459673a4f6"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"autoheuristic.py\" is to provide a framework for collecting data, learning a heuristic, and generating the heuristic to code. The framework is implemented through the AutoHeuristic class, which allows one to collect data for a choice and provides methods to get the chosen option based on the value of autoheuristic_use. The class also includes methods to get the top k choices, get collected feedback, and save data to a log file. Additionally, the file includes a subclass of AutoHeuristic called AutoHeuristicSelectAlgorithm, which is specifically designed for kernel choice selection.",
            "total_tokens": 128,
            "latency": 3.189935,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "eb6250ae-8234-412f-ad37-a87507092dfe"
        },
        "gpt-4.5": {
            "file_summary": "The given Python file implements an \"AutoHeuristic\" framework integrated into PyTorch, designed to collect data for heuristics, learn decision models (such as regression trees), and select optimal choices for certain operations, particularly GPU-related computations. Its main goal is to dynamically select optimal kernel and operation choices by learning from collected performance feedback data and contexts, creating heuristics to improve PyTorch's inductor performance. This functionality is encapsulated primarily in the \"AutoHeuristic\" and \"AutoHeuristicSelectAlgorithm\" classes, capable of decision-making and logging feedback for heuristic optimization.",
            "total_tokens": 2620,
            "latency": 6.00396,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "a0a4646e-1b85-4774-92af-6f66559dc118"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the \"autoheuristic.py\" file is to implement classes and methods for the AutoHeuristic framework in PyTorch. The file provides classes such as \"LocalFeedback\", \"InconsistentMetadata\", \"AutoHeuristic\", and \"AutoHeuristicSelectAlgorithm\" that allow for collecting data, learning heuristics, selecting algorithms, and providing feedback based on choices. It also includes methods for saving and logging data, handling preconditions, and making decisions based on learned heuristics within the AutoHeuristic framework.",
            "total_tokens": 2613,
            "latency": 1.23924,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "c9b20c10-9022-4e2b-9e57-84ebc94d12da"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"autoheuristic.py\" file is to implement an AutoHeuristic framework for collecting data, learning heuristics, and making decisions in PyTorch's inductor module. It provides classes and methods for managing choices, feedback, and context in auto-tuning scenarios, allowing for data collection, heuristic learning, and decision-making based on learned models. The file also includes specialized classes for algorithm selection and supports both local and global feedback mechanisms.",
            "total_tokens": 3421,
            "latency": 4.375328,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "90e36867-306f-4241-bb66-ed6452acae1f"
        },
        "nova-pro-v1": {
            "file_summary": "The `autoheuristic.py` file defines a framework for collecting data, learning heuristics, and generating heuristic code. It includes classes for providing local feedback, handling metadata inconsistencies, and implementing the core AutoHeuristic functionality, which allows for data collection and heuristic decision-making. Additionally, it provides a specialized subclass for selecting algorithms based on collected data.",
            "total_tokens": 3058,
            "latency": 1.119958,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e1224499-1dd2-40a4-89a6-ce68a8041190"
        },
        "gpt-4o": {
            "file_summary": "The `autoheuristic.py` file implements a framework for AutoHeuristic, which is used to collect data, train heuristics (such as regression trees), and make decisions in scenarios with multiple choices. It provides mechanisms for logging feedback on choices, managing metadata consistency, and applying learned heuristics to select optimal decisions or kernel algorithms, particularly within a GPU-based environment in PyTorch's inductor module. Additionally, it includes custom functionality for algorithm selection and feedback registration for measuring kernel performance.",
            "total_tokens": 2603,
            "latency": 1.614293,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d5e8c0ca-b094-4daa-8d0d-46e804b1aaa6"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of the file is to implement the AutoHeuristic class, which is a framework for collecting data, learning a heuristic, and generating the heuristic to code. The class allows for collecting data, learning a heuristic, and making decisions based on the learned heuristic. It also provides methods for getting the chosen option, getting the top k choices, and saving data. Additionally, the file defines a subclass AutoHeuristicSelectAlgorithm, which is specifically designed for kernel choice selection.",
            "total_tokens": 2607,
            "latency": 4.456353,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "6f708fb9-5602-4929-8e90-3fca0bb475d1"
        },
        "gpt-4": {
            "file_summary": "The Python file defines a module named 'autoheuristic' which facilitates the collection of data, learning, and deployment of heuristics for decision-making in computational tasks, likely in a machine learning or neural network context using PyTorch. It includes classes for handling local feedback, handling inconsistent metadata exceptions, and the primary AutoHeuristic class which allows data collection for heuristic training. Additionally, there is an AutoHeuristicSelectAlgorithm class, a subclass of AutoHeuristic, tailored for kernel choice selection in computational tasks, supporting functionality to dynamically adjust decisions based on feedback and predefined conditions.",
            "total_tokens": 2622,
            "latency": 4.525094,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d936ff89-c576-43c1-a8e1-9ebd86b6974c"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file is to implement an AutoHeuristic framework, which allows for the collection of data, learning of a heuristic, and generation of the heuristic to code. The framework is designed to work with PyTorch and utilizes a learned heuristic controller to make decisions based on collected data. The file includes classes such as `AutoHeuristic` and `AutoHeuristicSelectAlgorithm` that provide methods for collecting feedback, making choices, and saving data to a log file.",
            "total_tokens": 2611,
            "latency": 1.662358,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0a3db584-218c-4927-84a6-6e25c72d9827"
        },
        "gpt-4.1": {
            "file_summary": "The file autoheuristic.py implements a framework for collecting runtime feedback, training, and applying learned heuristics\u2014such as regression trees\u2014for choice selection in PyTorch's Inductor backend. It provides mechanisms to record and serialize data about algorithmic choices and their associated feedback (e.g., runtime or performance), and then allows learned models to guide future choices, with facilities for caching, device-specific logging, and context augmentation. Additionally, a specialized subclass supports gathering feedback specifically for kernel or algorithm selection scenarios, integrating with autotuning infrastructure.",
            "total_tokens": 2611,
            "latency": 1.699277,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 316,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "c0e28e52-a1b2-4769-a030-c13221e095a9"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/autoheuristic/autoheuristic.py",
            "number_of_lines": 316,
            "project_name": "pytorch",
            "total_tokens": 3670,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.626552,
            "file_summary": "This Python file defines the `AutoHeuristic` class, a framework designed to collect data for training and using learned heuristics within PyTorch Inductor. It captures context, available choices, and performance feedback for specific operations, logging this data to a file per device. A specialized subclass, `AutoHeuristicSelectAlgorithm`, applies this framework specifically to the task of selecting optimal kernel algorithms.",
            "uuid": "ffa88b0f-db7d-4148-a88e-76cd32a43978"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/autoheuristic/autoheuristic.py",
            "number_of_lines": 316,
            "project_name": "pytorch",
            "total_tokens": 3527,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.232708,
            "file_summary": "This file defines the `AutoHeuristic` framework within PyTorch Inductor, designed to automate decision-making processes like kernel selection. It enables the collection of performance data associated with different computational choices and their context. This collected data can then be used to learn and apply heuristics, allowing the system to make optimized, data-driven choices instead of relying solely on default fallbacks.",
            "uuid": "da39f20b-68f4-4cb0-9050-16d5dd76031f"
        },
        "Claude3.7": {
            "file_name": "/torch/_inductor/autoheuristic/autoheuristic.py",
            "number_of_lines": 316,
            "project_name": "pytorch",
            "total_tokens": 3432,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.173636,
            "file_summary": "This file implements the AutoHeuristic framework for PyTorch's inductor compiler, which collects data about different algorithm choices, learns heuristics (regression trees) to make optimal selections, and applies these learned decisions during compilation. The framework includes mechanisms for feedback collection, choice selection based on context, and persistence of collected data for training heuristics across different GPU models. Additionally, it provides specialized functionality for algorithm selection through the AutoHeuristicSelectAlgorithm subclass that integrates with PyTorch's kernel selection system.",
            "uuid": "aaadeebc-3c1b-4423-887e-a31ea3d81681"
        },
        "mistral-small": {
            "file_name": "/torch/_inductor/autoheuristic/autoheuristic.py",
            "number_of_lines": 316,
            "project_name": "pytorch",
            "total_tokens": 3531,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.880084,
            "file_summary": " The main functionality of the file \"autoheuristic.py\" is to provide a framework for collecting data, learning a heuristic, and generating the heuristic to code. The AutoHeuristic class is used to collect data, which can then be used to train a heuristic. The LearnedHeuristicController class is used to make decisions based on the trained heuristic. The LocalFeedback class is used to provide feedback for a given choice. The InconsistentMetadata exception is thrown when the metadata stored in a file does not match the metadata that would be stored if the file didn't exist. The AutoHeuristicSelectAlgorithm class is a subclass of AutoHeuristic that is specifically for kernel choice selection.",
            "uuid": "81e65a60-5460-4ce5-8297-441344d82f23"
        },
        "mistral-7b": {
            "file_name": "/torch/_inductor/autoheuristic/autoheuristic.py",
            "number_of_lines": 316,
            "project_name": "pytorch",
            "total_tokens": 3494,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.562306,
            "file_summary": " The given Python file, named \"autoheuristic.py\", is a part of the PyTorch AutoHeuristic framework. It defines a class named `AutoHeuristic` that allows collecting data and learning a heuristic for making choices based on given context. The framework supports collecting feedback for each choice using a `LocalFeedback` function and saving the collected data to a file. The file also includes classes for handling inconsistent metadata and exceptions, as well as a subclass `AutoHeuristicSelectAlgorithm` for kernel choice selection.",
            "uuid": "f4086d62-54ae-4b62-b7d9-9fd4a9a0862c"
        },
        "cohere-v1": {
            "file_name": "/torch/_inductor/autoheuristic/autoheuristic.py",
            "number_of_lines": 316,
            "project_name": "pytorch",
            "file_summary": "The code defines a framework called AutoHeuristic for collecting data, learning heuristics, and generating code for decision-making. It provides classes for managing feedback, choosing options based on learned heuristics, and selecting algorithms. The main purpose is to enable autotuning and kernel selection, with support for GPU-specific heuristics, while storing data for training is also implemented.",
            "total_tokens": 2923,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.339607,
            "uuid": "1d70fcfa-1eb3-4591-b149-c743a54f0092"
        }
    },
    "/torch/nn/modules/instancenorm.py": {
        "nova-lite-v1": {
            "file_summary": "The `instancenorm.py` file defines various instance normalization layers for different dimensions (1D, 2D, and 3D) using PyTorch. These layers include standard instance normalization and lazy instance normalization, which defers the initialization of certain parameters until the first forward pass. The normalization process involves computing the mean and variance for each instance separately and normalizing the input data accordingly.",
            "total_tokens": 5634,
            "latency": 1.285546,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e014dab9-5311-4213-ada9-941b9cdb0d26"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file is to define the InstanceNorm1d, InstanceNorm2d, and InstanceNorm3d classes, which are used to apply instance normalization to input data. The classes take in various parameters such as num_features, eps, momentum, affine, and track_running_stats, and use these parameters to normalize the input data. The classes also include methods for checking input dimensions and handling input with no batch dimension. Additionally, the file defines LazyInstanceNorm1d, LazyInstanceNorm2d, and LazyInstanceNorm3d classes, which are similar to the InstanceNorm classes but with lazy initialization of the num_features argument.",
            "total_tokens": 150,
            "latency": 4.026034,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "17021db6-d10b-4bc1-ad3d-72d2dde7fec2"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python code implements Instance Normalization modules (InstanceNorm1d, InstanceNorm2d, InstanceNorm3d, and their lazy initialization variants) for PyTorch. These normalization modules standardize inputs across each instance individually, often used in neural networks for style transfer and visual tasks. It includes options for learnable affine transformations and tracking running statistics for evaluation.",
            "total_tokens": 5154,
            "latency": 5.951022,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "1bd07f41-8874-43c7-8cdb-10dfef68d6ab"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file, 'instancenorm.py', defines classes for instance normalization operations in PyTorch. It includes classes for InstanceNorm1d, InstanceNorm2d, InstanceNorm3d, LazyInstanceNorm1d, LazyInstanceNorm2d, and LazyInstanceNorm3d. These classes implement instance normalization calculations and handle input validation and lazy initialization of parameters. Instance normalization is applied on different-dimensional inputs to normalize the mean and standard deviation per dimension separately, providing functionality similar to batch normalization but operating on individual samples or channels.",
            "total_tokens": 5170,
            "latency": 1.594334,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8a069a95-b905-4d78-b851-88091e5de129"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"instancenorm.py\" file is to implement Instance Normalization layers for 1D, 2D, and 3D inputs in PyTorch. It defines classes for InstanceNorm1d, InstanceNorm2d, InstanceNorm3d, and their lazy counterparts, which normalize input features across spatial dimensions independently for each channel and each sample in a batch. The file also includes functionality for handling running statistics, affine transformations, and various input dimensions.",
            "total_tokens": 6029,
            "latency": 4.269072,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "023dcf8a-c009-4666-bd12-5a3e28b8163a"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `instancenorm.py` file is to implement instance normalization layers for 1D, 2D, and 3D inputs in PyTorch, including both standard and lazy initialization versions. These layers normalize the input data per instance and per dimension, optionally with learnable affine parameters.",
            "total_tokens": 5619,
            "latency": 1.204012,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "e28f457c-18a0-4302-bb4c-377db13fbb71"
        },
        "gpt-4o": {
            "file_summary": "The `instancenorm.py` file implements various types of Instance Normalization layers in PyTorch, including 1D, 2D, and 3D, as well as their lazy initialization counterparts. These layers normalize inputs per instance and per feature dimension, allowing for improved convergence and stylization in neural networks. They support options like learnable affine parameters, tracking running statistics, and handling unbatchable inputs for flexibility in training and inference.",
            "total_tokens": 5171,
            "latency": 1.917756,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "63db8500-9b88-4199-9e24-b19734f40682"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this file is to implement instance normalization layers in PyTorch, specifically for 1D, 2D, and 3D inputs. Instance normalization is a technique used in deep learning to normalize the input data for each instance (i.e., each sample in a batch) separately, rather than normalizing across the entire batch. The file defines several classes, including `InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d`, and their lazy initialization counterparts, which inherit from a base class `_InstanceNorm`. These classes implement the instance normalization operation and provide options for learnable affine parameters, tracking running statistics, and more.",
            "total_tokens": 5202,
            "latency": 6.514301,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "f0ebafb5-3196-4a15-aaf5-3ba61f517c35"
        },
        "gpt-4": {
            "file_summary": "The provided Python file, \"instancenorm.py\", defines several classes that implement different versions of Instance Normalization (InstanceNorm1d, InstanceNorm2d, InstanceNorm3d) and their respective lazy initialization counterparts in a neural network using the PyTorch library. These layers are used to apply normalization on a per-instance basis (different from Batch Normalization which normalizes across the entire batch), suitable for both training and evaluation modes, with optional affine (learned parameters) and tracking of running statistics. The classes are built upon abstract base classes which ensure implementation of necessary methods and use PyTorch functionalities to adjust the data distribution of inputs for stabilizing and speeding up the training of neural networks.",
            "total_tokens": 5203,
            "latency": 6.72573,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "19fb9205-f6c4-4092-8da5-03bad88e6437"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of this Python file is to implement instance normalization layers for PyTorch, including `InstanceNorm1d`, `InstanceNorm2d`, and `InstanceNorm3d`, which apply instance normalization to 2D, 3D, and 5D inputs, respectively. These layers can be used for tasks such as image and video processing, and they provide options for learnable affine parameters and tracking running statistics. Additionally, the file also defines lazy initialization versions of these layers, `LazyInstanceNorm1d`, `LazyInstanceNorm2d`, and `LazyInstanceNorm3d`, which infer the `num_features` argument from the input size.",
            "total_tokens": 5203,
            "latency": 2.246698,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a4e5924a-3b05-40fd-9b5b-7acaa8e51a6c"
        },
        "gpt-4.1": {
            "file_summary": "This file implements PyTorch's instance normalization layers for 1D, 2D, and 3D inputs, including both standard and \"lazy\" variants that infer the number of features at runtime. Instance normalization normalizes input data across each instance and channel, with options for learnable affine parameters and tracking running statistics. The file defines the core logic, input shape validation, and module inheritance structure necessary for these layers to work within the PyTorch framework.",
            "total_tokens": 5173,
            "latency": 2.926177,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 472,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d242765b-e66b-4480-95ab-a0d0d5fffcea"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/modules/instancenorm.py",
            "number_of_lines": 472,
            "project_name": "pytorch",
            "total_tokens": 5987,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.480252,
            "file_summary": "This Python file defines PyTorch modules for Instance Normalization, a technique that normalizes input data channel-wise for each sample independently. It provides implementations for 1D, 2D, and 3D inputs, including standard and \"lazy\" versions that infer the number of features upon the first forward pass. These modules are commonly used in deep learning, particularly for tasks like image style transfer.",
            "uuid": "a74f3206-1483-4f86-a6ba-0bbc6f9df6e2"
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/modules/instancenorm.py",
            "number_of_lines": 472,
            "project_name": "pytorch",
            "total_tokens": 5977,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.308857,
            "file_summary": "This Python file defines PyTorch modules for applying Instance Normalization to 1D, 2D, and 3D input tensors. It includes a base class `_InstanceNorm` containing the core normalization logic and specific implementations (`InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d`) for different input dimensions. Additionally, it provides \"lazy\" versions (`LazyInstanceNorm1d`, `LazyInstanceNorm2d`, `LazyInstanceNorm3d`) that automatically infer the number of features from the input data during the first forward pass.",
            "uuid": "25f139d6-5ff0-4ed4-92fe-d859e9d88874"
        },
        "Claude3.7": {
            "file_name": "/torch/nn/modules/instancenorm.py",
            "number_of_lines": 472,
            "project_name": "pytorch",
            "total_tokens": 6026,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.680685,
            "file_summary": "The file \"instancenorm.py\" implements Instance Normalization layers for PyTorch, which normalize features across spatial dimensions independently for each channel and each sample in a batch. It defines classes for 1D, 2D, and 3D instance normalization (InstanceNorm1d, InstanceNorm2d, InstanceNorm3d) along with their lazy initialization counterparts, all inheriting from a base _InstanceNorm class that contains the core normalization functionality.",
            "uuid": "51133126-0410-4df6-9671-f986d8587308"
        },
        "mistral-small": {
            "file_name": "/torch/nn/modules/instancenorm.py",
            "number_of_lines": 472,
            "project_name": "pytorch",
            "total_tokens": 6370,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 6.637072,
            "file_summary": " The given Python file is a part of the PyTorch library and defines various Instance Normalization (InstanceNorm) modules. InstanceNorm is a type of normalization layer that normalizes the input data for each object in a mini-batch separately. The main functionality of the file is to provide different InstanceNorm modules for different input dimensions, such as 1D, 2D, and 3D. These modules apply Instance Normalization to the input data and optionally learn affine parameters (scale and bias) if the 'affine' argument is set to True. The file also includes LazyInstanceNorm modules, which lazily initialize the number of features based on the input data. The main functionality of the file can be summarized as follows:\n\n1. The file provides various InstanceNorm modules for different input dimensions, such as InstanceNorm1d, InstanceNorm2d, and InstanceNorm3d.\n2. These modules apply Instance Normalization to the input data and optionally learn affine parameters (scale and bias) if the 'affine' argument is set to True.\n3. The file also includes LazyInstanceNorm modules, which lazily initialize the number of features based on the input data.\n4. The modules in the file can be used for normalizing input data in various applications, such as image processing, natural language processing, and 3D modeling.\n5. The modules in the file can be used in both training and evaluation modes, and they can track running estimates of the mean and variance if the 'track_running_stats' argument is set to True.\n\nOverall, the main functionality of the file is to provide various InstanceNorm modules for different input dimensions and to enable users to apply Instance Normalization to their input data with optional affine parameters and running statistics tracking.",
            "uuid": "2574dc9f-0d8b-40f0-83bd-55ea0c744490"
        },
        "mistral-7b": {
            "file_name": "/torch/nn/modules/instancenorm.py",
            "number_of_lines": 472,
            "project_name": "pytorch",
            "total_tokens": 6130,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.499907,
            "file_summary": " This Python file defines a set of Instance Normalization layers for PyTorch, including 1D, 2D, and 3D versions. These layers apply normalization over each channel separately for each object in a mini-batch, as described in the paper \"Instance Normalization: The Missing Ingredient for Fast Stylization\". The normalization is performed using learnable parameters if `affine` is set to True, and the mean and standard deviation are calculated per-dimension for each object in the mini-batch. The layers support lazy initialization of the `num_features` argument, and they track running statistics by default.",
            "uuid": "e17b1360-9c91-468a-9404-a378579bcb26"
        },
        "cohere-v1": {
            "file_name": "/torch/nn/modules/instancenorm.py",
            "number_of_lines": 472,
            "project_name": "pytorch",
            "file_summary": "The code defines several instance normalization classes for PyTorch: InstanceNorm1d, LazyInstanceNorm1d, InstanceNorm2d, LazyInstanceNorm2d, InstanceNorm3d, and LazyInstanceNorm3d. These classes apply instance normalization to inputs of different dimensions, either lazily initializing or explicitly specifying the number of features.",
            "total_tokens": 5324,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.744566,
            "uuid": "070323d7-90b9-4e19-9b37-e2676fb29e14"
        }
    },
    "/tools/linter/adapters/flake8_linter.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"flake8_linter.py\" file is to serve as a wrapper around the Flake8 linter tool, providing additional features such as retrying failed commands, mapping specific lint codes to custom severity levels, and outputting results in JSON format. It allows users to specify custom severity levels for certain lint codes, retry failed commands, and provides verbose logging options. The script takes one or more file paths as input and outputs the linting results in JSON format.",
            "total_tokens": 3814,
            "latency": 1.241572,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "2951c144-e3c2-4ca1-ba01-ec035782e0df"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to serve as a wrapper for the Flake8 linter. It allows users to specify the severity level of certain linting errors, retry timed out Flake8 commands, and set the path to additional Flake8 plugins. The script takes a list of filenames as input, runs Flake8 on them, and outputs the linting results in JSON format. It also provides the documentation URL for each linting error. The script uses the argparse module for command-line argument parsing and the subprocess module to run Flake8. It also includes a regular expression pattern to parse the output of Flake8 and extract the relevant information. The script is designed to be used as a standalone command-line tool.",
            "total_tokens": 160,
            "latency": 4.052166,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3085cdd0-73d1-457d-bf31-0543e20d811f"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file acts as a wrapper around the \"flake8\" linter tool, executing Python linting commands on specified files, parsing linting results, categorizing them by severity (error, warning, advice, or disabled), and associating error codes with their respective documentation URLs. It supports additional functionalities such as configurable retry logic, overriding default severity for specific error codes, and enhancing logging output for troubleshooting purposes. It reports linting issues in a structured JSON format.",
            "total_tokens": 3166,
            "latency": 5.117224,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "7fbaccbf-a813-4012-a4a1-b26496ded7a0"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file \"flake8_linter.py\" is a script that serves as a wrapper for the Flake8 linter. It defines classes like `LintSeverity` and `LintMessage` for handling linting results, provides functions for running Flake8 commands, determining severity of lint issues, and checking files for linting errors based on defined severity levels. The main functionality of the script is to parse user input to customize linting severity levels, run Flake8 on specified files, and output linting results in JSON format.",
            "total_tokens": 3157,
            "latency": 1.50193,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "fc8e1b9e-f3b6-4d09-9b05-126df87ad761"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"flake8_linter.py\" file is to serve as a wrapper for the Flake8 linter, providing a custom interface to run Flake8 on Python files and process its output. It parses Flake8 results, categorizes issues by severity, and outputs lint messages in a structured JSON format. The script also includes features like retrying on timeouts and mapping custom severities to specific Flake8 codes.",
            "total_tokens": 3818,
            "latency": 3.841842,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "8d314de5-289d-40ba-817d-3346597af302"
        },
        "nova-pro-v1": {
            "file_summary": "The `flake8_linter.py` script is a wrapper for the Flake8 linter that processes Python files, captures linting results, and outputs them in a structured JSON format. It allows customization of severity levels for specific lint codes and retries failed linting commands.",
            "total_tokens": 3772,
            "latency": 0.963202,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a57b1fe6-938b-4722-9a25-ba76cdbfdb44"
        },
        "gpt-4o": {
            "file_summary": "The `flake8_linter.py` script is a Flake8 wrapper utility designed to lint Python files by identifying potential code issues and returning detailed messages including file paths, line numbers, error codes, and descriptions. It supports configurable retry mechanisms for timed out linting processes, maps lint error codes to severities (error, warning, or advice), and provides links to relevant documentation for specific issues. Additionally, it handles custom configurations for Flake8 plugins and logs outputs in a structured JSON format for further processing.",
            "total_tokens": 3174,
            "latency": 4.97613,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "cd231c21-81d4-4ec8-a32b-05ead8bdad5d"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to serve as a wrapper for the Flake8 linter, which checks Python code for style and syntax errors. The script takes in command-line arguments, including file paths to lint, and outputs linting results in JSON format. It also allows for customization of linting severity and retries for timed-out Flake8 commands.",
            "total_tokens": 3128,
            "latency": 3.70374,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0cc54beb-d5bc-4724-aac3-9657016ed44e"
        },
        "gpt-4": {
            "file_summary": "The provided Python file, named \"flake8_linter.py,\" serves as a custom wrapper for the flake8 linting tool. It enhances the linting process by allowing retries on failures, custom severity levels for linting issues, and providing additional documentation links for specific linting error codes. The main functionality includes parsing command-line arguments to configure linting options, executing the flake8 command on specified files, and handling and formatting lint-related output based on severity and retries configuration.",
            "total_tokens": 3148,
            "latency": 2.938107,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "7a21ad1c-ea23-4477-abd5-efc0e66754e3"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of this Python file is to serve as a wrapper for the flake8 linter, allowing for customizable severity levels and retries for timed-out commands. It takes in a list of filenames to lint, checks them using flake8, and outputs lint messages in JSON format, including information such as the file path, line number, and severity of the issue. The script also supports additional plugins and allows for verbose logging and customizable severity mappings.",
            "total_tokens": 3149,
            "latency": 1.834428,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "c48893f5-d2ae-44a6-a053-cef5eed89245"
        },
        "gpt-4.1": {
            "file_summary": "The file flake8_linter.py is a command-line Python script that wraps the flake8 static code analysis tool, allowing users to lint Python files while customizing plugin paths, error severities, and retries for timeouts. It parses and formats flake8 output into structured messages with additional metadata (such as documentation URLs and severity levels) and outputs the results as JSON objects. The script is designed to be robust, with error handling, logging, and support for environment-based plugin configuration.",
            "total_tokens": 3170,
            "latency": 1.615534,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 372,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d887e914-a1f9-471a-8b61-a2a0a37b2d4e"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/linter/adapters/flake8_linter.py",
            "number_of_lines": 372,
            "project_name": "pytorch",
            "total_tokens": 4538,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.034994,
            "file_summary": "This Python script is a wrapper around the `flake8` linter, designed to execute it on specified files. It parses the standard output of the `flake8` command using a regular expression, mapping the detected issues to structured `LintMessage` objects. The script assigns custom severities based on issue codes and includes documentation links before outputting the results as JSON.",
            "uuid": "4cb0bbb0-60ca-42af-a2ef-3ce218fe43cf"
        },
        "gemini-2.5": {
            "file_name": "/tools/linter/adapters/flake8_linter.py",
            "number_of_lines": 372,
            "project_name": "pytorch",
            "total_tokens": 4370,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.425343,
            "file_summary": "This Python script serves as a wrapper around the `flake8` linter, executing it on specified Python files with support for custom plugin paths and retries on timeouts. It parses the raw output from `flake8` using regular expressions, classifying each reported issue into severity levels like error, warning, or advice based on the issue code. Finally, the script formats each lint message into a structured JSON object and prints it to standard output.",
            "uuid": "eee6f0c8-ef75-455d-8eed-c48912b8207f"
        },
        "Claude3.7": {
            "file_name": "/tools/linter/adapters/flake8_linter.py",
            "number_of_lines": 372,
            "project_name": "pytorch",
            "total_tokens": 3805,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.893136,
            "file_summary": "This Python file implements a wrapper for the Flake8 linter that analyzes Python code for style and error issues. It parses Flake8 output, categorizes issues by severity (error, warning, advice), provides documentation links for error codes, and outputs results in a structured JSON format. The script includes retry logic for timed-out linting operations and allows customization of severity levels for different error codes.",
            "uuid": "9a61f8cb-afb3-4182-842a-8765783a4fde"
        },
        "mistral-small": {
            "file_name": "/tools/linter/adapters/flake8_linter.py",
            "number_of_lines": 372,
            "project_name": "pytorch",
            "total_tokens": 4275,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.253482,
            "file_summary": " The main functionality of the file \"flake8_linter.py\" is to serve as a wrapper for the Flake8 linter tool. It provides a command-line interface to lint Python files with Flake8, allowing users to specify custom severity levels for certain codes, set the number of retries for timed-out Flake8, and configure the Flake8 plugins path. The script then runs Flake8 on the specified files, parses the output, and prints the results in JSON format.",
            "uuid": "bc8b6a89-ec92-4875-97cd-e73d46206ad9"
        },
        "mistral-7b": {
            "file_name": "/tools/linter/adapters/flake8_linter.py",
            "number_of_lines": 372,
            "project_name": "pytorch",
            "total_tokens": 4256,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.226978,
            "file_summary": " This Python file is a script for running Flake8, a Python linter, with custom configurations and severity mappings. It allows users to specify a list of filenames to lint, as well as optional arguments for Flake8 plugins path, severity mappings, and number of retries for timed-out commands. The script also provides verbose logging and handles errors and timeouts gracefully.",
            "uuid": "9f7841cb-501a-4c05-89ca-39575fc2d826"
        },
        "cohere-v1": {
            "file_name": "/tools/linter/adapters/flake8_linter.py",
            "number_of_lines": 372,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, named flake8_linter.py, is a wrapper for the Flake8 tool, enhancing it with features like customizable severity levels for code issues, support for plugins, and a JSON output format. It allows users to specify custom severity mappings for specific issue codes and provides an option to retry timed-out Flake8 commands.\n\nThe script defines various regular expressions and functions to process the output of the Flake8 command and structure it into structured JSON messages, which are then printed to the console.",
            "total_tokens": 3686,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.761865,
            "uuid": "b04711de-c9f9-4ed5-9cc9-83c52db7481e"
        }
    },
    "/torch/_functorch/benchmark_utils.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"benchmark_utils.py\" file is to provide utilities for benchmarking GPU utilization and the percentage of time spent on specific operations (like matrix multiplication and convolution) in PyTorch models. It includes functions to profile the execution of a given function, dump the profiling results into a Chrome trace file, and compute the GPU utilization metrics from the trace data. The file also includes helper functions to synchronize GPU operations and filter specific events from the trace.",
            "total_tokens": 1956,
            "latency": 1.282652,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "d67e8ed6-1dc3-424e-a045-7de6fb97dc72"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"benchmark_utils.py\" is to provide a set of functions for benchmarking the GPU utilization and the percentage of time spent on matmul and convolution operations. The file includes functions for synchronizing the GPU, dumping the chrome trace, getting the chrome trace events, sorting the GPU events, getting the duration of the GPU events, getting the sorted GPU mm conv events, computing the GPU utilization, and benchmarking the GPU utilization. The file also includes a function for creating a directory if it does not exist.",
            "total_tokens": 113,
            "latency": 2.75112,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "c6732d3a-cc7b-44c8-b01d-547c108ce07a"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file contains utilities to benchmark GPU utilization when running PyTorch functions, specifically measuring total GPU usage and the percentage of time spent on matrix multiplication (matmul) and convolution operations. It leverages the PyTorch profiler to collect performance metrics as Chrome trace files and computes GPU utilization statistics from these trace outputs. The main functionality is encapsulated in the `benchmark_utilization()` method, which automates profiling, data extraction, and utilization analysis.",
            "total_tokens": 1633,
            "latency": 4.035805,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "40f8516e-124d-4f97-815f-3f9767841b01"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the 'benchmark_utils.py' file is to provide functions for benchmarking GPU utilization and the percentage of time spent on matrix multiplication and convolution operations using PyTorch profiling tools. The file includes functions to dump Chrome trace data of running a given function, analyze the trace data to compute GPU utilization metrics, and benchmark GPU utilization and operation percentages based on the trace data. Additionally, it offers a function 'benchmark_utilization' that combines these features to benchmark GPU utilization and operation percentages of a specified function with given inputs.",
            "total_tokens": 1645,
            "latency": 1.397907,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "87afcb2d-58b6-4785-8235-45d64511fa6c"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"benchmark_utils.py\" file is to provide utilities for benchmarking and profiling Python functions, particularly those using PyTorch and CUDA. It includes functions for generating Chrome trace files, analyzing GPU events, calculating GPU utilization, and measuring the time spent on matrix multiplication and convolution operations. The file also offers a high-level benchmarking function that combines these utilities to assess the performance of given functions with GPU operations.",
            "total_tokens": 2095,
            "latency": 5.199457,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "08a24cd1-ddc7-4ace-be6b-d8c00f28effa"
        },
        "nova-pro-v1": {
            "file_summary": "The `benchmark_utils.py` file provides utilities for benchmarking GPU utilization and the percentage of time spent on matrix multiplication and convolution operations using PyTorch's profiler. It generates Chrome trace files and computes relevant metrics based on the recorded events.",
            "total_tokens": 1914,
            "latency": 1.144369,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a96390f5-4390-4f27-a6c4-59b4079c4741"
        },
        "gpt-4o": {
            "file_summary": "This Python file provides utilities for benchmarking GPU utilization and analyzing the performance of PyTorch functions based on Chrome trace outputs generated by the PyTorch profiler. It allows users to measure total GPU usage, particularly focusing on operations like matrix multiplication (matmul) and convolution, and saves detailed traces of function executions for further analysis. The main functions include `dump_chrome_trace` for creating trace files, `compute_utilization` for calculating GPU utilization from traces, and `benchmark_utilization`, which integrates these functionalities for end-to-end benchmarking.",
            "total_tokens": 1649,
            "latency": 2.774241,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b60824c8-04e3-4b34-b7a7-db7e29bdc83f"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to benchmark the GPU utilization and time spent on matrix multiplication and convolution operations of a given function `f` with input `input` using the PyTorch profiler. The file provides functions to dump Chrome trace files, compute GPU utilization, and benchmark utilization. The `benchmark_utilization` function is the main entry point, which runs the function `f` with input `input` and returns the GPU utilization and time spent on matrix multiplication and convolution operations.",
            "total_tokens": 1643,
            "latency": 5.376667,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "3620f55e-cdfb-46d3-8223-9d6d0aa25863"
        },
        "gpt-4": {
            "file_summary": "The Python file `benchmark_utils.py` provides functionality for profiling and measuring the performance of PyTorch functions on GPUs. It includes utilities to generate and analyze Chrome trace files that detail GPU utilization and percentages of time spent on matrix multiplication (matmul) and convolution operations. Key functions in the file include `dump_chrome_trace` to record the trace, `compute_utilization` to compute overall GPU utilization and time spent on specific operations from the trace, and `benchmark_utilization` which combines these functionalities to evaluate and report performance metrics for provided function calls.",
            "total_tokens": 1650,
            "latency": 3.018811,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "bfd6757b-568d-40a3-a21a-47f5cff50047"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `benchmark_utils.py`, is to benchmark and analyze the GPU utilization and performance of PyTorch functions, specifically measuring the time spent on matrix multiplication and convolution operations. The file provides functions to dump Chrome trace files for profiling, compute GPU utilization, and benchmark the utilization of given functions. The `benchmark_utilization` function is the primary entry point, which runs a given function with specified inputs, generates a Chrome trace, and computes the GPU utilization and percentage of time spent on key operations.",
            "total_tokens": 1652,
            "latency": 1.894796,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "486aa712-7abb-4ffd-aaa6-0edd85bf9eab"
        },
        "gpt-4.1": {
            "file_summary": "The file benchmark_utils.py provides utility functions for benchmarking PyTorch functions, specifically measuring and analyzing GPU utilization and the percentage of time spent on matrix multiplication (matmul) and convolution operations using PyTorch's profiler and Chrome trace outputs. It automates running a function under profiling with warmups, exports traces for visualization, parses these traces, and computes statistics on GPU and operation-specific utilization. The main interface is benchmark_utilization, which orchestrates profiling, trace file management, and utilization computation for a given function and input.",
            "total_tokens": 1647,
            "latency": 1.501664,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 232,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "bea97307-7a58-4ce5-b87e-af9ec73090f3"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_functorch/benchmark_utils.py",
            "number_of_lines": 232,
            "project_name": "pytorch",
            "total_tokens": 2613,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 4.232077,
            "file_summary": "This Python file provides utilities for benchmarking PyTorch functions, specifically focusing on GPU performance analysis. It uses the PyTorch profiler to generate Chrome trace files while executing a target function. The code then parses these trace files to calculate overall GPU utilization and the percentage of time spent on matrix multiplication and convolution operations.",
            "uuid": "bf5b7dd7-8b69-4b08-a9b4-2e94247e4484"
        },
        "gemini-2.5": {
            "file_name": "/torch/_functorch/benchmark_utils.py",
            "number_of_lines": 232,
            "project_name": "pytorch",
            "total_tokens": 2236,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.107243,
            "file_summary": "This Python file provides utilities for benchmarking PyTorch functions, focusing on GPU performance analysis. It uses the PyTorch profiler to generate Chrome trace files detailing the execution timeline of operations. The core functionality involves running a given function, capturing its performance trace, and then analyzing this trace to calculate metrics like overall GPU utilization and the percentage of time spent specifically on matrix multiplication and convolution kernels.",
            "uuid": "9e2e68b2-d8af-4d40-8b3c-d61a8c71fd17"
        },
        "Claude3.7": {
            "file_name": "/torch/_functorch/benchmark_utils.py",
            "number_of_lines": 232,
            "project_name": "pytorch",
            "total_tokens": 2070,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.668329,
            "file_summary": "The benchmark_utils.py file provides utilities for profiling and benchmarking Python functions that use PyTorch, particularly focusing on GPU utilization. It contains functions to generate Chrome trace files of PyTorch operations, analyze GPU compute events, and calculate metrics such as overall GPU utilization and time spent on matrix multiplication and convolution operations.",
            "uuid": "a7ca1697-377f-4c2b-8c19-8747c50533ac"
        },
        "mistral-small": {
            "file_name": "/torch/_functorch/benchmark_utils.py",
            "number_of_lines": 232,
            "project_name": "pytorch",
            "total_tokens": 2220,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 4.052446,
            "file_summary": " The main functionality of the file \"benchmark_utils.py\" is to profile and analyze the performance of a given PyTorch function. It does this by running the function multiple times, recording the execution time, and generating a Chrome trace file. The file then processes this trace file to compute GPU utilization and the percentage of time spent on matrix multiplication and convolution operations. The results are returned as a tuple containing GPU utilization and the percentage of time spent on matrix multiplication and convolution.",
            "uuid": "fe50c2aa-17f4-4352-a379-b3824d81d8d0"
        },
        "mistral-7b": {
            "file_name": "/torch/_functorch/benchmark_utils.py",
            "number_of_lines": 232,
            "project_name": "pytorch",
            "total_tokens": 2211,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.889123,
            "file_summary": " The main functionality of the file \"benchmark\\_utils.py\" is to provide functions for benchmarking GPU utilization and the percentage of time spent on matrix multiplication (mm) and convolution operations during the execution of a given Python function using PyTorch profiler. The file includes functions to dump chrome traces, extract and process GPU events, and compute GPU utilization and mm-conv utilization from the chrome traces.",
            "uuid": "b06cd809-2c87-4f76-bb73-4ac70647a66d"
        },
        "cohere-v1": {
            "file_name": "/torch/_functorch/benchmark_utils.py",
            "number_of_lines": 232,
            "project_name": "pytorch",
            "file_summary": "The Python script provides utilities for benchmarking and analyzing GPU utilization while executing functions. It helps measure the time spent on GPU computations, including matmul and convolution operations, and exports the profiling data to a Chrome trace file. The key functions enable dumping Chrome traces, extracting relevant events, and computing GPU utilization metrics.",
            "total_tokens": 1848,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.061243,
            "uuid": "c7a63854-0d41-4a12-b701-eaf1df5ee1e6"
        }
    },
    "/torch/xpu/_utils.py": {
        "nova-lite-v1": {
            "file_summary": "The `_utils.py` file defines a function `_get_device_index` that retrieves the index of a device from a given input, which can be a `torch.device` object, a Python integer, or `None`. The function supports optional and CPU device inputs, and it defaults to using `torch._utils._get_device_index` if the input device is not a recognized XPU device. This utility function is intended to help manage device indices in PyTorch, particularly for XPU devices.",
            "total_tokens": 653,
            "latency": 1.234972,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "9d65d75c-7f46-4156-8da7-6888ce30804f"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the _utils.py file is to provide a function called _get_device_index, which takes a device object, an optional boolean flag, and an allow_cpu boolean flag as input. This function returns the device index from the given device, which can be a torch.device object, a Python integer, or None. The function checks the type of the device and returns the device index accordingly. If the device is a torch.device object, it checks if it is a XPU device and returns the device index if it is. If the device is a Python integer, it returns the integer as is. If the device is None, it returns the current default XPU device if the optional flag is True. If the device is not a XPU device and the allow_cpu flag is False, it raises a ValueError. If the device is a XPU device without a specified index, it returns the current default XPU device if the optional flag is True. If the device is a XPU device with a specified index, it returns the index. If the device is a CPU device and the allow_cpu flag is True, it returns -1. If the device is a CPU device and the allow_cpu flag is False, it raises a ValueError. If the device is not a XPU or CPU device, it raises a ValueError. The function also checks if the scripting mode is enabled and returns the device index accordingly. If the scripting mode is enabled and the device is a XPU device, it returns the device index. If the scripting mode is enabled and the device is not a XPU device, it raises a ValueError. If the scripting mode is not enabled and the device is a XPU device, it calls the _get_device_index function from torch._utils module to get the device index. If the scripting mode is not enabled and the device is not a XPU device, it raises a ValueError.",
            "total_tokens": 406,
            "latency": 8.348855,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "93b37c67-4962-48ed-800b-f7c162f422e5"
        },
        "gpt-4.5": {
            "file_summary": "This Python script defines a helper function \"_get_device_index\", which retrieves the device index (integer identifier) from various forms of device information (integer, string or torch.device) specifically tailored for \"xpu\" or optionally \"cpu\" devices. It includes additional validation to ensure only allowed device types (\"xpu\"/\"cpu\") are processed. The function calls an existing PyTorch utility \"_torch_get_device_index\" for the primary logic, extending its handling by adding specific checks for XPU device types.",
            "total_tokens": 591,
            "latency": 4.948428,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "bfea1498-3184-40e7-a30c-3eb0209f935e"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the `_utils.py` file is to define a function `_get_device_index` that retrieves the device index based on the input `device`, which can be a Torch device object, a Python integer, or `None`. The function handles different cases like checking if the device is an XPU device, handling CPU devices, and returning the device index accordingly. If the input device is not an integer or None, the function delegates the task to `_torch_get_device_index` function imported from `torch._utils`.",
            "total_tokens": 592,
            "latency": 1.401319,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "b2156053-888c-4bcb-a3c1-32783c289dad"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"_utils.py\" file is to provide a utility function called \"_get_device_index\" that retrieves the device index from various input types (torch.device object, Python integer, or None). This function is designed to handle XPU (presumably a custom or extended processing unit) and CPU devices, with options to allow CPU devices and handle optional device specifications. The file also imports and utilizes a similar function from torch._utils for compatibility and extended functionality.",
            "total_tokens": 664,
            "latency": 2.907892,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "780b4a62-60d1-4478-a4e7-e69ceee2e766"
        },
        "nova-pro-v1": {
            "file_summary": "The `_utils.py` file contains a single function, `_get_device_index`, which determines the device index for a given PyTorch device. This function accepts a device specified as a `torch.device` object, a Python integer, or `None`, and returns the corresponding device index. The function also handles optional parameters to allow for CPU devices and default device selection.",
            "total_tokens": 622,
            "latency": 1.074379,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "a034e03b-1010-4575-8513-73cedd6e035e"
        },
        "gpt-4o": {
            "file_summary": "The `_utils.py` file defines a function `_get_device_index` that retrieves the device index based on the input, which can be a `torch.device` object, a Python integer, or `None`. It includes checks for compatibility with XPU devices, optionally allowing CPU devices and handling default XPU devices when the device index is unspecified. The function leverages PyTorch utilities to streamline device management in a Torch-based computational framework.",
            "total_tokens": 577,
            "latency": 2.113496,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "80004e42-c2f2-44f2-bf15-375a7a8aef58"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to provide a utility function `_get_device_index` that retrieves the device index from a given device, which can be a PyTorch device object, a Python integer, or `None`. The function handles different types of devices, including XPU and CPU, and raises errors if the device is not valid. The function is designed to be used in a PyTorch environment and provides flexibility in handling different device types and indices.",
            "total_tokens": 586,
            "latency": 3.637648,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "968f4e6e-c417-4127-9a14-1517b32f9bd4"
        },
        "gpt-4": {
            "file_summary": "The Python file `_utils.py` provides a utility function `_get_device_index()`, which determines the index of a device specified by the `device` parameter that could either be a torch.device object, Python integer, or None. The function supports conditions where the device can optionally be None or a CPU device, returning the device index or `-1` (in the case of CPU when allowed), based on the passed parameters. Any incorrect device type raises a ValueError, while for valid XPUs and CPUs, it appropriately handles device indices, default behaviors, or directly returns the passed integer index.",
            "total_tokens": 604,
            "latency": 2.486237,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "ab5a0d6a-f2ee-443d-b2bc-c6696f3b9dad"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the `_utils.py` file is to provide a utility function `_get_device_index` that retrieves the device index from a given device, which can be a torch.device object, a Python integer, or None. This function is designed to handle XPU (Xilinx Programmable Unit) devices and optionally allows CPU devices, returning the device index or a default value based on the input and optional parameters. The function includes error checking to ensure the device type is valid and raises a ValueError if an invalid device is provided.",
            "total_tokens": 600,
            "latency": 1.383291,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "2a7e1bb7-5d59-4cde-9237-32fcfa953f41"
        },
        "gpt-4.1": {
            "file_summary": "The file defines a utility function, _get_device_index, which standardizes the extraction of a device index from various types of device representations (integer, string, torch.device) specifically for XPU (and optionally CPU) devices in PyTorch. It also includes compatibility logic for scripting and delegates to PyTorch's internal device index handling when necessary. This function helps ensure robust and consistent device management for code targeting XPU backends.",
            "total_tokens": 576,
            "latency": 1.300469,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 40,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "10f8e210-46bb-4b37-8f5d-d1de686489a1"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/xpu/_utils.py",
            "number_of_lines": 40,
            "project_name": "pytorch",
            "total_tokens": 836,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.082826,
            "file_summary": "This Python file defines a function `_get_device_index` that determines the integer index of a specified device. It handles various input types for the device, including integers, strings, `torch.device` objects, and `None`. The function primarily targets 'xpu' devices but can optionally accept 'cpu', ultimately delegating the core logic to `torch._utils._get_device_index`.",
            "uuid": "dec9b3b0-35ad-4910-a2fa-14be4a9d6170"
        },
        "gemini-2.5": {
            "file_name": "/torch/xpu/_utils.py",
            "number_of_lines": 40,
            "project_name": "pytorch",
            "total_tokens": 909,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 4.8692,
            "file_summary": "This Python file defines a utility function `_get_device_index` designed to extract the numerical device index from various input types like integers, strings, or `torch.device` objects. It specifically handles logic for XPU devices, optionally allows CPU devices (returning -1), and falls back to an internal PyTorch function for general device index retrieval. The function ensures the input represents a valid device and returns its corresponding integer index.",
            "uuid": "85cee9dd-76cc-494b-aa7e-55787807fb65"
        },
        "Claude3.7": {
            "file_name": "/torch/xpu/_utils.py",
            "number_of_lines": 40,
            "project_name": "pytorch",
            "total_tokens": 643,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.199599,
            "file_summary": "This file provides a utility function `_get_device_index` that extracts the device index from various input types (integers, strings, torch.device objects) specifically handling XPU devices in PyTorch. The function validates device types, allowing CPU devices when specified, and returns the appropriate device index, extending PyTorch's existing device index functionality to support XPU devices.",
            "uuid": "1ba764b6-47c9-4413-a7eb-96cb38e04572"
        },
        "mistral-small": {
            "file_name": "/torch/xpu/_utils.py",
            "number_of_lines": 40,
            "project_name": "pytorch",
            "total_tokens": 715,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.413628,
            "file_summary": " The main functionality of the file \"_utils.py\" is to provide a utility function named \"_get_device_index\" which retrieves the device index from a given device object, integer, or None. This function accepts a device, an optional boolean flag, and an allow_cpu boolean flag. It returns the device index if the device is an XPU device, the device itself if it's an integer, and the current default XPU device if the device is None and the optional flag is True. The function raises a ValueError if the device type is not 'xpu' or 'cpu' when allow_cpu is True, or if the device type is not 'xpu' when allow_cpu is False.",
            "uuid": "ba3e7349-8c57-4051-ac26-99ca24eeaa2c"
        },
        "mistral-7b": {
            "file_name": "/torch/xpu/_utils.py",
            "number_of_lines": 40,
            "project_name": "pytorch",
            "total_tokens": 663,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.151513,
            "file_summary": " This Python file, named \"_utils.py\", defines a custom function named \"_get_device_index\". The main functionality of this file is to provide a utility function for handling device indices in PyTorch. The function takes a device argument, which can be an integer, a string representing a device type, or a PyTorch device object. It returns the device index based on the given device argument, with support for XPU and CPU devices.",
            "uuid": "bfce6099-8506-4e06-b27b-529c914547eb"
        },
        "cohere-v1": {
            "file_name": "/torch/xpu/_utils.py",
            "number_of_lines": 40,
            "project_name": "pytorch",
            "file_summary": "The _utils.py file defines a function _get_device_index which retrieves the device index from various input types such as integers, strings, or torch.device objects. It primarily helps with device management, ensuring the correct device index is obtained for further computations. Additionally, it has some torch utility functions imported, although they are not shown in the provided code snippet.",
            "total_tokens": 588,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.139536,
            "uuid": "5ed5043d-f7a8-4022-ada7-a90a6f95cac0"
        }
    },
    "/functorch/examples/ensembling/parallel_train.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"parallel_train.py\" file is to demonstrate how to parallelize the training of multiple neural network models on a single GPU using PyTorch's eager-mode `vmap` function. It showcases the implementation of a two-layer MLP classifier, a training step function, and the use of `vmap` to train multiple models in parallel. The file also highlights the need for functional APIs, functional optimizers, and a functional gradient API to achieve similar results as JAX.",
            "total_tokens": 1636,
            "latency": 1.161126,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "686dffb0-cada-463a-8ac5-606559daba07"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the file \"parallel_train.py\" is to demonstrate the parallel training of multiple models using eager-mode vmap in PyTorch. The script generates spiral data, defines a two-layer MLP classifier, and implements a training step function. It then trains a single model and verifies that the loss decreases. Next, it initializes multiple models and trains them in parallel using vmap, showing that the loss continues to decrease. The script also highlights the need for random data subsets to prevent overfitting in the ensemble models. The file serves as an example of achieving similar functionality as JAX in PyTorch using additional features such as the NN module functional API, functional optimizers, and a functional grad API.",
            "total_tokens": 152,
            "latency": 3.402441,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "2a281f91-e39d-4353-80ee-051c831bc6d8"
        },
        "gpt-4.5": {
            "file_summary": "The provided Python file demonstrates how to parallelize training of multiple neural network models simultaneously using PyTorch's functional APIs and eager-mode vectorization (`vmap`). It creates a synthetic spiral dataset, implements a simple multi-layer perceptron (MLP) classifier with teacher-forced gradient calculations, manually optimizes parameters via a functional implementation of SGD, and showcases training multiple models concurrently on the same data points. The method is inspired by a similar functionality originally shown using JAX, aiming to facilitate parallel model ensembling and rapid experimentation on a single hardware device.",
            "total_tokens": 1466,
            "latency": 5.254515,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d6628875-3e3b-42d0-8555-57a5af5e20f0"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The main functionality of the \"parallel_train.py\" file is to demonstrate the parallel training of multiple models using PyTorch and the eager-mode vmap functionality. The file includes the creation of spirals for the dataset, definition of a two-layer MLP model and a training function, and showcases the training process for multiple models in parallel while highlighting the importance of avoiding overfitting by using different subsets of data for each model. The file also mentions the adaptations made from a tutorial on Model Ensembling with JAX by Will Whitney and highlights the additional functionalities used in PyTorch compared to JAX for achieving similar results.",
            "total_tokens": 1479,
            "latency": 1.252011,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "cdd2bec7-b633-49bd-a357-06f67518bba3"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"parallel_train.py\" file is to demonstrate parallel training of multiple neural network models using PyTorch's vmap functionality. It implements a two-layer MLP classifier for a spiral dataset and showcases how to train multiple models simultaneously using vectorized operations. The file also discusses potential improvements and limitations of the approach, such as the need for data subsampling to prevent overfitting across the ensemble.",
            "total_tokens": 1736,
            "latency": 2.909472,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "43e8c215-1162-4e08-841d-a17e4a9c971b"
        },
        "nova-pro-v1": {
            "file_summary": "The `parallel_train.py` script demonstrates how to use PyTorch's `vmap` function to parallelize the training of multiple neural network models on a single device. It creates synthetic spiral data, defines a two-layer MLP, and shows how to train multiple models in parallel while highlighting the need for functional APIs and optimizers.",
            "total_tokens": 1602,
            "latency": 0.991466,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "634ae0d2-54e6-4906-af0d-f9424fc4b769"
        },
        "gpt-4o": {
            "file_summary": "The `parallel_train.py` script demonstrates how to use PyTorch's functional APIs, such as `vmap` and `functional_call`, to parallelize the training of multiple neural network models on the same dataset, inspired by an analogous technique in JAX. It includes functionalities to generate spiral data, define an MLP classifier, and implement gradient-based training (manually mimicking a stochastic gradient descent optimizer), along with showcasing both sequential and parallelized training processes. The script concludes by highlighting PyTorch's current limitations, such as the lack of native functional optimizers, while providing a foundation for further experiments with ensemble models trained on diverse subsets of data.",
            "total_tokens": 1486,
            "latency": 2.635299,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "1a9e27ea-b05e-4129-8088-e58670629abc"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to demonstrate the use of PyTorch's `vmap` function to parallelize the training of multiple neural network models on a single GPU. The file defines a two-layer MLP classifier and a loss function, and then uses `vmap` to train multiple instances of the model in parallel on a dataset of spirals. The file also explores the use of `vmap` to train multiple models on different subsets of the data to prevent overfitting.",
            "total_tokens": 1461,
            "latency": 4.053443,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "0d65e75f-4e75-4ddb-9c74-3730b25f1438"
        },
        "gpt-4": {
            "file_summary": "The Python file `parallel_train.py` is designed to demonstrate parallel training of neural network models on a single GPU using PyTorch, inspired by similar techniques in JAX. It specifically uses a two-layer MLP (Multi-Layer Perceptron) classifier to train on a synthetic dataset (spiral data) and implements various functionalities including model initialization, ensembling, and parallelism via `vmap`. The file includes a step-by-step guide illustrating the training process for individual models and extends this to parallel training of multiple models, highlighting how different models can be trained simultaneously by vectorizing the training loop.",
            "total_tokens": 1476,
            "latency": 3.294626,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "9517a87e-518e-4908-bbb2-04095052c6a2"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of the provided Python file, `parallel_train.py`, is to demonstrate the use of PyTorch's `vmap` function to parallelize the training of multiple neural network models on a single GPU. The script defines a simple two-layer MLP classifier and trains it on a spiral dataset, then extends this to train multiple models in parallel using `vmap`. The goal is to show that PyTorch can achieve similar parallelization capabilities as JAX, a library known for its parallelization features.",
            "total_tokens": 1465,
            "latency": 1.4033,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "",
            "uuid": "fac27243-67d9-4f77-aaaf-f89a48878571"
        },
        "gpt-4.1": {
            "file_summary": "The file parallel_train.py demonstrates how to use PyTorch's functional and vectorized mapping APIs (functorch's vmap and related utilities) to parallelize training of multiple neural network models on synthetic spiral data, inspired by an analogous JAX tutorial. It defines a simple two-layer MLP and shows both single-model and multi-model (ensembled) training, using functional_call and vmap to perform parallel SGD updates across the ensemble. The script concludes by noting that, while the example trains all models on the same dataset for simplicity, further steps would be required to avoid overfitting by introducing data diversity\u2014a topic left as an exercise.",
            "total_tokens": 1484,
            "latency": 1.80284,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 152,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "d99c2879-f38f-4a38-861c-48765d19af1f"
        },
        "gemini-2.5-flash": {
            "file_name": "/functorch/examples/ensembling/parallel_train.py",
            "number_of_lines": 152,
            "project_name": "pytorch",
            "total_tokens": 1777,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 1.570158,
            "file_summary": "This Python script demonstrates how to train multiple neural network models in parallel on a single device using PyTorch and its `torch.func` (functorch) library. It defines an MLP model and a training step, then uses `torch.vmap` to apply this step simultaneously to an ensemble of models represented by batched parameters. The goal is to showcase the capability of eager-mode `vmap` for parallelizing training over models, similar to techniques used in JAX.",
            "uuid": "62811913-78fc-4a10-8319-3fef0e899b00"
        },
        "gemini-2.5": {
            "file_name": "/functorch/examples/ensembling/parallel_train.py",
            "number_of_lines": 152,
            "project_name": "pytorch",
            "total_tokens": 2034,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 6.187229,
            "file_summary": "This Python script demonstrates how to train an ensemble of MLP classifiers in parallel on a single device using PyTorch's functional programming features. It utilizes `vmap` to vectorize the training step across multiple model instances, applying the same logic simultaneously to different sets of model parameters. The example trains the models on a synthetic spiral dataset and showcases how PyTorch can achieve parallel model training similar to frameworks like JAX.",
            "uuid": "2670531e-6836-4702-9e2f-3d822e36699e"
        },
        "Claude3.7": {
            "file_name": "/functorch/examples/ensembling/parallel_train.py",
            "number_of_lines": 152,
            "project_name": "pytorch",
            "total_tokens": 1746,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.602573,
            "file_summary": "This file demonstrates how to use PyTorch's functional API and vmap to parallelize the training of multiple neural network models (ensemble learning) on a single GPU. It implements a spiral classification problem using a simple MLP classifier, first showing traditional training of a single model, then extending to parallel training of multiple models simultaneously using vmap. The code is adapted from a JAX tutorial by Will Whitney and highlights the functional programming capabilities in PyTorch that enable efficient model ensembling.",
            "uuid": "71ad42d9-e9dc-47ee-b465-7b9bbb2131e7"
        },
        "mistral-small": {
            "file_name": "/functorch/examples/ensembling/parallel_train.py",
            "number_of_lines": 152,
            "project_name": "pytorch",
            "total_tokens": 1799,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.809489,
            "file_summary": " The main functionality of the file \"parallel_train.py\" is to demonstrate parallel training of neural networks on a single GPU using PyTorch's Functorch library. It defines a two-layer MLP model for a binary classification task on spiral data, and then shows how to train multiple instances of this model in parallel using the `vmap` function from Functorch. The goal is to improve the performance and generalization of the model by ensembling multiple models.",
            "uuid": "ddf07b32-35ba-470a-a2d3-ed33b6bddd63"
        },
        "mistral-7b": {
            "file_name": "/functorch/examples/ensembling/parallel_train.py",
            "number_of_lines": 152,
            "project_name": "pytorch",
            "total_tokens": 1792,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.181265,
            "file_summary": " The main functionality of the file is to demonstrate the use of eager-mode vmap in PyTorch to parallelize training over multiple models. The script defines a two-layer MLP classifier, generates data for training, and implements a training step function using PyTorch's functional API and vmap. The goal is to achieve similar results as in the JAX tutorial by Will Whitney, which is cited in the file.",
            "uuid": "8f4f4753-6283-4bf2-b432-5667a85d69dd"
        },
        "cohere-v1": {
            "file_name": "/functorch/examples/ensembling/parallel_train.py",
            "number_of_lines": 152,
            "project_name": "pytorch",
            "file_summary": "The code demonstrates parallel training of multiple instances of a two-layer MLPClassifier model on CPU or GPU. It uses the torch library to define the models, a custom loss function, and a training step function. The models are trained in parallel using the vmap function, and the process is repeated with different initializations and data subsets.",
            "total_tokens": 1618,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.10231,
            "uuid": "ec6cd3bd-9336-4ae2-a5cc-0d188df95033"
        }
    },
    "/tools/gen_vulkan_spv.py": {
        "nova-lite-v1": {
            "file_summary": "The main functionality of the \"gen_vulkan_spv.py\" file is to generate SPIR-V shader binaries from GLSL source files and create corresponding C++ files for shader registration and dispatch in a Vulkan API. It takes GLSL source files, processes them with given parameters, compiles them into SPIR-V bytecode, and generates C++ header and source files that register these shaders for use in a Vulkan application.",
            "total_tokens": 7105,
            "latency": 1.572992,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "6bfbaff1-3a9f-48e5-9011-1aae31d74697"
        },
        "titan_premier": {
            "file_summary": "The main functionality of the given Python file is to generate SPIR-V binary files and C++ source files from GLSL source files. The file takes in a list of paths to look for GLSL source files, a path to the GLSLC compiler, a temporary directory path, and an output directory path. It also allows for setting a number of key-value pairs for environment variables.\n\nThe file first initializes a SPVGenerator object with the given paths, environment variables, and GLSLC path. It then adds the GLSL source files and template YAML files to the generator object. The generator object then constructs an output map of shader names to their corresponding source GLSL files and shader parameters.\n\nThe file then generates the SPIR-V binary files for each shader in the output map. It does this by preprocessing the GLSL source files with the given shader parameters, and then compiling the preprocessed GLSL files with the GLSLC compiler. The resulting SPIR-V binary files are saved to the temporary directory.\n\nFinally, the file generates the C++ source files. It does this by determining the shader information for each SPIR-V binary file, and then generating the necessary C++ code to register the shader information and dispatch the shaders. The resulting C++ source files are saved to the output directory.\n\nOverall, the file provides a convenient way to generate SPIR-V binary files and C++ source files from GLSL source files, with the ability to customize the environment variables and shader parameters.",
            "total_tokens": 326,
            "latency": 7.80263,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "9e256129-c6ee-490a-af49-d17c8fa58eaf"
        },
        "gpt-4.5": {
            "file_summary": "This Python script preprocesses GLSL shader source files using parameters defined in YAML templates, generates GLSL variant combinations, and compiles them into SPIR-V binaries using an external compiler (glslc). It then packages these binaries into a C++ source file containing embedded SPIR-V shader data arrays. Additionally, the script generates relevant shader metadata and registers these shaders for use with a Vulkan-based pipeline.",
            "total_tokens": 5833,
            "latency": 6.683722,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "2167b800-eb88-40f1-b291-aaa0bb677471"
        },
        "gpt-3.5-turbo": {
            "file_summary": "The given Python file is a tool for SPIR-V code generation. It includes functions for creating SPIR-V binaries from GLSL source files, generating shader parameters, and registering shaders for Vulkan. The file also contains functions to parse GLSL source files for shader information such as tile size, layouts, storage types, and registry keys. Additionally, it includes a function to generate C++ files with SPIR-V binary arrays, shader information, and shader registry handling. The main functionality of the file is to process GLSL source files, generate SPIR-V binaries, and create C++ files for shader registration in Vulkan.",
            "total_tokens": 5828,
            "latency": 1.803373,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "85ee840c-f242-450e-8278-f7df8b32940b"
        },
        "Claude3.5": {
            "file_summary": "The main functionality of the \"gen_vulkan_spv.py\" file is to generate SPIR-V shader code and corresponding C++ files for Vulkan shaders. It processes GLSL source files and YAML configuration files, generates shader variants based on specified parameters, compiles the GLSL shaders to SPIR-V using glslc, and creates C++ files containing shader information and registration code for use in a Vulkan-based application.",
            "total_tokens": 7806,
            "latency": 6.719789,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "19616eea-f34b-408d-8d5a-6b958a746af2"
        },
        "nova-pro-v1": {
            "file_summary": "The main functionality of the `gen_vulkan_spv.py` file is to generate SPIR-V binary files from GLSL shader source files and to create corresponding C++ files for shader registration in a Vulkan environment. It processes GLSL files, applies templating and parameterization, compiles them using `glslc`, and generates C++ code to register these shaders.",
            "total_tokens": 7093,
            "latency": 1.669738,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "ec21f02b-98cb-41e9-b820-2be32db254f6"
        },
        "gpt-4o": {
            "file_summary": "The script `gen_vulkan_spv.py` is a tool for generating SPIR-V (Shader Portable Intermediate Representation) binary files and accompanying C++ shader registration code from GLSL shader files using template parameters and optional YAML configurations. It facilitates shader variant generation and preprocessing with dynamic substitutions, integrates SPIR-V compilation via an external GLSL compiler, and organizes shader information for Vulkan environments. Additionally, it creates a C++ source file to register the generated shaders and their metadata into a Vulkan shader registry.",
            "total_tokens": 5852,
            "latency": 2.211433,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "8f3cac84-9093-4872-97c3-18a3d57e61fe"
        },
        "Llama3.1": {
            "file_summary": "The main functionality of this Python file is to generate Vulkan SPIR-V shaders from GLSL source files, and then use these shaders to generate C++ files for registering the shaders in a Vulkan shader registry. The script takes several command-line arguments, including the paths to the GLSL source files, the path to the GLSL compiler, and the output directory for the generated C++ files.",
            "total_tokens": 5788,
            "latency": 5.090415,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "7da1568b-94d7-4735-8c48-34f9c4bfd5f5"
        },
        "gpt-4": {
            "file_summary": "The Python script \"gen_vulkan_spv.py\" is designed for generating Vulkan SPIR-V shaders from GLSL (OpenGL Shading Language) source files. It reads shader configurations from YAML files and provides functionalities to preprocess and compile GLSL shaders into SPIR-V format using a specified GLSL compiler. Additionally, the script supports generating C++ source files that integrate with the shader binaries, registering the shaders for specific operations or dispatch keys using a Vulkan API shader registry mechanism. This setup aims to facilitate the use of custom shaders in Vulkan-based applications.",
            "total_tokens": 5814,
            "latency": 4.561784,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "13ed4b0f-a8fc-4097-94fb-d81d4b1937df"
        },
        "Llama3.3": {
            "file_summary": "The main functionality of this Python file is to generate Vulkan SPIR-V shaders from GLSL source files and create corresponding C++ files for shader registration. The script takes in command-line arguments for GLSL source file paths, a temporary directory path, an output path, and optional environment variables, and uses these to generate the shaders and C++ files. The generated C++ files include shader binary arrays and registration code for the shaders.",
            "total_tokens": 5797,
            "latency": 3.596583,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AWS",
            "uuid": "0ddc7fa3-a4cd-45b0-996e-27db92ca2f3b"
        },
        "gpt-4.1": {
            "file_summary": "The file gen_vulkan_spv.py is a code generation tool that automates the process of preprocessing GLSL shader source files (including handling templates and variants defined in YAML), compiling them into SPIR-V binaries using glslc, and generating corresponding C++ files to register the shaders for use in a Vulkan-based pipeline. It parses input directories for shader sources and YAML template definitions, manages environment variables for code generation, invokes compilation, and extracts associated metadata to be included in C++ registries. This script streamlines shader preparation and integration for Vulkan projects with build automation and template variant management.",
            "total_tokens": 5871,
            "latency": 2.111897,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "number_of_lines": 773,
            "project_name": "pytorch",
            "provider_name": "AZURE",
            "uuid": "dbe98144-44d4-4651-9ff9-c87b665956f9"
        },
        "gemini-2.5-flash": {
            "file_name": "/tools/gen_vulkan_spv.py",
            "number_of_lines": 773,
            "project_name": "pytorch",
            "total_tokens": 7724,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.414766,
            "file_summary": "This Python script reads GLSL shader source files and associated YAML configuration files to generate shader variants. It preprocesses the GLSL using a custom templating engine and compiles the resulting code into SPIR-V binaries using an external GLSL compiler. Finally, it generates C++ source code that embeds these SPIR-V binaries and registers them along with extracted metadata into a C++ shader registry.",
            "uuid": "08ea2552-63dc-4b7a-9046-d63085d03871"
        },
        "gemini-2.5": {
            "file_name": "/tools/gen_vulkan_spv.py",
            "number_of_lines": 773,
            "project_name": "pytorch",
            "total_tokens": 7925,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 7.26646,
            "file_summary": "This Python script processes GLSL shader templates and YAML configuration files to generate multiple Vulkan SPIR-V (SPV) shader variants based on defined parameters. It compiles the resulting GLSL code into SPV binaries using an external compiler (`glslc`) and extracts metadata like descriptor layouts and tile sizes from comments in the source files. Finally, it generates C++ header and source files that embed these SPV binaries as byte arrays and register them along with their metadata for runtime access within a C++ application.",
            "uuid": "6eb0be43-2e19-4c67-a387-b92a35bf2f8b"
        },
        "Claude3.7": {
            "file_name": "/tools/gen_vulkan_spv.py",
            "number_of_lines": 773,
            "project_name": "pytorch",
            "total_tokens": 7796,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.497928,
            "file_summary": "This Python script generates SPIR-V shader code for Vulkan from GLSL templates, processing shader variants based on YAML configuration files and creating C++ code that registers these shaders with a shader registry. It handles template substitution, compiles GLSL to SPIR-V using glslc, and extracts shader metadata like tile sizes and descriptor layouts to produce optimized shader implementations for different data types and configurations.",
            "uuid": "9f373e8e-3da9-4337-a188-b5f170e35234"
        },
        "mistral-small": {
            "file_name": "/tools/gen_vulkan_spv.py",
            "number_of_lines": 773,
            "project_name": "pytorch",
            "total_tokens": 8331,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 6.411346,
            "file_summary": " The main functionality of the provided Python file is to analyze and process Vulkan SPIR-V shader files. The file defines a class called `SPVGenerator` that is responsible for generating SPIR-V binaries from GLSL source files, registering the shaders in a C++ header file, and generating a C++ source file for the shader registry.\n\nThe `SPVGenerator` class takes the following arguments during initialization:\n\n* `src_dir_paths`: A list of directories containing GLSL source files.\n* `env`: A dictionary containing environment variables used for shader generation.\n* `glslc_path`: The path to the GLSL compiler (glslc).\n\nThe `SPVGenerator` class has the following main methods:\n\n* `generateSPV`: Generates SPIR-V binaries from GLSL source files and returns a dictionary mapping SPIR-V binary file paths to their corresponding GLSL source file paths.\n* `genCppFiles`: Generates C++ header and source files for the shader registry using the SPIR-V binaries and GLSL source files.\n\nThe `main` function in the file parses command-line arguments, initializes an instance of the `SPVGenerator` class, generates SPIR-V binaries, and generates C++ header and source files for the shader registry. The generated files are saved to the output directory specified by the `--output-path` command-line argument.\n\nIn summary, the provided Python file is used to generate SPIR-V binaries from GLSL source files and register the shaders in a C++ header file for use in a Vulkan application.",
            "uuid": "56d7ed4c-f182-4825-a468-87ce3825d9bf"
        },
        "mistral-7b": {
            "file_name": "/tools/gen_vulkan_spv.py",
            "number_of_lines": 773,
            "project_name": "pytorch",
            "total_tokens": 8098,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.058383,
            "file_summary": " The given Python file, 'gen_vulkan_spv.py', is a script for generating SPIR-V shader binaries and associated C++ header and source files from GLSL source files. It uses the GLSLC compiler to convert GLSL code to SPIR-V, and then generates C++ code to register the shaders with the Vulkan Shader Registry. The script takes command-line arguments to specify the paths to GLSL source files, the GLSLC compiler path, the temporary directory for storing intermediate files, and the output directory for storing the generated C++ files.",
            "uuid": "68998b96-1705-4b24-8981-a8760613f3af"
        },
        "cohere-v1": {
            "file_name": "/tools/gen_vulkan_spv.py",
            "number_of_lines": 773,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, gen_vulkan_spv.py, is a tool for generating Vulkan SPIR-V shaders. It takes GLSL shader files and associated YAML configuration files as input, processes them, and generates SPIR-V binary files and C++ header/source files for Vulkan shader registration. The tool also extracts shader information like tile sizes and descriptor layouts, helping with shader registration and dispatch in the Vulkan API.",
            "total_tokens": 6935,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.992126,
            "uuid": "3997f515-b304-45bd-a308-f5256764e60c"
        }
    },
    "/torch/_lazy/device_context.py": {
        "Llama3.3": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 363,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.365493,
            "file_summary": "The main functionality of the `device_context.py` file is to manage device contexts for PyTorch, a popular deep learning framework, using a thread-safe approach. The `DeviceContext` class represents a device context, and the `get_device_context` function retrieves or creates a device context for a given device, with a default device type used if none is specified. The device contexts are stored in a dictionary, `_CONTEXTS`, which is protected by a lock to ensure thread safety.",
            "uuid": "5abd74c9-830a-48c4-90f1-16f5746efe05"
        },
        "titan_premier": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 229,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 6.145046,
            "file_summary": "The file \"device_context.py\" is a Python file that defines a class called \"DeviceContext\" and a function called \"get_device_context\". The \"DeviceContext\" class is used to manage device contexts, and the \"get_device_context\" function is used to retrieve the device context for a given device or the default device. The file also imports the necessary modules for threading and typing, as well as the \"torch._C._lazy\" module for accessing device-related functionality.\n\nHere are the key takeaways from the file:\n\n* The \"DeviceContext\" class is used to manage device contexts, with a dictionary to store the contexts and a lock to ensure thread safety.\n* The \"get_device_context\" function retrieves the device context for a given device or the default device, creating a new context if necessary.\n* The file imports the necessary modules for threading, typing, and device-related functionality.\n\nOverall, the main functionality of the file is to provide a way to manage device contexts in a thread-safe manner.",
            "uuid": "c5254175-2172-4879-9591-0624a35224ef"
        },
        "nova-pro-v1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 329,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 0.811631,
            "file_summary": "The `device_context.py` file defines a `DeviceContext` class and a `get_device_context` function to manage and retrieve device contexts for PyTorch operations, ensuring thread-safe access and reuse of context instances.",
            "uuid": "2511e1f6-f3df-4be9-ae4b-b3bb862bbf39"
        },
        "gpt-4": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 343,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 2.127384,
            "file_summary": "The Python file `device_context.py` defines a class `DeviceContext` and a function `get_device_context`. This function retrieves or creates a `DeviceContext` instance for a specified device, storing each instance in a dictionary with threading locks to handle concurrency. The `DeviceContext` class manages a context related to a specific device (for example, hardware or simulation devices), facilitating the management of device-specific states or configurations.",
            "uuid": "eff4ff0b-193e-4072-97a4-6a5d36f2da57"
        },
        "Claude3.5": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 382,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 2.221844,
            "file_summary": "The main functionality of the \"device_context.py\" file is to manage device contexts in a multi-threaded environment. It provides a DeviceContext class and a get_device_context function that creates or retrieves a DeviceContext object for a given device, using a thread-safe mechanism to ensure proper handling of shared resources across different threads.",
            "uuid": "1839bc2e-03ec-45fe-a644-8ba98aea4db0"
        },
        "gpt-4o": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 349,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.810665,
            "file_summary": "The file \"device_context.py\" provides functionality to manage and retrieve a thread-safe singleton instance of `DeviceContext`, which represents the context associated with a specific device (e.g., CPU, GPU). It ensures that a single context instance is created per device using a lock and maintains these instances in a shared dictionary. Additionally, it leverages a default device type from PyTorch\u2019s lazy module when no device is explicitly specified.",
            "uuid": "96b23bdf-b3e7-4565-a9dc-232375be92ce"
        },
        "Llama3.1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 351,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 3.526815,
            "file_summary": "The main functionality of the `device_context.py` file is to manage a thread-safe cache of device contexts, where each context is associated with a specific device (e.g. a GPU or CPU). The `get_device_context` function retrieves or creates a device context for a given device, using a default device if none is specified. The device context is stored in a dictionary, which is protected by a lock to ensure thread safety.",
            "uuid": "397f7d23-9a9b-4bf0-a4a5-ea9df142b9bd"
        },
        "gpt-4.1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 345,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 1.807766,
            "file_summary": "The file defines a thread-safe mechanism for managing and retrieving singleton device context objects associated with computation devices (such as CPU or GPU), using Python threading locks. It provides a DeviceContext class to represent a computational device and a get_device_context() function to retrieve or create (if needed) a DeviceContext for a specified or default device. The implementation ensures only one context per device exists throughout the application's lifetime.",
            "uuid": "895d83dd-0789-468f-93ef-2545e7f695a1"
        },
        "nova-lite-v1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 376,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 0.904497,
            "file_summary": "The main functionality of the \"device_context.py\" file is to manage and provide access to device-specific contexts in a thread-safe manner. It utilizes a dictionary to store instances of `DeviceContext` associated with different devices, ensuring that each device context is created only once. The `get_device_context` function retrieves or creates a `DeviceContext` instance for a specified device, defaulting to the default device type if no device is provided.",
            "uuid": "1b69c9a8-f81b-4f2d-90a8-e64e7dc738d2"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 331,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 0.984957,
            "file_summary": "The main functionality of the \"device_context.py\" file is to define a class DeviceContext that manages the context of different devices. It provides a method get_device_context() to retrieve or create a DeviceContext object based on the specified device type. The class uses a dictionary to store and reuse DeviceContext objects for each device type, ensuring a single context per device.",
            "uuid": "57536aae-50ff-40ee-8cf4-238860d80ef8"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 794,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 3.41509,
            "file_summary": "This Python file defines a `DeviceContext` class to represent a specific device type. It provides a thread-safe function, `get_device_context`, which acts as a factory to retrieve a unique `DeviceContext` instance for a given device string. The function caches instances in a dictionary, ensuring that only one `DeviceContext` object exists per device type and returning the existing one if available.",
            "uuid": "81527368-1380-4a43-b10a-1af7f05f3305"
        },
        "gpt-4.5": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 348,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 9.055873,
            "file_summary": "This Python file defines a `DeviceContext` class, which manages and caches device contexts in a thread-safe manner. It provides a global function `get_device_context()` that retrieves or creates a context instance for a specific device (e.g., CPU, GPU), storing it in a shared dictionary to avoid redundant instantiations. The default device type is obtained from `torch._C._lazy` if not explicitly specified.",
            "uuid": "80a4aedd-7396-4289-9560-932b9dbe97d0"
        },
        "gemini-2.5": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 836,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.895201,
            "file_summary": "This Python file defines a thread-safe mechanism for managing and retrieving device-specific context objects (`DeviceContext`). It uses a global dictionary protected by a lock to ensure that only one `DeviceContext` instance is created per unique device string. The `get_device_context` function acts as a factory, returning the existing context for a device or creating a new one, defaulting to the PyTorch lazy tensor default device if none is specified.",
            "uuid": "32ef416c-a2c7-4c7d-9e33-50dd76c47767"
        },
        "Claude3.7": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 390,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.499983,
            "file_summary": "The file \"device_context.py\" implements a device context management system for PyTorch's lazy execution, providing a thread-safe singleton pattern to create and retrieve DeviceContext objects based on specified device types. It maintains a dictionary of device contexts and ensures that only one context exists per device type through a locking mechanism, with a fallback to the default device type when none is specified.",
            "uuid": "49c61d54-0d6b-464a-939d-284ae4c877fc"
        },
        "mistral-small": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 397,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.748892,
            "file_summary": " The main functionality of the file \"device_context.py\" is to manage device contexts for PyTorch operations. It defines a `DeviceContext` class that stores a device string and a dictionary `_CONTEXTS` to store instances of `DeviceContext` for different devices. The `get_device_context` function is used to retrieve or create a `DeviceContext` instance for a given device, ensuring thread safety with a lock.",
            "uuid": "d44dc6f5-3afe-4a58-b6b5-5b593f321664"
        },
        "mistral-7b": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "total_tokens": 383,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.001338,
            "file_summary": " The given Python file defines a class named `DeviceContext` which serves as a context manager for managing torch devices. The `get_device_context` function is used to get or create a device context instance based on the given device name. The file also uses threading and a lock to ensure thread safety when accessing the `_CONTEXTS` dictionary.",
            "uuid": "6e22fcb0-68f4-4496-bafe-4c96e2a901ad"
        },
        "cohere-v1": {
            "file_name": "/torch/_lazy/device_context.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The code defines a DeviceContext class and a get_device_context function. It maintains a dictionary of device contexts, ensuring only one context is created per device. The function returns an existing context or creates a new one if it doesn't exist.",
            "total_tokens": 326,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 0.854451,
            "uuid": "17650ada-bdc3-477f-b278-a60704a76552"
        }
    },
    "/torch/utils/benchmark/op_fuzzers/spectral.py": {
        "gpt-3.5-turbo": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1008,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.213898,
            "file_summary": "The given 'spectral.py' file contains a class called 'SpectralOpFuzzer' that extends the 'benchmark.Fuzzer' class. This class is used to generate benchmark parameters for spectral operations, specifically for testing Fast Fourier Transform (FFT) implementations in PyTorch. The parameters include dimensionality of data, shapes for input tensors (including regular sizes for FFT testing), steps for strided memory access, and other tensor properties like data type and memory allocation.",
            "uuid": "73b62bc4-0602-4798-b3fa-6d2b6904854a"
        },
        "gpt-4.1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1023,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 2.596718,
            "file_summary": "The main functionality of spectral.py is to define a fuzz testing utility, SpectralOpFuzzer, for benchmarking spectral (e.g., FFT) operations in PyTorch. It generates varied tensor shapes\u2014including those optimized for fast FFT performance\u2014using parameterized fuzzing, allowing systematic exploration of dimensionality, individual axis sizes, strides, and memory layouts. The generated tensors and parameters are particularly tailored to test and evaluate the efficiency and robustness of PyTorch's spectral operators, emphasizing sizes (\"regular numbers\") that are typically performant for FFT algorithms.",
            "uuid": "04c6dac6-883e-4a82-a7d5-fae97c762b77"
        },
        "Llama3.1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1003,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 3.236405,
            "file_summary": "The main functionality of this Python file is to define a class `SpectralOpFuzzer` that inherits from `torch.utils.benchmark.Fuzzer`. This class is designed to generate random inputs for benchmarking spectral operations, such as Fast Fourier Transform (FFT), in PyTorch. The fuzzer generates random tensor shapes, sizes, and strides, with a focus on regular sizes that are optimized for FFT implementations.",
            "uuid": "a506692b-aaa8-4a81-a143-d27f28789d1c"
        },
        "nova-pro-v1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1083,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 1.158452,
            "file_summary": "The `spectral.py` file defines a `SpectralOpFuzzer` class for fuzz testing spectral operations (like FFT) in PyTorch. It generates various input tensor shapes and strides to benchmark the performance, with a focus on \"regular\" sizes that are multiples of 2, 3, and 5. The fuzzer utilizes PyTorch's benchmarking utilities to create and test these configurations.",
            "uuid": "4736ee40-c548-4538-bac2-d912bd49d586"
        },
        "nova-lite-v1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1095,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 0.880591,
            "file_summary": "The `spectral.py` file defines a class `SpectralOpFuzzer` for fuzz testing spectral operations in PyTorch, focusing on generating tensors with specific shapes and strides to test the efficiency and robustness of FFT implementations. It uses a combination of regular and random sizes for tensor dimensions, and includes parameters for controlling the distribution of these sizes, the data type, and whether to use CUDA. The file also includes utility functions for generating power ranges and regular sizes.",
            "uuid": "af1c3784-43ae-4ea5-ad5e-e015febee068"
        },
        "gpt-4o": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1007,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.609978,
            "file_summary": "This Python file defines the `SpectralOpFuzzer`, a utility for creating and benchmarking fuzzed tensors and parameters targeted specifically for spectral operations such as FFT (Fast Fourier Transform). It generates test cases with configurable dimensions, tensor shapes, strides, and probabilities emphasizing \"regular sizes\" that are computationally efficient for FFT. The fuzzer leverages PyTorch's benchmarking utilities to evaluate performance under diverse conditions, focusing on varied tensor configurations and memory access patterns.",
            "uuid": "2b0987f6-15b4-44a6-ac29-712393624fc1"
        },
        "titan_premier": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 175,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 4.042059,
            "file_summary": "The Python file \"spectral.py\" contains a class named \"SpectralOpFuzzer\" that inherits from the \"Fuzzer\" class in the \"torch.utils\" module. This class generates random inputs for benchmarking spectral operations in PyTorch. The class initializes various parameters and tensors, including the dimensionality, shape, and steps of the input tensor, as well as the data type and device (CPU or GPU) for the tensor. The class also includes a list of regular sizes that are commonly used in FFT implementations and are generated using prime factors 2, 3, and 5. The probability of generating a regular size can be controlled by the \"probability_regular\" parameter. Overall, the main functionality of the file is to provide a framework for fuzz testing of spectral operations in PyTorch.",
            "uuid": "a760f60a-6fe1-4cbd-ba47-e63b8842d0f4"
        },
        "gpt-4": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1013,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 2.831055,
            "file_summary": "The file `spectral.py` defines a class `SpectralOpFuzzer` for fuzz testing spectral operations (like FFT). It utilizes `torch` and `benchmark` from PyTorch to set up a fuzzing environment where tensor dimensions and shapes can be randomly generated according to specific parameters, including both regular and irregular distribution sizes optimized for FFT performance. Different data types, memory strides, and contingencies for tensor creation on both CPU and GPU are supported by configuring detailed fuzzer parameters.",
            "uuid": "a6bc98e6-6569-465f-9873-feab73af2d15"
        },
        "Llama3.3": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1049,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.880654,
            "file_summary": "The main functionality of the provided Python file, `spectral.py`, is to define a class `SpectralOpFuzzer` that inherits from `torch.utils.benchmark.Fuzzer` and is designed to generate random inputs for benchmarking spectral operations, particularly those involving Fast Fourier Transforms (FFTs). The fuzzer generates random tensor shapes, sizes, and strides, with a focus on \"regular\" sizes that are optimized for FFT performance. The class allows for customization of parameters such as data type, CUDA usage, and probability of selecting regular sizes, making it a versatile tool for benchmarking and testing spectral operations in PyTorch.",
            "uuid": "891bd4b3-f1f8-4fb4-b203-4eb84c5a4c86"
        },
        "Claude3.5": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1261,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 4.148508,
            "file_summary": "The main functionality of the \"spectral.py\" file is to define a SpectralOpFuzzer class, which is a custom fuzzer for spectral operations in PyTorch. This fuzzer generates random input parameters and tensors for testing spectral operations, with a focus on creating various shapes and sizes, including regular sizes that are optimal for FFT implementations. The file also includes utility functions and constants to support the fuzzer's operation, particularly in generating and handling dimension sizes for the input tensors.",
            "uuid": "265e251b-366b-4d44-85e8-b10c1cbb172d"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1414,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.294311,
            "file_summary": "This Python file defines a `SpectralOpFuzzer` class, inheriting from `torch.utils.benchmark.Fuzzer`, designed to generate fuzzed inputs for benchmarking spectral operations like FFT. It pre-calculates a list of \"regular\" sizes (factorizable by 2, 3, and 5) known to be efficient for FFT implementations. The fuzzer generates tensors with varying dimensions, shapes (with a preference for these regular sizes), and strides to test performance across different input configurations.",
            "uuid": "daf4198d-1f82-4f1b-aeb6-4aa232625017"
        },
        "gpt-4.5": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1017,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 4.359109,
            "file_summary": "The provided Python file defines a \"SpectralOpFuzzer,\" a benchmarking utility built using PyTorch's benchmarking framework to fuzz-test spectral operations (such as FFTs). This fuzzer generates randomized tensor shapes and strides targeting particularly fast (regular) sizes that factorize into powers of primes 2, 3, and 5, as well as various memory layouts to test strided access effectively. Its primary purpose is to systematically stress-test and benchmark the spectral operations' performance in various realistic tensor configurations.",
            "uuid": "87d01819-7439-48ea-a8df-eec354f363c0"
        },
        "gemini-2.5": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1387,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 4.369911,
            "file_summary": "This Python file defines a `SpectralOpFuzzer` class used for benchmarking PyTorch spectral operations like FFT. It generates randomized input tensors with varying dimensions, sizes, and memory layouts (strides) to test performance under diverse conditions. The fuzzer specifically prioritizes generating tensor sizes known as \"regular numbers\" (factors of 2, 3, 5), which are often optimized in FFT implementations.",
            "uuid": "7a8e84ac-5824-43a2-9895-bbc3e96b5ca3"
        },
        "Claude3.7": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1262,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.206839,
            "file_summary": "The SpectralOpFuzzer class is a specialized fuzzer for benchmarking spectral operations (like FFT) in PyTorch, generating test tensors with carefully controlled dimensionality, shapes, and memory strides. It particularly emphasizes \"regular\" sizes (numbers factorizable by 2, 3, and 5) which are typically optimized in FFT implementations, while allowing configuration of tensor properties like dtype, CUDA support, and the probability of using regular sizes versus arbitrary ones.",
            "uuid": "adbd3453-62ed-426e-881a-24d3186e96b0"
        },
        "mistral-small": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1288,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.801826,
            "file_summary": " The main functionality of the file \"spectral.py\" is to define a fuzzer for spectral operations in PyTorch. The fuzzer generates random tensor shapes and strides, with a bias towards \"regular\" sizes that are typically faster in FFT implementations. It also allows for the selection of data types and whether to use CUDA. The purpose of this fuzzer is to benchmark and test the performance of spectral operations in PyTorch under various conditions.",
            "uuid": "e619b5a2-f150-4389-a7d8-4b29ae43d657"
        },
        "mistral-7b": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "total_tokens": 1323,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.682677,
            "file_summary": " The given Python file, named \"spectral.py\", is a script for creating a custom fuzzer named `SpectralOpFuzzer` using PyTorch's benchmarking library. The main functionality of the file is to generate random tensors with specific shapes and sizes, focusing on regular sizes that factorize into multiples of prime factors 2, 3, and 5. These regular sizes are important for Fast Fourier Transform (FFT) implementations and are given special attention. The fuzzer also sets various parameters such as dimensionality, steps, and data types for the generated tensors.",
            "uuid": "e9a06873-7c25-4742-9b60-c172a35e07e0"
        },
        "cohere-v1": {
            "file_name": "/torch/utils/benchmark/op_fuzzers/spectral.py",
            "number_of_lines": 95,
            "project_name": "pytorch",
            "file_summary": "The spectral.py file defines a SpectralOpFuzzer class that inherits from the benchmark.Fuzzer class. This class has an initializer that takes several parameters related to dimensionality and tensor properties, including a seed, data type, and CUDA usage. The file also contains helper functions for generating regular sizes and setting up fuzzing parameters.",
            "total_tokens": 1063,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.070578,
            "uuid": "8049605a-e949-476d-a29e-5641f0779962"
        }
    },
    "/torch/distributed/tensor/_sharding_prop.py": {
        "gpt-4": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 4395,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 3.403851,
            "file_summary": "The provided Python file \"_sharding_prop.py\" implements and manages sharding strategies and propagation rules for distributed tensor operations in PyTorch. It includes classes and methods for caching strategies locally, registering sharding rules for different tensor operations, propagating tensor metadata dynamically based on operations' sharding strategies, and managing output specifications in a distributed environment. The automation within the file aims to enhance computation by effectively distributing data and computations in a multi-node setup, adapting strategies for tensor operation, and minimizing data redistribution overhead during execution.",
            "uuid": "05047df8-3f82-4e4c-bdd9-7529adb831de"
        },
        "nova-pro-v1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 5352,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 1.345853,
            "file_summary": "The main functionality of the `_sharding_prop.py` file is to define and manage the propagation of sharding strategies for distributed tensor operations in PyTorch, utilizing caching mechanisms and strategy selection based on cost.",
            "uuid": "9dbcf5ff-a723-48d5-8720-a64bb1d307da"
        },
        "Llama3.1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 4373,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 6.225286,
            "file_summary": "The main functionality of this file is to define a class `ShardingPropagator` that is responsible for propagating sharding information through PyTorch operators. It uses a set of predefined rules and strategies to determine the output sharding of an operator based on its input sharding. The class also provides methods for registering custom sharding propagation rules and strategies for specific operators.",
            "uuid": "adce7bb9-a0fc-4b04-8aef-4ec1f5253311"
        },
        "Llama3.3": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 4392,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 2.689875,
            "file_summary": "The main functionality of the given Python file `_sharding_prop.py` is to propagate sharding for PyTorch operators, which involves determining how to split and distribute tensors across multiple devices. The `ShardingPropagator` class is responsible for this task, using a combination of predefined rules and strategies to determine the optimal sharding for a given operator. The propagator takes into account factors such as tensor shapes, strides, and device placements to make its decisions.",
            "uuid": "61d57d6f-6a3b-416d-890e-9249858d9bdc"
        },
        "gpt-4o": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 4430,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.456099,
            "file_summary": "The `_sharding_prop.py` file implements functionality for propagating and managing tensor sharding in distributed systems using PyTorch's distributed tensor framework. It provides mechanisms for registering sharding rules and strategies for various operators, optimizing tensor reshaping and redistribution, and maintaining metadata such as tensor shape and stride during sharding operations. The main component, `ShardingPropagator`, handles sharding propagation by dynamically adjusting tensor placements and metadata based on defined rules, caching mechanisms, and efficient strategy selection.",
            "uuid": "9be6fd06-48bc-4637-af34-cf0fe16e606a"
        },
        "titan_premier": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 63,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 2.47016,
            "file_summary": "The main functionality of the file is to propagate the sharding for an operator given the op schema. It also has methods to register sharding propagation rule for an operator and register a sharding strategy generator for an operator. It also has methods to propagate the tensor metadata and wrap the output spec with the tensor metadata.",
            "uuid": "3a2d7460-b518-406a-8d21-8ac26f52ad3c"
        },
        "nova-lite-v1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 5383,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 1.55044,
            "file_summary": "This Python file defines a `ShardingPropagator` class that manages the propagation of tensor sharding strategies in distributed tensor operations. It registers rules and strategies for specific operations to determine how tensors should be sharded across different devices in a distributed computing environment. The class provides methods to propagate sharding information for given operation schemas and associate the resulting sharding with tensor metadata.",
            "uuid": "6b7a75f8-a3fd-4582-867d-b9c77e578e90"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 4406,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.711209,
            "file_summary": "The main functionality of the provided Python file \"_sharding_prop.py\" is to define classes and methods related to sharding propagation for operators in PyTorch. It includes functionality to register sharding rules and strategies for operators, propagate sharding information for operators based on rules/strategies, adjust shape and stride arguments for certain operations, and handle tensor metadata propagation. The file also contains classes such as `ShardingPropagator` for managing sharding information, `LocalLRUCache` for caching user functions locally, and methods for wrapping output specifications with tensor metadata.",
            "uuid": "4a8a6f36-d7a0-443e-8f26-4707ab704477"
        },
        "Claude3.5": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 5859,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 5.596799,
            "file_summary": "The main functionality of the \"_sharding_prop.py\" file is to implement a sharding propagation system for distributed tensor operations in PyTorch. It defines a ShardingPropagator class that handles the registration and execution of sharding propagation rules and strategies for various PyTorch operators, allowing for efficient distribution of tensor computations across multiple devices or machines.",
            "uuid": "3db3ee83-fff7-432e-adde-355c8f192b4d"
        },
        "gpt-4.1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 4434,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 3.204338,
            "file_summary": "The primary functionality of this file is to define the ShardingPropagator class and its associated logic, which is responsible for propagating sharding strategies and tensor metadata for distributed tensor operations in PyTorch. It provides mechanisms to register sharding propagation rules and operator strategies, automatically select optimal sharding placements, and handle redistributions for a wide variety of tensor operations. The file ensures seamless and efficient manipulation, suggestion, and validation of sharded tensor layouts during distributed execution by supporting both rule-based and strategy-based propagation.",
            "uuid": "42a5b4d7-63c5-49d9-b987-200c62044226"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 5754,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.292712,
            "file_summary": "This Python file defines the `ShardingPropagator` class, which is responsible for determining the output sharding of PyTorch operations based on the sharding of their inputs. It uses registered rules or strategy functions for specific operators to compute the output sharding and can suggest input redistributions if necessary. The class also handles propagating tensor metadata and adjusting shape/stride arguments for certain operations.",
            "uuid": "d9e30349-a34d-4916-8885-cd73bbcd7942"
        },
        "gpt-4.5": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 4429,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 10.363015,
            "file_summary": "The Python file implements a system for sharding propagation in PyTorch distributed tensors, primarily through the class `ShardingPropagator` responsible for registering and applying sharding rules and strategies to different torch operators (`OpOverload` instances). It provides methods for handling tensor metadata propagation (`TensorMeta`), managing caching for efficiency, and automatically adjusting tensor shapes and strides when performing distributed operations. Overall, this module enables efficient, strategy-driven automatic sharding for distributed tensor operations in PyTorch.",
            "uuid": "31cbfdd9-3bb1-496b-b523-9d6d31565e5a"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 6055,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 7.437133,
            "file_summary": "This Python file defines the `ShardingPropagator` class, responsible for determining how sharding (distribution across devices) should be applied to the outputs of PyTorch operations involving distributed tensors (DTensors). It uses registered rules or strategies specific to each operator to calculate the output sharding based on the input tensors' distributed layout and the operation itself. The propagator also handles tensor metadata propagation and can suggest input tensor redistributions if the current sharding is incompatible with the chosen execution strategy.",
            "uuid": "8b6a4dfe-9ed1-424f-9b67-957ea57c51d5"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 5879,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.884429,
            "file_summary": "This file defines a `ShardingPropagator` class that handles the propagation of tensor sharding information across PyTorch operations in a distributed tensor environment. The class maintains rules and strategies for different PyTorch operators, determining how tensor sharding should be handled when operations are performed, and includes caching mechanisms to improve performance. It's a core component of PyTorch's distributed tensor system that enables efficient distributed computation by analyzing operations and determining optimal sharding strategies.",
            "uuid": "e46a1f03-977d-4277-a124-3ca0036af236"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 6267,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 4.943747,
            "file_summary": " The provided Python file, `_sharding_prop.py`, is part of PyTorch's distributed tensor library. It defines a class called `ShardingPropagator` that is responsible for propagating sharding information for PyTorch operators.\n\nThe main functionality of the file is to analyze the sharding of PyTorch tensors during the execution of operations. This is done by registering sharding propagation rules and sharding strategy generators for different operators. These rules and strategies are then used to determine how the shards of tensors should be distributed and rearranged during the execution of operations.\n\nThe `ShardingPropagator` class has several methods for registering sharding propagation rules and strategies, as well as methods for propagating sharding information and selecting the best sharding strategy for a given operation. It also includes methods for adjusting the shape and stride of tensors, as well as for wrapping output specifications with tensor metadata.\n\nIn summary, the main functionality of the `_sharding_prop.py` file is to provide a mechanism for propagating sharding information for PyTorch tensors during the execution of operations, and to select the best sharding strategy for each operation based on the registered rules and strategies.",
            "uuid": "7eef55b4-14bf-4d0e-8b19-9fe9323204e3"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "total_tokens": 6106,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.621265,
            "file_summary": " This Python file defines a class named `ShardingPropagator` which is used for propagating sharding information in PyTorch. The main functionality of the file is to register sharding propagation rules and strategies for PyTorch operators, and propagate sharding information based on the given operator schema. The sharding information includes output sharding and potential input redistribution. The file also includes helper functions for computing local shapes and strides, and wrapping output specs with tensor metadata.",
            "uuid": "29b500c7-c738-4a9a-946c-4b285ebb7e83"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/tensor/_sharding_prop.py",
            "number_of_lines": 531,
            "project_name": "pytorch",
            "file_summary": "The code defines a ShardingPropagator class that propagates the sharding strategy for torch operators. It has methods to register sharding propagation rules and strategies, and a propagate method to determine the output sharding based on the operator's input specifications. The class uses local caching to optimize repeated sharding calculations for the same operator.",
            "total_tokens": 5033,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.681355,
            "uuid": "2cc81886-c66d-4245-b1e4-4dcc593f3390"
        }
    },
    "/torch/nn/quantizable/modules/activation.py": {
        "nova-pro-v1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 257,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 0.899444,
            "file_summary": "The `activation.py` file serves as a compatibility layer during the migration of quantizable modules to the `torch/ao/nn/quantizable` package. It imports the `MultiheadAttention` module from the new location to ensure continued functionality during the transition.",
            "uuid": "4bd07abd-6a21-46d7-92c8-16e214ba3cf8"
        },
        "Llama3.1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 271,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 2.843743,
            "file_summary": "The main functionality of the file \"activation.py\" is to provide a module for quantizable activation functions, specifically importing the MultiheadAttention class from another module. The file is currently being migrated to a new location and is being kept for compatibility purposes. It does not contain any original functionality, but rather serves as a redirect to the new location.",
            "uuid": "0d79d056-374b-47ae-8d18-d6c36b04c21a"
        },
        "Llama3.3": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 293,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.36056,
            "file_summary": "The main functionality of the `activation.py` file is to provide a compatibility layer for quantizable modules, specifically importing the `MultiheadAttention` module from `torch.ao.nn.quantizable.modules.activation`. This file is currently being migrated to `torch/ao/nn/quantizable` and is being kept for backwards compatibility. The file does not contain any new functionality and instead serves as a redirect to the new location of the quantizable modules.",
            "uuid": "17620235-7e1c-4b2b-a9e6-4d76e51bb57a"
        },
        "nova-lite-v1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 292,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 0.691123,
            "file_summary": "The main functionality of the \"activation.py\" file is to provide compatibility for quantizable modules during the migration process to a new location in the PyTorch library. It imports the `MultiheadAttention` class from the new module location, `torch.ao.nn.quantizable.modules.activation`, and serves as a transitional file until the migration is complete. New functionalities should be added to the new module location instead of this file.",
            "uuid": "4336610f-6fa1-4b6c-ac20-aff4231a73bb"
        },
        "gpt-4o": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 269,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.09162,
            "file_summary": "The file `activation.py` serves as a compatibility layer during the migration of quantizable modules to the `torch/ao/nn/quantizable` namespace. It re-exports the `MultiheadAttention` module from the new location to ensure backward compatibility. The file also advises developers to add new functionality directly to the updated namespace while maintaining import statements here.",
            "uuid": "6553f466-489f-451c-9610-76bac5199611"
        },
        "gpt-4.1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 286,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 3.457431,
            "file_summary": "The file activation.py serves as a transitional compatibility module for quantizable neural network components, specifically re-exporting the MultiheadAttention class from torch.ao.nn.quantizable.modules.activation. It includes documentation stating that its contents are being migrated to a new location and advises contributors to add new features to the updated directory while importing them here for backward compatibility. Thus, its main function is to maintain compatibility during the refactoring of quantizable PyTorch modules.",
            "uuid": "6b542be0-a3a2-43b3-9075-dbbf78ac8a9d"
        },
        "Claude3.5": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 301,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 3.042503,
            "file_summary": "The file \"activation.py\" serves as a compatibility layer during the migration process of quantizable modules from their current location to \"torch/ao/nn/quantizable\". It imports the MultiheadAttention module from the new location while maintaining backward compatibility for existing code. The file also includes a note advising developers to add new functionalities directly to the appropriate files under the new directory structure.",
            "uuid": "e3144dc6-fdad-4702-9264-f2fb9a859550"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 283,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.086942,
            "file_summary": "The main functionality of the \"activation.py\" file is to provide information about Quantizable Modules being migrated to a specific directory in the torch library. It serves as a placeholder for compatibility during the migration process and instructs users to add new entries or functionalities to the appropriate file under the designated directory while ensuring import statements are added to this file. Specifically, it imports the MultiheadAttention module from torch.ao.nn.quantizable.modules.activation.",
            "uuid": "6ec06fe8-a68f-407a-a848-928efab449a2"
        },
        "titan_premier": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 92,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 3.846073,
            "file_summary": "The file \"activation.py\" is a Python file that contains the import statement for the MultiheadAttention class from the torch.ao.nn.quantizable.modules.activation module. This file is currently in the process of being migrated to the torch/ao/nn/quantizable directory and is kept here for compatibility purposes. The file also contains a docstring that provides information about the ongoing migration process and instructions for adding new entries or functionalities.",
            "uuid": "81fb3128-3b10-422e-b638-026fac08dfc0"
        },
        "gpt-4": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 279,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 1.909914,
            "file_summary": "The Python file \"activation.py\" is involved in the migration of quantizable modules to the new directory `torch/ao/nn/quantizable`. It currently primarily handles legacy support for compatibility during this migration by importing the `MultiheadAttention` module from the new location `torch.ao.nn.quantizable.modules.activation`. Thus, this file acts as a transitional linker to the new modular structure while maintaining backward compatibility.",
            "uuid": "eed06885-a3f1-43a9-9f75-b06467f81c01"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 555,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.396711,
            "file_summary": "This Python file is part of a migration process for quantizable modules in PyTorch, serving as a compatibility layer. It re-exports modules from their new location to maintain backward compatibility during the transition. Currently, it specifically imports and makes available the `MultiheadAttention` module from `torch.ao.nn.quantizable.modules.activation`.",
            "uuid": "36d8c8cf-8b77-4e22-8a93-ab5616304f97"
        },
        "gpt-4.5": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 280,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 3.207523,
            "file_summary": "The provided Python file acts as a backward compatibility layer for quantizable modules in PyTorch, specifically for activation modules undergoing migration to a new location (`torch/ao/nn/quantizable`). It imports the `MultiheadAttention` module from its new location, enabling a seamless transition for codebases relying on the previous structure. The file also includes instructions for developers to contribute new functionality directly to the target module location.",
            "uuid": "252c3b56-fdc9-4726-a33f-867b69313ff7"
        },
        "gemini-2.5": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 881,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 8.095404,
            "file_summary": "This Python file acts as a compatibility shim within the PyTorch library, specifically for quantizable modules. It facilitates the migration of modules like `MultiheadAttention` to the newer `torch.ao.nn.quantizable` namespace. Its main function is to re-export these migrated modules to maintain backward compatibility for users during the transition period.",
            "uuid": "36249a08-ad7f-48a0-9f6f-2ce4c960d4d5"
        },
        "Claude3.7": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 282,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 1.975742,
            "file_summary": "The file \"activation.py\" serves as a compatibility layer during the migration of quantizable modules from its current location to \"torch/ao/nn/quantizable\". It specifically imports the MultiheadAttention class from the new location while maintaining backward compatibility for existing code that may still reference this file.",
            "uuid": "de32f07a-b123-42ec-800f-ed3a4fff0913"
        },
        "mistral-small": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 313,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.610028,
            "file_summary": " The main functionality of the file \"activation.py\" is to provide quantizable modules, specifically the MultiheadAttention module from the torch.ao.nn.quantizable.modules.activation package. However, the file is in the process of migration to torch/ao/nn/quantizable, and new entries/functionalities should be added to the appropriate file under torch/ao/nn/quantizable/modules.",
            "uuid": "3a095680-5c04-4bfa-b51a-a13381da5727"
        },
        "mistral-7b": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "total_tokens": 338,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.663978,
            "file_summary": " The given Python file, named \"activation.py\", is part of a migration process to the `torch/ao/nn/quantizable` module. It currently imports the MultiheadAttention function from the \"activation\" submodule within the \"quantizable\" module of PyTorch's Automatic Optimizer (TorchAutograd). This file is used to manage and maintain compatibility with existing functionality while new entries or features are being added to the appropriate files under `torch/ao/nn/quantizable/modules`.",
            "uuid": "755c767a-d701-4096-abbb-fcf5a339dfdf"
        },
        "cohere-v1": {
            "file_name": "/torch/nn/quantizable/modules/activation.py",
            "number_of_lines": 11,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the activation.py file is to provide quantizable modules, specifically implementing a MultiheadAttention class. It appears to be part of a larger codebase migration, directing new entries to a different location. The file maintains import statements while the migration is in progress.",
            "total_tokens": 258,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 0.845072,
            "uuid": "a932f02e-1e13-4e6c-b432-1f6e0ae2e7d6"
        }
    },
    "/torch/_export/db/examples/class_method.py": {
        "Llama3.1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 318,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 3.773656,
            "file_summary": "The main functionality of this Python file is to define a PyTorch neural network module `ClassMethod` that contains a class method `method` and an instance method `forward`. The `forward` method applies a linear transformation to the input and then calls the class method `method` in three different ways to perform some computation. The file also creates an instance of the `ClassMethod` model and defines some example input arguments.",
            "uuid": "51c16b85-96de-4c8f-b3d0-e8f87b8510a0"
        },
        "nova-pro-v1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 341,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 1.491621,
            "file_summary": "The file `class_method.py` defines a custom PyTorch neural network module named `ClassMethod` that includes a class method `method` which increments its input by 1. The `forward` method of the module applies a linear transformation to the input, then multiplies the result by three versions of the class method applied to the input. Finally, the file creates an instance of the `ClassMethod` module and prepares example arguments for its execution.",
            "uuid": "1da4cf15-9a70-4d64-ad49-9ade0cce6184"
        },
        "Claude3.5": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 354,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 3.834355,
            "file_summary": "The main functionality of the \"class_method.py\" file is to define a PyTorch neural network module called ClassMethod. This module demonstrates the use of a class method within a neural network, showing how it can be inlined during tracing. The forward method of the module applies a linear transformation to the input and then uses the class method in three different ways to process the result.",
            "uuid": "84c9b52d-71c9-4242-bd8a-d392e5e63468"
        },
        "gpt-4.1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 324,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 2.152192,
            "file_summary": "The file defines a PyTorch neural network module called `ClassMethod` that demonstrates the use of class methods within a model's computation. In the `forward` method, the module applies a linear transformation to the input and then calls a class method (`method`) in three different ways, multiplying the results. This file serves as an example of how class methods can be invoked and used in the forward pass of a PyTorch model, particularly in contexts where tracing or scripting might be involved.",
            "uuid": "7e93a84d-1fe5-4224-b265-26c23eac766a"
        },
        "gpt-4": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 333,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 2.86058,
            "file_summary": "The file `class_method.py` defines a Python class `ClassMethod` that inherits from `torch.nn.Module`. The class encapsulates neural network operations and includes a class method `method`, which simply increments its input by one. The `forward` function processes an input tensor through a linear transformation and then multiplies the result by three instances of the output from the `method` (applied different ways to demonstrate class method access). This hints at potential experimentation with class-level method invocations within a PyTorch model computation graph.",
            "uuid": "f3884669-9e86-4aa6-9349-cd14f8da54d6"
        },
        "Llama3.3": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 337,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.989565,
            "file_summary": "The main functionality of the given Python file is to define a PyTorch neural network module `ClassMethod` that inherits from `torch.nn.Module`. This module contains a class method `method` that increments the input by 1, and an instance method `forward` that applies a linear transformation to the input and then applies the class method in three different ways. The file also creates an instance of the `ClassMethod` model and defines example input arguments for it using PyTorch's `torch.randn` function.",
            "uuid": "5c5d9c75-48dd-4ac9-af45-b966278adda6"
        },
        "gpt-4o": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 319,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 1.169305,
            "file_summary": "The Python file defines a neural network class `ClassMethod` using PyTorch's `torch.nn.Module`, which includes a custom forward method that applies a linear transformation, followed by computations involving a class method `method`. The `method` function increments the input tensor by 1 and is statically referenced multiple times during the forward pass. Overall, the file integrates PyTorch's tracing capabilities to demonstrate the inline usage of class methods within a model's computation graph.",
            "uuid": "c8d6c8fc-f68c-446b-9eb2-74d65b670114"
        },
        "titan_premier": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 132,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 3.229346,
            "file_summary": "The main functionality of the file \"class_method.py\" is to define a class called \"ClassMethod\" that inherits from the \"torch.nn.Module\" class. This class has a class method called \"method\" that takes an input \"x\" and returns \"x + 1\". The class also has an instance method called \"forward\" that takes an input \"x\", applies a linear transformation using the \"linear\" attribute, and then applies the \"method\" class method to the transformed input. The file also includes an example usage of the class, where an instance of the class is created and passed a random input tensor.",
            "uuid": "806c4a19-f751-4af9-9e7e-a36fc5e4545d"
        },
        "nova-lite-v1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 328,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 1.938332,
            "file_summary": "The main functionality of the file is to define a PyTorch neural network class `ClassMethod` that includes a class method `method`, an initialization method, and a forward pass method. The forward method utilizes the class method to perform operations on the input tensor, combining it with the class method in various ways. The file also includes an example of instantiating the class and preparing sample input data.",
            "uuid": "f6ce8d61-6fb4-448b-b976-b93f273aca37"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 317,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 0.979658,
            "file_summary": "The given 'class_method.py' file defines a class 'ClassMethod' that inherits from torch.nn.Module. It includes a class method 'method' that performs a simple operation on the input, an initialization method '__init__' that sets up a linear layer, and a 'forward' method that uses the linear layer and calls the class method 'method'. An example argument is provided, and an instance of the 'ClassMethod' class is created.",
            "uuid": "0b508fb4-9aa1-4836-8512-80631db1f929"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 539,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.08725,
            "file_summary": "This Python file defines a PyTorch `nn.Module` subclass named `ClassMethod`. It includes a `classmethod` called `method` that adds 1 to its input. The `forward` method applies a linear transformation and then calls the `method` class method in three different ways (`self.method`, `self.__class__.method`, and `type(self).method`) to process the output.",
            "uuid": "c2954bef-2cdb-499c-9c2b-137daa26b2b5"
        },
        "gpt-4.5": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 306,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 3.563017,
            "file_summary": "The Python file defines a PyTorch neural network module named \"ClassMethod,\" which contains a linear layer and a class method \"method\" performing a simple addition operation. In its forward method, the module processes input through the linear transformation, then applies the class method multiple times before returning the combined outcome. Additionally, a model instance and example input tensors are created to demonstrate usage of the defined module.",
            "uuid": "d7ca2a4e-c083-4d4d-840a-ef0da1a813b7"
        },
        "gemini-2.5": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 961,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 7.812061,
            "file_summary": "This Python file defines a PyTorch neural network module named `ClassMethod`. The module includes a `@classmethod` called `method` that performs a simple addition. The main functionality demonstrated in the `forward` method is applying a linear layer to the input and then calling this class method multiple times using different access patterns (`self.method`, `self.__class__.method`, `type(self).method`).",
            "uuid": "3bed3b6c-1e74-4905-b037-5ad386e90da3"
        },
        "Claude3.7": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 356,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.483146,
            "file_summary": "The file \"class_method.py\" defines a PyTorch neural network module called ClassMethod that demonstrates how class methods are inlined during tracing. The module contains a simple linear layer and a forward method that applies the linear transformation to the input and then multiplies the result by three applications of the same class method (accessed through different syntax forms), which simply adds 1 to its input.",
            "uuid": "79ab3343-9c13-455f-8f26-664570077f58"
        },
        "mistral-small": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 367,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.749953,
            "file_summary": " The main functionality of the file \"class_method.py\" is to define a custom PyTorch module named `ClassMethod`. This module contains a class method named `method` that adds 1 to its input. The `forward` method of the module applies a linear transformation to its input, then multiplies the result with the output of the `method` class method, called on the module itself, its class, and its type.",
            "uuid": "67b2c14c-aa54-47e1-bb33-ac178d64862e"
        },
        "mistral-7b": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "total_tokens": 387,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.352418,
            "file_summary": " The given Python file defines a custom PyTorch neural network class named `ClassMethod`. This class contains a class method `method` that takes an input `x` and returns the result of applying the method three times, once for each occurrence of `self.method(x)`, `self.__class__.method(x)`, and `type(self).method(x)`. The main functionality of the file is to demonstrate the use of class methods in PyTorch and their inlining during tracing.",
            "uuid": "027b59e6-a289-4410-afad-da3f338c9b03"
        },
        "cohere-v1": {
            "file_name": "/torch/_export/db/examples/class_method.py",
            "number_of_lines": 23,
            "project_name": "pytorch",
            "file_summary": "The code defines a class called ClassMethod that inherits from torch.nn.Module and has a method called method which is a class method. This class also has a forward method that takes input x, processes it using a Linear layer and applies the class method on the output. The example code at the end instantiates the ClassMethod model and prepares it for inference.",
            "total_tokens": 315,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.132707,
            "uuid": "6cd5cbfd-5ac0-4814-9c53-b9f333e87257"
        }
    },
    "/scripts/export/update_schema.py": {
        "gpt-4o": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 994,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 3.083895,
            "file_summary": "The Python file `update_schema.py` is a script designed to update and validate schema files used in PyTorch while ensuring version compatibility for the schema and treespec definitions. It provides functionality to generate updated schema and associated files, with options for dry-run and forced unsafe updates. The script checks for compatibility, generates headers, and writes YAML, C++ header, and Thrift schema files to specified paths.",
            "uuid": "335b3b85-f78a-4671-9d3c-fb96d3c0b3d9"
        },
        "gpt-4.1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1030,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 5.209708,
            "file_summary": "The main functionality of update_schema.py is to update, validate, and optionally write new versions of PyTorch export schema files (in YAML, C++, and Thrift formats), ensuring that schema and treespec versions do not regress and that changes maintain compatibility unless explicitly overridden. The script uses command-line arguments to control its behavior (e.g., dry-run, force-unsafe), performs version checks, and integrates with a schema_check module to coordinate the schema update process. If not in dry-run mode, it writes the resulting schema files with appropriate headers and checksums to the specified directory.",
            "uuid": "a859eb40-bbed-4351-9517-a0e9fc0914de"
        },
        "Llama3.3": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1039,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 1.722528,
            "file_summary": "The main functionality of the `update_schema.py` file is to update the schema of a PyTorch project by checking and validating the schema version and treespec version, and then writing the updated schema to YAML, C++ header, and Thrift schema files. The script takes several command-line arguments, including the root directory of the PyTorch project, a dry-run option, and a force-unsafe option to override schema validation. The script ensures that the schema version and treespec version are not downgraded and that the schema is properly formatted and written to the correct files.",
            "uuid": "2f7ec811-db39-4097-a57e-ac9c3b85cadc"
        },
        "gpt-4": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1034,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 2.622999,
            "file_summary": "The Python script `update_schema.py` acts as a tool to update serialization schema for the PyTorch project using command-line arguments provided by the user. It ensures schema version compatibility by checking against predetermined base versions and allows users to conditionally force updates even when schema validations fail through an explicit command-line flag. If not being run in dry-run mode (where updates are just printed), the script writes the updated schema in YAML format, as well as the associated header file for C++ integration and a Thrift schema to their respective files within a specified directory tree in the PyTorch repository.",
            "uuid": "84f1a1b9-a1b4-4250-bd10-ffd9c447b1d7"
        },
        "titan_premier": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 130,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 3.580286,
            "file_summary": "The main functionality of the \"update_schema.py\" file is to update the schema version and generate the corresponding YAML, C++ header, and Thrift schema files. It takes command-line arguments for the root directory of the PyTorch project, whether to perform a dry run, and whether to force an update even if schema validation fails. The script checks the current schema version, compares it with the base version, and updates the schema version if necessary. It then generates the required files with the updated schema version and checksum. If a dry run is specified, the script prints the updated schema instead of writing it to files.",
            "uuid": "b8c5a3c2-046e-4e3a-a495-be55f69a91cf"
        },
        "nova-lite-v1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1167,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 0.882524,
            "file_summary": "The script `update_schema.py` is designed to update and validate the schema for a PyTorch project. It takes a root directory as input and checks the schema version, ensuring it does not downgrade. If the schema is valid, it updates the schema files and writes them to the specified directory, or it can print the schema without writing if the `--dry-run` option is used. The script also allows forcing the update even if the schema validation fails, using the `--force-unsafe` option.",
            "uuid": "e5991622-c9ba-44d7-82bf-1cce0a2cfd5c"
        },
        "Claude3.5": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1264,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 4.367885,
            "file_summary": "The main functionality of the \"update_schema.py\" file is to update and validate schema files for a PyTorch project. It checks for version compatibility, generates new schema content in YAML, C++, and Thrift formats, and either prints the updated schema (in dry-run mode) or writes the new content to the respective files. The script also includes safety checks and options for forcing updates, making it a tool for managing schema versioning and compatibility in the PyTorch codebase.",
            "uuid": "1a80978d-bb5e-480e-91a1-04df7f25a877"
        },
        "nova-pro-v1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1131,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 1.038294,
            "file_summary": "The `update_schema.py` script updates the schema files for a PyTorch project by generating YAML, C++, and Thrift schema files. It validates the schema versions and writes the updated schema to the specified directory unless a dry-run is requested. Various checks and assertions ensure the integrity and versioning of the schema.",
            "uuid": "925e9ac2-59c5-42b2-83ed-4302cd57bad5"
        },
        "gpt-3.5-turbo": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1009,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.045443,
            "file_summary": "The given \"update_schema.py\" file is a script that updates the schema in a PyTorch directory based on certain conditions and user-provided arguments. It checks for schema versions, validates schema changes, generates YAML and C++ headers, and writes them to their respective files in the PyTorch directory. The script handles scenarios where the schema version is not updated correctly and provides options to resolve the issue, including using a force-unsafe flag and detailed error messages.",
            "uuid": "442ddcde-453c-4c68-b045-3a48526c8bdf"
        },
        "Llama3.1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1031,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 4.075416,
            "file_summary": "The main functionality of this Python file, `update_schema.py`, is to update the schema of a PyTorch project by generating new schema files (YAML, C++, and Thrift) based on the current project state. It checks for schema version compatibility and validates the schema before writing the new files, with options for dry-run and force-unsafe modes. The script is designed to be run from the root of the PyTorch directory, and it uses the `torch._export.serde.schema_check` module for schema checking and generation.",
            "uuid": "6f92944c-9e05-4d10-981b-31f6d78480c4"
        },
        "gemini-2.5-flash": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1397,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.235741,
            "file_summary": "This Python script updates serialization schema files (YAML, C++, and Thrift) used by PyTorch's export functionality. It generates new schema content based on the current state defined in the codebase and performs version checks against existing files. The script then writes the updated schema files to a specified directory prefix within the PyTorch source tree, with options for a dry run or forcing the update.",
            "uuid": "409a2ec2-52b9-4eb7-9181-84f4307c24f2"
        },
        "gpt-4.5": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1013,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 4.498155,
            "file_summary": "The provided Python script updates and validates schema-related files within a PyTorch repository by generating YAML, C++, and Thrift schema files based on the results from schema checking. It ensures compatibility by validating versions and checksums, providing options like `--dry-run` (to preview without applying changes) and `--force-unsafe` (to force updates despite potential incompatibilities). If conditions are not met, such as non-existent paths or unexpected schema version downgrades, the script raises appropriate errors.",
            "uuid": "1b47590e-887d-4287-847c-dadb8cb8a465"
        },
        "gemini-2.5": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1371,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 3.960782,
            "file_summary": "This script updates the serialization schema definitions used within the PyTorch project. It generates new schema versions based on the current codebase, performs validation checks for compatibility and version increments, and then writes the updated schema to YAML, C++, and Thrift files. The script allows for dry runs to preview changes and an option to force updates even if validation checks fail.",
            "uuid": "458dd07c-5d77-4a62-ad58-62ba9cd43e58"
        },
        "Claude3.7": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1237,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.965696,
            "file_summary": "This script updates schema files for PyTorch's export functionality by validating schema versions, ensuring compatibility, and writing updated schema content to YAML, CPP header, and Thrift files. It performs version checks to prevent downgrades and requires explicit confirmation via the --force-unsafe flag for potentially breaking changes, with options for dry runs to preview changes before committing them.",
            "uuid": "e74d8420-84a2-4889-89d8-0b05c2111938"
        },
        "mistral-small": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1306,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.851465,
            "file_summary": " The main functionality of the file \"update_schema.py\" is to update and check the schema of the PyTorch library. It takes the root directory of PyTorch as an argument, checks the current schema version, and updates it if necessary. The script also checks for downgrades in schema and treespec versions, and raises an error if any are found. The updated schema is then written to a YAML file, and a C++ header and Thrift schema are also generated.",
            "uuid": "af03b058-aeee-4b9c-8669-3fdd5f91cc11"
        },
        "mistral-7b": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "total_tokens": 1306,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.223007,
            "file_summary": " The main functionality of the file \"update\\_schema.py\" is to validate and update the schema files of the PyTorch library using user-provided arguments. It checks the schema version and treespec version against the latest commit, and if the versions are outdated, it raises an error. If the schema file does not exist, the user can force the update using the \"--force-unsafe\" flag. The updated schema files are then written to the specified directory.",
            "uuid": "cc6446ac-054c-499b-a3ca-7f83b68e01f3"
        },
        "cohere-v1": {
            "file_name": "/scripts/export/update_schema.py",
            "number_of_lines": 91,
            "project_name": "pytorch",
            "file_summary": "The Python script, named update_schema.py, is responsible for updating schema versions in PyTorch. It validates schema and treespec versions, and if there's an issue, it either raises an error or suggests a solution. The script also has an option to force unsafe writing over existing schema files.",
            "total_tokens": 1080,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.002126,
            "uuid": "03cdcb92-f194-4f2b-9389-79af7f339590"
        }
    },
    "/torch/profiler/_pattern_matcher.py": {
        "Llama3.1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 5714,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009
            },
            "latency": 5.090148,
            "file_summary": "The main functionality of this Python file is to define a set of patterns for identifying performance anti-patterns in PyTorch code, and to provide a function `report_all_anti_patterns` that can be used to report all matched anti-patterns in a given PyTorch profiler. The patterns include things like extra CUDA copies, for loop indexing, FP32 matrix multiplication, and more. The function can also benchmark the matched events and provide a summary of the results.",
            "uuid": "180c17d4-ec17-477f-9c1d-cdc568d9059a"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 5700,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 1.487047,
            "file_summary": "The main functionality of the `_pattern_matcher.py` file is to define a set of patterns implemented as classes to identify anti-patterns in PyTorch code related to performance optimization. These patterns include identifying issues such as unnecessary operations, inefficient data loading, incorrect settings in optimizers, and more. The file uses event profiling data to match against these patterns and generate reports with recommendations on how to resolve the identified anti-patterns.",
            "uuid": "fa3cfa98-d749-46ce-ae47-57d72d6a03cc"
        },
        "gpt-4": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 5735,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03
            },
            "latency": 4.984866,
            "file_summary": "The Python file implements a system designed to identify and report various anti-patterns in PyTorch profiling events, which could potentially hinder performance. These patterns include unnecessary copying of data to the GPU, redundant bias settings in combination with BatchNorm2d layers, improper optimizer configurations, synchronized DataLoader operations, and the use of suboptimal tensor operations like element-wise setting in loops. This process involves defining different classes for each pattern where each class contains methods to match specific events, report problems, and suggest improvements or benchmarks. The system allows overall compilation and output of these reports and even writes to JSON if necessary.",
            "uuid": "e5a83658-a0f4-4f30-beff-1a229c28321d"
        },
        "Llama3.3": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 5710,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072
            },
            "latency": 2.24138,
            "file_summary": "The main functionality of the provided Python file is to identify and report anti-patterns in PyTorch code, specifically performance-related issues such as unnecessary CUDA copies, inefficient data loading, and suboptimal tensor operations. The file defines a set of patterns to match these anti-patterns and provides a reporting mechanism to output the findings, including source code locations and recommendations for improvement. The report can also be saved to a JSON file for further analysis.",
            "uuid": "4b63266b-a633-42b1-92c1-380a4012a242"
        },
        "nova-pro-v1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 6653,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042
            },
            "latency": 2.113673,
            "file_summary": "The main functionality of the `_pattern_matcher.py` file is to define a base `Pattern` class and several subclasses that identify specific anti-patterns in PyTorch code, providing detailed reports and optional performance benchmarks for matched events. The file also includes a function to generate and print a comprehensive report of all detected anti-patterns.",
            "uuid": "b1fd33a2-f9b9-4401-8393-e51908faa41f"
        },
        "nova-lite-v1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 6683,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312
            },
            "latency": 1.303635,
            "file_summary": "The main functionality of the `_pattern_matcher.py` file is to define and implement various patterns for detecting anti-patterns in PyTorch code, providing detailed reports and recommendations for optimization. The file includes a base `Pattern` class and several subclasses that identify specific anti-patterns, such as inefficient tensor operations, suboptimal data loading, and improper optimizer configurations. It also includes a function to generate a comprehensive report of detected issues, which can be printed or saved as a JSON file.",
            "uuid": "9b2c84c3-3dc7-41e5-8599-0c02626b0968"
        },
        "Claude3.5": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 7230,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015
            },
            "latency": 4.574387,
            "file_summary": "The main functionality of the \"_pattern_matcher.py\" file is to define and implement various anti-patterns in PyTorch code. It includes a base Pattern class and several specific pattern classes that inherit from it, such as ExtraCUDACopyPattern, FP32MatMulPattern, and OptimizerSingleTensorPattern. The file also provides a function called \"report_all_anti_patterns\" that analyzes a PyTorch profile, identifies matching anti-patterns, and generates a report with suggestions for code optimization.",
            "uuid": "548e898a-2754-4b56-a2a0-00a5281685fe"
        },
        "titan_premier": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 500,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015
            },
            "latency": 11.372683,
            "file_summary": "The Python file named \"_pattern_matcher.py\" is a part of the TorchTidy tool, which is used to analyze PyTorch profiling results and identify anti-patterns. The file contains several classes that inherit from the base class \"Pattern\". Each class represents a specific anti-pattern and implements methods to match the pattern and generate a report.\n\nThe main functionality of the file is to define various anti-patterns and provide methods to match these patterns in the profiling results. The anti-patterns include:\n\n1. Extra CUDA Copy Pattern: This pattern identifies if a constant tensor is created on the CPU and immediately moved to the GPU.\n2. For Loop Indexing Pattern: This pattern identifies if a for loop is used to index a tensor that can be vectorized.\n3. FP32 MatMul Pattern: This pattern identifies if the FP32 version of the MatMul operation is used instead of the TF32 version on GPUs that support TF32.\n4. Optimizer Single Tensor Pattern: This pattern identifies if the single-tensor version of an optimizer is used instead of the multi-tensor version.\n5. Synchronized DataLoader Pattern: This pattern identifies if the DataLoader is running with a synchronized implementation instead of an asynchronous implementation.\n6. Grad Not Set To None Pattern: This pattern identifies if the gradient is set to zero instead of None in the zero_grad method.\n7. Conv2d Bias Followed By BatchNorm2d Pattern: This pattern identifies if bias is enabled in Conv2d that is followed by BatchNorm2d.\n8. MatMul Dim In FP16 Pattern: This pattern identifies if the dimension of the input tensors for the matmul operation is not aligned when using FP16.\n\nThe file also includes helper functions to extract information from the profiling results, such as the source code location and input shapes and types.\n\nThe main function \"report\\_all\\_anti\\_patterns\" takes a profiling object, a boolean flag to enable benchmarking, a boolean flag to enable printing the report, and an optional directory to save the report in JSON format. It iterates through all the anti-patterns, matches them in the profiling results, generates a report for each pattern, and",
            "uuid": "be4f08c6-7f67-42bd-8460-591dd17fd6c8"
        },
        "gpt-4.1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 5731,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008
            },
            "latency": 3.53415,
            "file_summary": "The file \"_pattern_matcher.py\" defines a set of pattern-matching classes that analyze PyTorch profiler traces to identify inefficient or suboptimal code patterns (anti-patterns) in deep learning workflows. Each pattern targets specific inefficiencies\u2014such as unnecessary CPU-to-GPU tensor copies, lack of vectorization, suboptimal optimizer usage, inefficient data loading, or misaligned matrix multiplication\u2014and provides recommendations and optional benchmark-based speedup estimates for remedying these issues. The main functionality is to scan profiler data, detect these anti-patterns, report findings with actionable advice, and optionally generate a JSON report summarizing the results.",
            "uuid": "7e94e9f2-3bbe-495e-a892-a9de307d3b79"
        },
        "gpt-4o": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 5701,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01
            },
            "latency": 2.117479,
            "file_summary": "The `_pattern_matcher.py` file provides a framework for identifying and reporting anti-patterns in PyTorch code by analyzing profiling data. It defines a base `Pattern` class and specific subclasses for detecting inefficiencies, such as unnecessary data copying between CPU and GPU, non-vectorized tensor indexing, suboptimal tensor operations, or improper configuration of PyTorch modules. Additionally, it includes functionality for detailed event traversal, benchmarking, and generating reports to help developers optimize their code.",
            "uuid": "ddd95813-a6e4-4744-8a0e-6742e74f8314"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 7038,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006
            },
            "latency": 2.553372,
            "file_summary": "This Python file defines a framework for detecting performance anti-patterns within a PyTorch profiler trace. It includes a base `Pattern` class and several subclasses, each implementing logic to identify specific inefficiencies like unnecessary CUDA copies, suboptimal optimizer usage, or misconfigured layers. The main function `report_all_anti_patterns` applies these patterns to a given profile and generates a report detailing the findings and suggested optimizations.",
            "uuid": "bbe76fd4-b369-4f6e-8dab-4edec3a681a7"
        },
        "gpt-4.5": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 5711,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 7.605196,
            "file_summary": "The provided Python file defines a framework that analyzes performance profiles from PyTorch models by matching them against various predefined anti-patterns, such as wasteful CUDA tensor transfers, inefficient optimizer implementations, or lack of vectorization. It includes logic to identify problematic code patterns within PyTorch execution (e.g., unnecessary tensor copy from CPU to GPU or using single-tensor optimizers) and to generate human-readable performance improvement recommendations. Additionally, it can produce summarized reports and JSON output detailing matched anti-patterns along with suggestions for optimization.",
            "uuid": "f7461454-516a-48ea-b71a-9dffb697589e"
        },
        "gemini-2.5": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 7580,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 9.746848,
            "file_summary": "This Python file defines a framework for analyzing PyTorch profiler output to detect common performance anti-patterns. It includes a base `Pattern` class and several specific subclasses that identify issues like unnecessary CUDA copies, inefficient data loading, or suboptimal operator usage. The main function `report_all_anti_patterns` uses these patterns to scan a given profile, generate a report summarizing findings with source code locations, and optionally benchmark potential speedups.",
            "uuid": "76f78297-c9a7-45a5-9731-5a55742c631a"
        },
        "Claude3.7": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 7223,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 5.044003,
            "file_summary": "The file \"_pattern_matcher.py\" implements a pattern matching system for PyTorch code optimization, detecting anti-patterns in profiled PyTorch code. It defines a base Pattern class and several specific pattern detectors (like ExtraCUDACopyPattern, FP32MatMulPattern, etc.) that identify common inefficient coding practices in PyTorch applications and provide optimization recommendations. The file also includes a reporting mechanism that summarizes detected issues and can output them in both human-readable format and JSON.",
            "uuid": "c8b2074d-9ae8-41a1-b21f-dfd0dc38d37f"
        },
        "mistral-small": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 7586,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 3.824137,
            "file_summary": " The given Python file is a script for analyzing PyTorch models and identifying potential performance issues. It defines several classes that inherit from a base class called `Pattern`. Each of these classes represents a specific performance issue or anti-pattern that can be detected in PyTorch models.\n\nThe main functionality of the file is to analyze a given PyTorch model and report any detected performance issues. This is done by creating instances of the various `Pattern` classes and calling their `matched_events()` method, which returns a list of events in the model that match the pattern. The script then prints out a summary of the detected issues and their potential impact on performance.\n\nOverall, the main functionality of the file is to provide a tool for analyzing PyTorch models and identifying potential performance issues, with the goal of helping users optimize their models for better performance.",
            "uuid": "a7e4bd64-88f1-4dfb-8dbf-a137c0734521"
        },
        "mistral-7b": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "total_tokens": 7596,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.683402,
            "file_summary": " The given Python file defines a class called `Pattern` which serves as a base class for various patterns to identify potential performance issues in PyTorch code. These patterns include \"Extra CUDA Copy Pattern\", \"For Loop Indexing Pattern\", \"FP32 MatMul Pattern\", \"Optimizer Single Tensor Pattern\", \"Synchronized DataLoader Pattern\", \"Grad Not Set To None Pattern\", and \"Conv2dBiasFollowedByBatchNorm2dPattern\". Each pattern subclasses the `Pattern` class and implements methods to match specific PyTorch events based on their names and input shapes. The matched events are reported with a message describing the issue and a suggested solution. Additionally, the file provides helper functions for source code location and input shapes/dtypes of events. The `report_all_anti_patterns` function is used to report all matched events and their corresponding issues.",
            "uuid": "bc3126cf-c374-4aca-ab24-1c52768aa906"
        },
        "cohere-v1": {
            "file_name": "/torch/profiler/_pattern_matcher.py",
            "number_of_lines": 663,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, named _pattern_matcher.py, is a tool for analyzing PyTorch code performance. It defines a set of patterns to identify anti-patterns in the code and provides recommendations to optimize them. The script traverses the event tree of a PyTorch profiler output, matches events against predefined patterns, and suggests improvements for performance enhancements.",
            "total_tokens": 6326,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.781081,
            "uuid": "36e23a74-b1e7-4760-a2fe-2a5e6c0b47e3"
        }
    },
    "/torch/distributed/checkpoint/logging_handlers.py": {
        "nova-lite-v1": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 248,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 1.111085,
            "file_summary": "The main functionality of the \"logging_handlers.py\" file is to configure a custom logger named \"dcp_logger\" by associating it with a `NullHandler` from the `torch.distributed.logging_handlers` module. This setup ensures that any logging events for \"dcp_logger\" are effectively ignored. The file imports necessary modules and updates the logging handlers to include the custom logger.",
            "uuid": "070d1f71-9008-44fd-8edc-34b1a8439e3c"
        },
        "nova-pro-v1": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 243,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 1.390854,
            "file_summary": "The `logging_handlers.py` file configures logging for a specific logger named \"dcp_logger\" by associating it with a `NullHandler`, which effectively silences all log messages for this logger, preventing them from being output unless further handlers are added. It imports necessary modules and updates a global registry of log handlers (`_log_handlers`) to include this configuration.",
            "uuid": "d00c7efd-96ac-4027-924f-86c4d8636598"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 798,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 7.929991,
            "file_summary": "This Python script configures logging by defining a specific logger named \"dcp_logger\". It then updates a central dictionary (`_log_handlers`), likely used within the `torch.distributed` library, to associate this logger name with a `logging.NullHandler`. This effectively ensures that log messages from \"dcp_logger\" are discarded by default unless another handler is explicitly configured elsewhere.",
            "uuid": "325465b5-dea5-48a5-994b-8b56c0184e9d"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 628,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 3.769734,
            "file_summary": "This Python file configures a default logging handler within PyTorch's distributed logging system. It registers a `logging.NullHandler` under the name \"dcp_logger\". This setup ensures that logs directed to \"dcp_logger\" are discarded by default unless explicitly handled elsewhere.",
            "uuid": "f8b3788d-177e-420c-8a40-0d8165298077"
        },
        "Llama3.3": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 249,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 2.583167,
            "file_summary": "The main functionality of the `logging_handlers.py` file is to configure logging handlers for a distributed computing framework, specifically by adding a custom logger named \"dcp_logger\" to the existing `_log_handlers` dictionary. This logger is initialized with a `NullHandler`, which means that it will not output any log messages by default. The file updates the `_log_handlers` dictionary from the `torch.distributed.logging_handlers` module, allowing for customization of logging behavior in the application.",
            "uuid": "3f4173b7-d23d-4332-808f-934dec962327"
        },
        "titan_premier": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 104,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 3.876885,
            "file_summary": "The file \"logging_handlers.py\" is a Python script that imports the logging module and defines a constant DCP_LOGGER_NAME. It also updates the _log_handlers dictionary with a new entry for DCP_LOGGER_NAME, which is set to a NullHandler instance. The file does not define any functions or classes, and the __all__ list is empty. This file is likely used to configure logging for a specific component or module in a larger software system.",
            "uuid": "4c4e12b2-2f6d-4a7f-9b81-28c7eab84856"
        },
        "Claude3.5": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 274,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.612529,
            "file_summary": "The file \"logging_handlers.py\" is primarily focused on setting up logging configurations for a distributed computing environment. It imports the necessary logging modules, defines a constant for a specific logger name (DCP_LOGGER_NAME), and updates the _log_handlers dictionary with a NullHandler for the defined logger. This file appears to be part of a larger system for managing logging in a distributed computing context, possibly related to PyTorch's distributed computing functionality.",
            "uuid": "6906c50b-2d0a-4755-adb9-055d1f7e5f4a"
        },
        "gpt-4o": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 231,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 1.714818,
            "file_summary": "The file defines and configures a custom logger named \"dcp_logger\" by associating it with a `NullHandler`, ensuring that no log messages are output by default unless explicitly configured. It modifies the `_log_handlers` dictionary from the `torch.distributed.logging_handlers` module to include this logger. Additionally, it initializes the `__all__` list as empty, possibly indicating no public API exports.",
            "uuid": "2ab0fe6d-b192-41b5-a688-99417c7670f3"
        },
        "Llama3.1": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 227,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 3.287956,
            "file_summary": "The main functionality of this Python file is to configure logging handlers for a distributed computing platform (DCP) using the PyTorch library. It updates the `_log_handlers` dictionary with a new handler named \"dcp_logger\" that uses a `NullHandler`, which discards all logging messages. This file is likely used to set up logging for a distributed computing application.",
            "uuid": "768f31b3-68ed-44fb-948b-069688919d78"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 216,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 1.924148,
            "file_summary": "The main functionality of the `logging_handlers.py` file is to import the necessary modules for logging, specifically from `torch.distributed.logging_handlers`, and define a logger with the name `DCP_LOGGER_NAME`. It then updates the `_log_handlers` dictionary with the `DCP_LOGGER_NAME` and assigns a `logging.NullHandler()` to it.",
            "uuid": "11d23ec2-d911-4fa1-a78f-022dc7e7158d"
        },
        "gpt-4": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 235,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 2.94605,
            "file_summary": "The file `logging_handlers.py` is primarily involved in configuring and managing logging handlers for distributed computing processes, specifically within a Python environment using the `logging` module. It imports and updates logging handlers from `torch.distributed.logging_handlers` to specify a `NullHandler` for a logger named \"dcp_logger\". This configuration indicates that the logger for distributed computing processes will intentionally avoid processing log messages by default, until further configuration is potentially applied.",
            "uuid": "334ae66f-228e-4a54-bfcb-f17445d484a6"
        },
        "gpt-4.1": {
            "file_name": "/torch/distributed/checkpoint/logging_handlers.py",
            "number_of_lines": 15,
            "project_name": "pytorch",
            "total_tokens": 233,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 2.629417,
            "file_summary": "The main functionality of this file is to set up a logging handler for a custom logger named \"dcp_logger\" by registering a logging.NullHandler() under this name within the torch.distributed.logging_handlers._log_handlers dictionary. This prevents warnings for unhandled logs from \"dcp_logger\" if no other handlers are configured. The file also defines an empty __all__ list, specifying no public API from this module.",
            "uuid": "937f35e3-b9d3-4fdb-b60b-bed51b3f558a"
        }
    },
    "/torch/_inductor/codegen/rocm/rocm_template_buffer.py": {
        "gpt-4": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 359,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 2.289771,
            "file_summary": "The Python file \"rocm_template_buffer.py\" defines a class called ROCmTemplateBuffer that specializes a TemplateBuffer for ROCm (a platform for GPU computing). This class manages buffers and kernels in memory while computing. It has features like workspace size handling and kernel rendering through callable parameters, encapsulating the attributes and functionality necessary for managing templates in the ROCm ecosystem.",
            "uuid": "5e17d770-f9f0-4f31-957f-02b7234d3ed5"
        },
        "titan_premier": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 130,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 4.215424,
            "file_summary": "The main functionality of the file \"rocm_template_buffer.py\" is to define a class called \"ROCmTemplateBuffer\" that inherits from the \"TemplateBuffer\" class. This class takes in a layout, a sequence of buffers, a callable function for rendering the kernel, a workspace size, and a template object. It also includes a method called \"get_workspace_size\" that returns the workspace size if it is not None, otherwise it returns 0. The file imports necessary modules such as \"collections.abc\", \"typing\", and \"typing_extensions\" to support the class definition.",
            "uuid": "d70c645e-8fb9-41c9-b110-061e72781e20"
        },
        "nova-pro-v1": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 385,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 0.791812,
            "file_summary": "The `rocm_template_buffer.py` file defines a `ROCmTemplateBuffer` class that extends the `TemplateBuffer` class. It initializes with a layout, input buffers, a kernel rendering function, workspace size, and a template. It also provides a method to retrieve the workspace size associated with the buffer.",
            "uuid": "2a016ea6-4e23-4cc7-aa2b-87ab35e2d902"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 487,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 2.551296,
            "file_summary": "This Python file defines a class `ROCmTemplateBuffer` that extends `TemplateBuffer` to represent a buffer associated with a ROCm (Radeon Open Compute platform) template. It stores information about the buffer's layout, input buffers, a function to render the kernel, and the required workspace size in global memory. The class also provides a method to access this workspace size, defaulting to zero if not specified.",
            "uuid": "b6bbe5c8-1292-4346-a14b-af05bf2de0d1"
        },
        "Llama3.1": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 385,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 3.630215,
            "file_summary": "The main functionality of the `rocm_template_buffer.py` file is to define a class `ROCmTemplateBuffer` that inherits from `TemplateBuffer` and represents a buffer for ROCm (Radeon Open Compute) templates. The class takes in a layout, inputs, a kernel render function, workspace size, and a template, and provides a method to get the workspace size. This class is likely used in a larger system for managing and rendering ROCm templates.",
            "uuid": "f69b1455-c84f-4759-a3be-80aef8039c12"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 366,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 0.931209,
            "file_summary": "The main functionality of the \"rocm_template_buffer.py\" file is to define a class called ROCmTemplateBuffer that extends the TemplateBuffer class. This class represents a buffer template specific to ROCm (Radeon Open Compute) platform, with attributes such as layout, inputs, kernel rendering function, workspace size, and template. It also includes a method to retrieve the workspace size required by the template.",
            "uuid": "bc40c5df-a99e-407d-a6d9-2db26eec1c89"
        },
        "gpt-4o": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 388,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 1.095772,
            "file_summary": "The file defines a class `ROCmTemplateBuffer`, which extends `TemplateBuffer` and provides functionality for managing a template buffer specific to ROCm (a platform for GPU-accelerated computing). It initializes the buffer with a layout, input buffers, a kernel rendering function, a workspace size, and a linked template, while also exposing a method to retrieve the allocated global memory size for the template. This class serves as an abstraction layer for working with ROCm-related buffer templates in the context of GPU programming.",
            "uuid": "a116a390-f407-4980-a5e1-d01283688faf"
        },
        "Claude3.5": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 448,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.718822,
            "file_summary": "The file \"rocm_template_buffer.py\" defines a class called ROCmTemplateBuffer, which is a subclass of TemplateBuffer. This class is designed to represent a template buffer specifically for ROCm (Radeon Open Compute) operations, including functionality to manage workspace size and store a reference to a ROCm template. The class provides a method to retrieve the workspace size, which is useful for memory management in ROCm-based computations.",
            "uuid": "258d96a4-5d44-4e02-8ab6-682f80990ad8"
        },
        "gpt-4.1": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 364,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 1.340808,
            "file_summary": "The file defines the ROCmTemplateBuffer class, which extends the TemplateBuffer class to represent a buffer template specific to AMD ROCm GPU kernels. It manages attributes such as memory layout, input buffers, a kernel rendering function, associated workspace size requirements, and a template reference for ROCm-specific operations. The class also provides a method to retrieve the allocated workspace size in bytes for kernel execution.",
            "uuid": "8805f4ae-bd4f-4d49-a300-63d43c52e933"
        },
        "Llama3.3": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 407,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 1.44217,
            "file_summary": "The main functionality of the `rocm_template_buffer.py` file is to define a class `ROCmTemplateBuffer` that inherits from `TemplateBuffer` and provides additional functionality for managing a template buffer on a ROCm (Radeon Open Compute) platform. This class takes in a layout, input buffers, a kernel render function, workspace size, and a template, and provides a method to retrieve the workspace size. The class is designed to work with a specific template and buffer layout, and is likely part of a larger system for managing compute kernels on ROCm devices.",
            "uuid": "96a4b090-1b67-4844-9381-932ea6bf8498"
        },
        "nova-lite-v1": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 407,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 0.736836,
            "file_summary": "The main functionality of the `rocm_template_buffer.py` file is to define a class `ROCmTemplateBuffer` that extends `TemplateBuffer` for managing buffer layouts and inputs in the context of ROCm (Radeon Open Compute) templates. This class initializes with a layout, input buffers, a kernel rendering function, and a workspace size, and it provides a method to retrieve the workspace size needed for the template.",
            "uuid": "267ce96d-f4d4-4724-bd1b-637c6f66eeff"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 646,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 2.186842,
            "file_summary": "This Python file defines the `ROCmTemplateBuffer` class, which extends `TemplateBuffer` to represent a buffer specifically associated with a ROCm template. It stores information such as the buffer layout, input dependencies, a kernel rendering function, and the required workspace memory size. The class provides a method to retrieve this workspace size, aiding in memory allocation for ROCm operations.",
            "uuid": "84f21797-550a-4a2a-847b-36ba1d4c8dfb"
        },
        "Claude3.7": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 423,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.697838,
            "file_summary": "The `rocm_template_buffer.py` file defines a `ROCmTemplateBuffer` class that extends `TemplateBuffer` to represent a buffer specifically for ROCm (AMD GPU) templates, adding functionality to track workspace memory requirements. It maintains a reference to its parent template and provides a method to retrieve the workspace size needed for GPU execution.",
            "uuid": "4d5f9044-80a4-4d57-a514-d65381f094ab"
        },
        "mistral-small": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 456,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.896598,
            "file_summary": " The main functionality of the file \"rocm_template_buffer.py\" is to define a class named `ROCmTemplateBuffer` which inherits from the `TemplateBuffer` class. This class is used to create a buffer template for ROCm (Radeon Open Compute Platform) with specific layout, inputs, a kernel render function, workspace size, and a ROCm template. The `get_workspace_size` method is used to retrieve the workspace size of the buffer template.",
            "uuid": "1c117ea8-6d32-46e2-8028-666d6422ceee"
        },
        "mistral-7b": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "total_tokens": 457,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.27862,
            "file_summary": " The Python file defines a class named `ROCmTemplateBuffer` that extends `TemplateBuffer`. It initializes an instance of `ROCmTemplateBuffer` with a given layout, a sequence of input buffers, a callable for making kernel renders, and a workspace size for ROCm (Radeon Open Compute) templates. The main functionality of the file is to create and initialize an instance of `ROCmTemplateBuffer` with the specified parameters and set its workspace size.",
            "uuid": "a62777be-0f28-4bc1-9345-958b0019b7d5"
        },
        "gpt-4.5": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "file_summary": "The provided Python file defines a class `ROCmTemplateBuffer`, inheriting from `TemplateBuffer`, designed for handling buffer-related functionality specifically for ROCm (Radeon Open Compute) computing. It includes attributes for managing global workspace memory and associating it with a ROCm-specific template object. Additionally, it provides a method `get_workspace_size()` to retrieve the global memory workspace size required by the template.",
            "total_tokens": 367,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 7.009172,
            "uuid": "fc84de16-bb6b-44db-89c3-8b5f1262c578"
        },
        "cohere-v1": {
            "file_name": "/torch/_inductor/codegen/rocm/rocm_template_buffer.py",
            "number_of_lines": 28,
            "project_name": "pytorch",
            "file_summary": "The python file defines a class called ROCmTemplateBuffer which appears to be related to buffer management and template rendering in the context of ROCm (Rapid Operating System Development for Heterogeneous Systems). It has methods to initialize the class instance and retrieve the required workspace size. The class likely serves as an intermediary to interact with graphics processing unit (GPU) buffers and templates.",
            "total_tokens": 379,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.071557,
            "uuid": "ab757002-ea57-416f-840f-4fcd8bd163b9"
        }
    },
    "/torch/_inductor/runtime/triton_compat.py": {
        "gpt-4o": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1068,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 1.228866,
            "file_summary": "The `triton_compat.py` file is a compatibility layer for handling differences between versions of the `triton` library and ensuring fallback options when the library is unavailable or certain features are missing. It defines alternative implementations or placeholders for essential `triton` components like `Config`, `CompiledKernel`, `GPUTarget`, `libdevice`, and `math`, while also addressing GPU-specific configurations like warp size. Additionally, it exposes a unified interface for functionality across `triton` versions and integrates gracefully with PyTorch features like `autograd.profiler`.",
            "uuid": "baa8b6d9-5921-48ed-bcf2-1fd5e10a3866"
        },
        "Claude3.5": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1262,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.915387,
            "file_summary": "The main functionality of the \"triton_compat.py\" file is to provide compatibility and fallback mechanisms for the Triton library in Python. It attempts to import Triton and its components, and if Triton is not available, it creates mock objects and functions to prevent errors. The file also includes version-specific imports and adaptations to handle different Triton versions and GPU architectures.",
            "uuid": "8f69e75d-9be7-4bce-9653-c7123961a588"
        },
        "Llama3.1": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1036,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 3.44362,
            "file_summary": "This Python file, `triton_compat.py`, provides compatibility functions and classes for working with the Triton library, a Python-based programming language for writing high-performance GPU code. The file checks if Triton is installed and imports its modules, providing fallback implementations if it's not available. It also defines utility functions, such as `cc_warp_size`, and exports various Triton-related classes and functions for use in other parts of the codebase.",
            "uuid": "425dc708-aba8-4bdd-acc8-2f072384cd16"
        },
        "gpt-4.1": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1079,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 1.736733,
            "file_summary": "The file triton_compat.py serves as a compatibility layer between PyTorch and Triton, providing fallbacks and alternative implementations for key Triton classes, functions, and modules if the Triton package (or certain parts of it) is not installed or has breaking API differences. It conditionally imports or defines objects such as Config, CompiledKernel, GPUTarget, mathematical utilities, and error classes to ensure seamless usage regardless of the Triton version or presence. The file also defines helper functionality (e.g., cc_warp_size) and exposes a consistent interface via its __all__ list for downstream modules.",
            "uuid": "925dd347-9ba2-4450-b6fb-117721990643"
        },
        "nova-lite-v1": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1141,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 1.025441,
            "file_summary": "The `triton_compat.py` file provides compatibility and fallback mechanisms for the Triton library, ensuring that the code can run even if Triton is not installed. It includes conditional imports and fallback definitions for Triton-specific classes and functions, allowing the script to work with or without Triton. The file also includes utility functions like `cc_warp_size` to determine the warp size based on the compute capability.",
            "uuid": "7b5664fe-8656-4fad-bf03-a40b79da65e3"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1667,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 3.692577,
            "file_summary": "This Python file acts as a compatibility layer for the Triton library. It attempts to import necessary components from Triton if available, providing fallback definitions or raising errors if Triton is not installed or specific features are missing. This allows code depending on Triton to handle its absence gracefully and potentially support variations across Triton versions.",
            "uuid": "61961963-75b9-4457-9c41-701fb8414ef8"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1350,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 4.487859,
            "file_summary": "This Python file acts as a compatibility layer for the Triton library, a language and compiler for writing highly efficient GPU code. It attempts to import various Triton components and, if they are unavailable (e.g., Triton not installed or an older version is used), it defines fallback stubs, mock objects, or error-raising functions. This ensures that code relying on Triton can still run or provide informative errors, and it helps manage API differences across Triton versions.",
            "uuid": "a4631681-9899-4194-9b86-9266ad832ce1"
        },
        "nova-pro-v1": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1117,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 0.919004,
            "file_summary": "The `triton_compat.py` file provides compatibility and fallback mechanisms for using the Triton library with PyTorch, handling imports, defining fallback classes and functions if Triton is not installed, and offering utility functions like `cc_warp_size` to determine warp sizes based on compute capability.",
            "uuid": "4613b599-1577-414e-b5eb-b2c314d76d38"
        },
        "gpt-4": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1047,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 2.36509,
            "file_summary": "The python file \"triton_compat.py\" is designed to serve as a compatibility layer for the Triton programming language, primarily used for writing high-performance GPU code. The file handles imports and attempts to maintain compatibility if certain modules or features are missing, such as different GPU targets, explicit autotuning errors, and access to specific mathematical libraries or functions like `libdevice`. Additionally, it provides fallback definitions and raises errors if the Triton package itself is not installed, ensuring that the user is aware of missing dependencies or library versions.",
            "uuid": "55b9217c-5d4c-4562-9759-3187aa9a6c5b"
        },
        "Claude3.7": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1288,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 6.527169,
            "file_summary": "The file \"triton_compat.py\" provides compatibility layers for working with the Triton library in Python, handling cases where Triton may or may not be installed. It imports Triton-related components when available and creates mock objects with appropriate error handling when Triton is not installed, ensuring code that depends on Triton can gracefully handle its absence. Additionally, it includes utility functions like determining warp size based on compute capability and handles differences between various versions of Triton by adapting to API changes.",
            "uuid": "331ac220-f245-4d16-863a-6eb19a63ce02"
        },
        "titan_premier": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 133,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 25,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 3.536732,
            "file_summary": "The main functionality of the \"triton_compat.py\" file is to provide compatibility with the Triton package for Python. It imports necessary modules and defines functions to handle different versions of Triton. The file also includes a function to determine the warp size based on the compute capability and defines a mock version of the torch.autograd.profiler for compatibility. The file exports several classes and functions, including Config, CompiledKernel, OutOfResources, KernelInterface, ASTSource, GPUTarget, tl, _log2, libdevice, math, triton, and cc_warp_size.",
            "uuid": "340979ab-9c08-4e04-8619-45019eabbf8a"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1026,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 1.034338,
            "file_summary": "The main functionality of the `triton_compat.py` file is to handle compatibility issues and imports based on the availability of the `triton` package. It imports various modules and classes from `triton` if it is available, otherwise, it defines placeholder classes and functions to prevent import errors. Additionally, it provides a function `cc_warp_size` to calculate the warp size based on the compute capability of the GPU.",
            "uuid": "5f86baf8-2293-4c5b-af02-a79d131f4126"
        },
        "Llama3.3": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1069,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 1.758462,
            "file_summary": "The main functionality of the provided Python file, `triton_compat.py`, is to provide compatibility and utility functions for working with the Triton library, a Python-based programming language and compiler for writing high-performance GPU code. The file defines various classes, functions, and variables that either wrap or mock Triton's functionality, allowing for smoother integration and fallbacks when Triton is not installed or when dealing with different versions of the library. This compatibility layer ensures that code using these utilities can run with or without Triton installed, making it easier to develop and test applications that optionally utilize Triton for GPU acceleration.",
            "uuid": "c614dce9-6272-4a84-a56a-f4e119c4c722"
        },
        "mistral-small": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1348,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.54094,
            "file_summary": " The main functionality of the file \"triton_compat.py\" is to provide compatibility and fallback functionality for the \"triton\" package, which is a GPU-accelerated library for high-performance machine learning. The file checks if the \"triton\" package is installed, and if not, it defines placeholder classes and functions to prevent runtime errors. If the \"triton\" package is installed, it imports necessary modules and classes, and provides some additional functionality such as defining missing classes, functions, and constants. The file also defines a function \"cc_warp_size\" which returns the warp size based on the compute capability of the GPU.",
            "uuid": "1bbc538f-cf11-41ad-89a1-df263663d08c"
        },
        "mistral-7b": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "total_tokens": 1345,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.760855,
            "file_summary": " This Python file, named \"triton\\_compat.py\", is a compatibility module for the Triton deep learning compiler. It provides a way to import and use Triton functions and modules based on the availability of the Triton package. If Triton is installed, the file imports the necessary modules and provides aliases for functions and classes that may have been moved or renamed in newer versions. If Triton is not installed, it defines placeholder functions and classes with the same names to prevent errors. The main functionality of the file is to enable the use of Triton with different versions and to provide fallbacks for missing functions or modules.",
            "uuid": "bd139059-6b6c-47f3-9683-9cbd6b19224e"
        },
        "gpt-4.5": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "file_summary": "This Python file serves as a compatibility layer for Triton, handling conditional imports and fallbacks based on Triton's availability, thus providing definitions to prevent runtime issues if Triton isn't installed. It appropriately configures relevant Triton functionalities like math libraries, error exceptions, GPU architecture detection, and warp size based on the available backend (CUDA/HIP). The objective is to maintain compatibility and stability of the larger codebase relying on Triton within diverse runtime environments.",
            "total_tokens": 1046,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 9.09946,
            "uuid": "eb3f79bc-23ae-4749-a32e-820eacb69765"
        },
        "cohere-v1": {
            "file_name": "/torch/_inductor/runtime/triton_compat.py",
            "number_of_lines": 142,
            "project_name": "pytorch",
            "file_summary": "The Triton Compatibility module ensures compatibility between PyTorch and the Triton library. It provides a bridge between the two frameworks, enabling the use of Triton's functionality within PyTorch. The module handles import errors and enables key features such as autotuning and GPU support while also offering workarounds for torch.autograd.profiler.",
            "total_tokens": 1043,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 0.963724,
            "uuid": "750dee27-5f78-41b5-8d68-d359594adc6d"
        }
    },
    "/torch/export/_trace.py": {
        "nova-pro-v1": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 22966,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 3.646177,
            "file_summary": "The main functionality of the file is to provide a set of functions and utilities for exporting PyTorch models or functions into an intermediate representation (IR) suitable for further optimizations or deployment. It includes functionalities for tracing models, handling dynamic shapes, producing guards and constraints, and verifying the exported IR. The file supports both strict and non-strict tracing modes and provides utilities for managing fake tensors and constants during the export process.",
            "uuid": "0f957e97-14ac-40c1-b9ee-159a9fcba3aa"
        },
        "gemini-2.5": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 23760,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.518425,
            "file_summary": "This Python file defines the core logic for tracing PyTorch `nn.Module` instances or callables into an `ExportedProgram`, which is a serialized representation of a PyTorch model. It orchestrates the tracing process using TorchDynamo, converts the model to an ATen Intermediate Representation (IR), and handles various configurations such as dynamic shapes, module call signatures, and preservation of model state. The file also includes utilities for managing export artifacts, verifying the integrity of the exported graph, and logging the export process for debugging and analytics.",
            "uuid": "045251b1-79e7-44e1-b0b6-2dc1fefaaac3"
        },
        "Llama3.3": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 18573,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 3.864814,
            "file_summary": "The main functionality of the given Python file is to implement the PyTorch export functionality, which involves tracing a PyTorch module's forward function or a callable with PyTorch operations and producing an `ExportedProgram` containing the traced method. The file provides two main functions, `_export_for_training` and `_export`, which handle tracing for training and export purposes, respectively. The tracing process involves creating a graph module, generating a graph signature, and producing runtime asserts. The file also includes various utility functions for tasks such as verifying the graph module, processing export inputs, and logging export usage.",
            "uuid": "62aebab1-cff0-4cb4-9e37-ded462b0a418"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 23658,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 2.484585,
            "file_summary": "This Python file implements the core tracing logic for `torch.export`, converting a PyTorch module or callable into an `ExportedProgram`. It leverages `torch._dynamo` and `aot_autograd` to capture the model's computation graph, handling dynamic shapes, constraints, and metadata like graph signatures and state dictionaries. The process involves fakifying inputs, tracing the model, converting the graph to ATen IR, and collecting necessary information for the final `ExportedProgram` representation.",
            "uuid": "1d70e9f1-6bbc-4e79-9877-5528fe7dc72e"
        },
        "Claude3.5": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 24993,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 9.003405,
            "file_summary": "This Python file contains the implementation of the `_export` function and related helper functions for exporting PyTorch modules to an intermediate representation (IR) suitable for further optimization or deployment. Here's a summary of its main functionality:\n\n1. The file defines various utility functions and classes for handling different aspects of the export process, such as creating fake inputs, managing dynamic shapes, and handling module hierarchies.\n\n2. The core `_export` function takes a PyTorch module, input arguments, and optional parameters to trace the module's forward function and produce an `ExportedProgram`.\n\n3. It supports both strict and non-strict export modes, as well as pre-dispatch and AOT (Ahead-of-Time) dispatch options.\n\n4. The export process involves creating fake inputs, tracing the module's execution, and generating a graph representation of the computations.\n\n5. The resulting `ExportedProgram` includes the traced graph, graph signature, state dictionary, range constraints, and other metadata useful for further processing or deployment.",
            "uuid": "70779120-2b0f-4f4b-a730-846ea105f812"
        },
        "gpt-4.1": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 18579,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 3.452867,
            "file_summary": "This file implements the core logic for exporting PyTorch nn.Module computations into an intermediate representation (IR), specifically for the `torch.export` API. It provides functions and context managers to trace modules and functions, handle dynamic/static shapes, manage parameters, buffers, and constants, and process metadata to ensure fidelity of module state and input/output signatures. The file also includes mechanisms to verify the correctness of the exported graph, supports both strict and non-strict modes, and records detailed logging and error handling during the export process.",
            "uuid": "8d06d63f-ece6-4780-b2c6-aece40cab1af"
        },
        "Llama3.1": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 18543,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 49.329288,
            "file_summary": "The main functionality of the given Python file is to export PyTorch models to a format that can be used for inference. The file contains various functions and classes that work together to achieve this goal, including tracing the model's forward pass, converting the traced graph to a format suitable for export, and verifying the exported model. The file also includes various utility functions for tasks such as handling dynamic shapes, processing input and output specifications, and logging export-related events.",
            "uuid": "eadea701-c4e6-4f63-acac-ca4d4f209fdb"
        },
        "gpt-4o": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 18572,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 2.016711,
            "file_summary": "This Python file, `_trace.py`, primarily provides a suite of tools and functions for tracing and exporting PyTorch modules or functions into a serialized representation in Torch FX or a custom intermediate representation (IR). It includes extensive utilities for managing metadata, handling dynamic shapes, lifting constants, and verifying constraints during tracing operations. Additionally, it provides support for strict and non-strict modes of export, enabling both training and inference-specific IR generation while preserving module call signatures and handling dynamic tensor behaviors.",
            "uuid": "84fed21b-fc3d-43b2-b095-9fda5b9a393f"
        },
        "nova-lite-v1": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 22989,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 2.415586,
            "file_summary": "This Python file is part of a system for exporting PyTorch models to a format that can be used for high-performance inference. The main functionality includes tracing PyTorch models or callables, converting them into an intermediate representation (IR), and producing an exported program that can be optimized and verified. Key features involve handling dynamic shapes, managing tensor constants and buffers, and ensuring the exported model maintains the original model's behavior and structure. The file also includes utilities for logging, debugging, and verifying the exported models to ensure correctness and performance.",
            "uuid": "fd6b1131-c6c4-4ae2-b84a-d705268755ad"
        },
        "titan_premier": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 500,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 14.538379,
            "file_summary": "The given Python file, `_trace.py`, is a part of the PyTorch library and contains functions for exporting a PyTorch model to an Intermediate Representation (IR) format. This IR format is used for various purposes, such as optimizing the model for inference, generating code for different backends, and analyzing the model's structure and behavior.\n\nThe main function in this file is `_export`, which takes a PyTorch model, input arguments, and various options as input and returns an `ExportedProgram` object. This object contains the exported model in the IR format, along with metadata such as input and output specifications, constraints, and the original state dictionary of the model.\n\nThe `_export` function uses several helper functions and classes to perform the export process. These include:\n\n* `_export_to_torch_ir`: This function traces the input function or module and produces a `torch.fx.GraphModule` object in the Torch IR format.\n* `_export_to_aten_ir`: This function takes the `torch.fx.GraphModule` object produced by `_export_to_torch_ir` and converts it to the ATen IR format, which is a lower-level representation used by PyTorch's backend.\n* `_export_to_aten_ir_make_fx`: This function is an alternative to `_export_to_aten_ir` that uses the `torch.fx` framework to produce the ATen IR format.\n* `_non_strict_export`: This function is used when exporting a model for training purposes. It wraps the input model with a `torch.nn.Module` wrapper to handle non-strict mode, where the model may contain tensor attributes that are not parameters or buffers.\n* `_strict_export`: This function is used when exporting a model for inference purposes. It performs additional checks and transformations to ensure that the exported model is in a strict, well-defined format.\n* `_get_module_hierarchy`: This function generates a dictionary that maps module names to their fully qualified names, which is used for verifying the structure of the exported model.\n* `_get_original_state_dict`: This function retrieves the original state dictionary of the input model, which",
            "uuid": "62e236c6-05c0-4958-8424-6ce652a8814d"
        },
        "gpt-4": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 18600,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 6.538655,
            "file_summary": "The provided Python code appears to be a module that dynamically exports PyTorch models to a format suitable for various execution environments, possibly for inference or further processing. It consists of several important parts: enabling specific logging and tracing functionalities, faking and handling of module inputs for simulation, providing a structured framework to modify and reconfigure internal states during the tracing/exporting phase, and more. Furthermore, the module might be used to facilitate the transformation of PyTorch computational graphs into a format that can be analyzed or optimized outside of the standard PyTorch execution pipeline, possibly by replacing certain operations with equivalent but more efficient ones. Lastly, it uses decorators and context managers extensively to manage the complexity arising from these transformations.",
            "uuid": "7f8fc853-20ca-4512-9dbd-35233e2eebfb"
        },
        "Claude3.7": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 24878,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 6.851206,
            "file_summary": "The file \"_trace.py\" is a core component of PyTorch's export functionality, providing mechanisms to trace PyTorch modules and functions into exportable graph representations. It implements the infrastructure for converting PyTorch code into serializable computational graphs, handling dynamic shapes, preserving module call signatures, and managing fake tensor modes for symbolic tracing. The file contains utilities for processing inputs, extracting metadata, verifying graph properties, and producing ExportedProgram objects that can be used for deployment or further optimization.",
            "uuid": "741073dc-6f45-4a2e-b79b-5c01abf90991"
        },
        "mistral-small": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 25690,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 5.852063,
            "file_summary": " The provided code is a Python script that defines a function called `_export` for exporting a PyTorch model. The function takes several arguments, including the model to be exported, example inputs, keyword arguments, dynamic shapes, and various flags for controlling the export process.\n\nThe function first processes the input arguments and then calls either `_strict_export` or `_non_strict_export` based on the provided flags. These functions perform the actual export of the model using the PyTorch export library. The exported model is then returned as an `ExportedProgram` object.\n\nThe script also defines several helper functions and classes for handling various aspects of the export process, such as processing input arguments, handling dynamic shapes, and verifying the exported model.\n\nOverall, the script appears to be part of the PyTorch library and is used for exporting PyTorch models for deployment or further processing.",
            "uuid": "c47d063b-5969-44b0-a966-459fce195604"
        },
        "mistral-7b": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "total_tokens": 25800,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 8.020181,
            "file_summary": " The given Python file is a script for exporting PyTorch models to a format that can be used by other systems or tools. It defines several functions for handling different aspects of the export process, including:\n\n1. `_extract_fake_inputs`: This function extracts fake inputs from a given graph module and maps them to the original input arguments and keywords.\n2. `_produce_aten_artifact`: This function produces an ATen export artifact, which includes the graph module, its signature, and constants.\n3. `_export_to_torch_ir`: This function traces a PyTorch module or callable and converts it to TorchScript IR.\n4. `_export_to_aten_ir`: This function exports a PyTorch module or callable to the PyTorch Autograd IR format.\n5. `_strict_export` and `_non_strict_export`: These functions are used to export a PyTorch module with different levels of strictness. The strict export ensures that all inputs and outputs are explicitly defined, while the non-strict export allows for more flexibility in handling dynamic shapes and inputs.\n\nThe script also includes some helper functions for handling module hierarchies, placeholder names, and other export-related tasks. Additionally, it defines the `ExportedProgram` class, which is used to store and manage the exported model.",
            "uuid": "44d572e4-ed9e-4841-9582-169cf488dfe5"
        },
        "gpt-4.5": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "file_summary": "The provided Python file implements PyTorch Dynamo's tracing mechanisms for exporting PyTorch modules and functions into a symbolic Intermediate Representation (IR). It handles tracing of computational graphs, performing graph transformations, enforcing export constraints, managing fake tensor contexts, and converting traced modules into exportable formats. It supports various specialized modes (such as strict or non-strict, pre-dispatch or AOT dispatch) and generates metadata-rich, serialized export artifacts and signatures suitable for runtime execution or later compilation.",
            "total_tokens": 18571,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 28.320727,
            "uuid": "c2762fe9-d893-4387-aefe-7eed1b254ff8"
        },
        "cohere-v1": {
            "file_name": "/torch/export/_trace.py",
            "number_of_lines": 2209,
            "project_name": "pytorch",
            "file_summary": "The given Python file defines a function called _export which is used to trace either an nn.Module's forward function or a callable with PyTorch operations inside to produce a ExportedProgram object. The ExportedProgram contains a traced method along with some additional metadata used for verification and specialized execution. The main functionality of the file is to provide a way to perform AOT (ahead-of-time) compilation of PyTorch models.",
            "total_tokens": 21711,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 4.090422,
            "uuid": "9b791de5-e52b-446d-8be6-a691c2da06fb"
        }
    },
    "/torch/_inductor/virtualized.py": {
        "titan_premier": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 181,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 4.418756,
            "file_summary": "The file \"virtualized.py\" is a part of the Inductor library and provides a number of \"global\" variables/handlers that are actually thread local and dynamically scoped. These handlers are interacted with in a stylized way, typically by importing V from this module and accessing various handlers as attributes on this module. The file defines a class called Virtualized that implements a global variable that redirects via thread local variable. It also defines a class called OpsValue that allows for overloading magic methods for writing mathematical expressions fluently. The file also defines a number of virtualized global variables such as ops, graph, real_inputs, fake_mode, kernel, debug, interpreter, aot_compilation, current_node, and local_buffer_context. The file also defines a number of functions and classes related to these virtualized global variables.",
            "uuid": "e57d0c27-101d-4285-8881-a1d3c148d30f"
        },
        "gpt-4.1": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 3531,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 2.383706,
            "file_summary": "The main functionality of virtualized.py is to implement a framework for managing dynamic, thread-local global \"handlers\" or variables\u2014such as operator handlers, graph state, and compilation settings\u2014in a way that allows them to be safely and conveniently patched or accessed during code generation and compilation (primarily for TorchInductor). It achieves this by providing the Virtualized class, which wraps these handlers and dispatches their usage via thread-local storage, exposing them through an object V, and includes special support for mathematical operation overloads via the ops/global operator handler. This design enables flexible, context-specific behavior in a multi-threaded environment without explicitly passing state throughout the call stack.",
            "uuid": "d274e8c5-d060-42f9-88e2-3281b5441384"
        },
        "nova-lite-v1": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 4000,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 1.228121,
            "file_summary": "The main functionality of the \"virtualized.py\" file is to provide a set of thread-local, dynamically scoped global variables that can be patched to various implementations by Inductor depending on the situation. These variables are used for implicit argument passing, per-compilation global state, and alternate define-by-run interpretations. The file also includes a mechanism to overload arithmetic operations for a more fluent syntax when working with operations.",
            "uuid": "327ea39f-05b5-48ec-bbeb-bccfb5a268fa"
        },
        "Claude3.7": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 4236,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.913563,
            "file_summary": "The file \"virtualized.py\" implements a thread-local dynamic scoping system for PyTorch's Inductor compiler, providing global variables that can be patched with different implementations depending on the context. It defines a `Virtualized` class that manages thread-local variables with handlers for operations, graph lowering, and kernel generation, allowing code to access these context-dependent implementations through a central `V` object without explicitly passing them as arguments.",
            "uuid": "a909a12c-8603-42e3-8e59-b585f250a6ab"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 4127,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 1.726659,
            "file_summary": "This Python file defines a system for managing thread-local, dynamically scoped \"global\" variables or handlers, primarily used within PyTorch Inductor. It provides a `Virtualized` class to wrap thread-local storage, allowing different implementations or values to be swapped in based on the current execution context. This mechanism facilitates implicit argument passing, managing per-compilation state, and providing alternate interpretations for define-by-run operations via the `V` object and the top-level `ops` variable.",
            "uuid": "b065d949-6e89-4a8c-bc35-fc4eb3281792"
        },
        "nova-pro-v1": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 3980,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 1.242706,
            "file_summary": "The `virtualized.py` file provides a mechanism for managing thread-local, dynamically scoped global variables and handlers in a Python application, allowing different implementations to be patched in depending on the context. It facilitates implicit argument passing, maintains per-compilation global state, and supports alternate define-by-run interpretations for operations.",
            "uuid": "5f8cfaf3-7cd4-4f14-a00f-048d7b3bdbd5"
        },
        "gemini-2.5": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 4107,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 3.101324,
            "file_summary": "This Python file implements a system for \"virtualized\" global variables within the PyTorch Inductor. These variables are thread-local and dynamically scoped, allowing different handlers or implementations to be swapped in depending on the context, such as different compilation stages or define-by-run interpretations. This mechanism is used for implicit argument passing, managing per-compilation state, and providing alternate semantics for operations by patching handlers like `V.ops`.",
            "uuid": "b01c90fe-4587-4c03-8796-d12907d18633"
        },
        "gpt-4o": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 3486,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 1.128246,
            "file_summary": "The provided Python file primarily defines a framework for managing virtualized global variables that are thread-local and dynamically scoped. It facilitates the redirection of these variables to different handlers during operations such as code generation, implicit argument passing, and alternate define-by-run interpretations commonly used in PyTorch Inductor. This infrastructure enables flexible and dynamic execution by allowing global state and operator handling to be patched and queried seamlessly across diverse execution contexts, avoiding manual parameter threading.",
            "uuid": "4657bdc1-27fd-4e2b-a5e4-5b9260760d90"
        },
        "Llama3.1": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 3477,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 3.944485,
            "file_summary": "The main functionality of this file is to provide a set of virtualized global variables and handlers that can be dynamically scoped and patched to different implementations depending on the situation. These variables and handlers are used to manage various aspects of code generation, such as operator handling, graph generation, and debugging. The file also defines a set of classes and functions that wrap and unwrap IR values to enable fluent mathematical expressions.",
            "uuid": "9196df80-7b34-41ea-a497-6dc70be09d80"
        },
        "Llama3.3": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 3515,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 1.86642,
            "file_summary": "The main functionality of the given Python file is to provide a set of virtualized global variables and handlers that can be used to manage and switch between different implementations of operators and graphs in a thread-local and dynamic scoping context. These variables and handlers are designed to be used in a stylized way, allowing for implicit argument passing, per-compilation global state, and alternate define-by-run interpretations. The file defines a class `Virtualized` that implements this functionality, and creates several instances of it to manage various global variables, such as `ops`, `graph`, and `kernel`.",
            "uuid": "0f135566-3fc3-431d-8a4a-41a3d585ec36"
        },
        "gpt-4": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 3532,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 3.623527,
            "file_summary": "The Python file `virtualized.py` facilitates dynamic scoping and thread-local storage of various global variables (handlers) for thread-safe operations within code execution contexts, particularly when working with computational graphs and operations in a multi-threaded or asynchronous environment like in the case of PyTorch\u2019s Inductor system. It defines a `Virtualized` class to manage thread-local instances and provides multiple specific handler implementations for operations and graph handling, allowing for dynamic switching of computational behaviors, and implementation details based on the execution context. Different handlers can be installed and accessed through a module-level instance `V`, providing an easy way to change the behavior of code segments during runtime without passing numerous arguments or maintaining global state modifications.",
            "uuid": "773099ec-c625-4c3c-8f7b-9f54eecddb5c"
        },
        "Claude3.5": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 4245,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.471122,
            "file_summary": "The main functionality of the \"virtualized.py\" file is to provide a system for managing thread-local and dynamically scoped \"global\" variables and handlers in the Inductor framework. It defines a `Virtualized` class that implements these virtualized globals, which can be used for implicit argument passing, per-compilation global state, and alternate define-by-run interpretations. The file also includes an `OpsWrapper` class to enable more fluent mathematical expressions in the code.",
            "uuid": "f8c87ecd-1d07-4e84-8602-3ea32ca66a4c"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 3469,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 1.186609,
            "file_summary": "The `virtualized.py` file contains definitions for virtualized global variables/handlers that are dynamically scoped using thread-local storage. These virtualized variables allow for different implementations to be patched depending on the situation and are accessed in a stylized way using the `V` module. The file also includes specific usage patterns for these virtualized global variables and provides a mechanism for overriding default implementations.",
            "uuid": "fe8e06ea-84c5-4680-bb4c-e27127b155f6"
        },
        "mistral-small": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 4428,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 3.856203,
            "file_summary": " The main functionality of the provided Python file, named \"virtualized.py\", is to provide a number of \"global\" variables/handlers that are actually thread local and dynamically scoped. These handlers are interacted with in a stylized way and are used for various purposes such as implicit argument passing, per-compilation global state, and alternate define-by-run interpretations. The file defines a class named `Virtualized` which is used to create these global variables, and several instances of this class are created for different purposes like `_ops`, `_graph`, `_real_inputs`, `_fake_mode`, `_kernel`, `_debug`, `_interpreter`, `_aot_compilation`, `_current_node`, `_local_buffer_context`, and `_choices`. These instances are then accessed via the `V` object, which is an instance of the `_V` class. The file also defines some helper classes and functions related to these global variables.",
            "uuid": "1b8a4368-2b07-443b-ab4f-252dec69cdb2"
        },
        "mistral-7b": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "total_tokens": 4364,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.306543,
            "file_summary": " The given Python file, named \"virtualized.py\", provides a dynamic scoping mechanism for global variables using thread-local storage. It allows swapping out different implementations of these global variables during code generation. The file defines several virtualized global variables, such as 'ops', 'graph', 'real\\_inputs', 'fake\\_mode', 'kernel', 'debug', 'interpreter', 'aot\\_compilation', 'current\\_node', and 'local\\_buffer\\_context'. These variables can be accessed using the 'V' object, and their behavior can be modified by setting new handlers for them. The file also includes a NullHandler sentinel to indicate when a global variable is unset.",
            "uuid": "d9b08508-032b-417e-b203-c9aed08e210f"
        },
        "gpt-4.5": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "file_summary": "This file provides a mechanism for creating \"virtualized globals\"\u2014variables or handlers whose values are dynamically scoped and thread-local\u2014to allow flexible patching of implementations at runtime within PyTorch's Inductor compiler backend. It defines the `Virtualized` class to manage global variables through thread-local storage and supplies various contexts (`ops`, `graph`, `kernel`, etc.) accessible via the `V` singleton to switch between different runtime implementations. Additionally, it includes helpers like `OpsValue` and `OpsWrapper` to simplify operator handling and support intuitive syntax for mathematical expressions.",
            "total_tokens": 3512,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 13.25273,
            "uuid": "09e2b262-0532-4dfa-add8-bf570b57d436"
        },
        "cohere-v1": {
            "file_name": "/torch/_inductor/virtualized.py",
            "number_of_lines": 412,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, named virtualized.py, creates a mechanism for managing global variables across threads. It uses thread-local storage to implement dynamic scoping, allowing these variables to be accessed and modified within specific contexts. The global variables serve different purposes, such as passing implicit arguments, maintaining compilation-specific state, and supporting alternate interpretations of define-by-run IR.",
            "total_tokens": 3725,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.676194,
            "uuid": "ea2123bf-a9b8-46ec-8f8f-8380124b986a"
        }
    },
    "/torch/_prims/rng_prims.py": {
        "Llama3.3": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 3708,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 2.512669,
            "file_summary": "The main functionality of this Python file is to register and implement various random number generator (RNG) primitives and higher-order operators for PyTorch, including `run_and_save_rng_state`, `run_with_rng_state`, and `graphsafe_run_with_rng_state`, which enable functional random number generation and state management across different devices. These primitives and operators aim to provide a way to capture and restore the state of the random number generator, allowing for reproducibility and deterministic behavior in PyTorch computations. The file also includes implementations for different backends, such as CUDA, CPU, HPU, and XPU, and supports fake tensor modes and proxy tensor dispatching.",
            "uuid": "ae549b7b-5f7d-42ea-bfbe-26a44e9b54b1"
        },
        "Llama3.1": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 3655,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 4.546837,
            "file_summary": "The main functionality of this file is to register and implement various random number generator (RNG) primitives for PyTorch, including Philox-based stateless rand operators and higher-order operators for running and saving RNG states. The file also defines functions for registering these primitives and implementing their behavior for different devices (CPU, CUDA, HPU, XPU) and modes (autograd, fake tensor, proxy tensor).",
            "uuid": "f691fd69-1054-41f5-8c35-c2f560cb2935"
        },
        "gpt-4": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 3732,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 4.48982,
            "file_summary": "The Python file `rng_prims.py` primarily defines custom PyTorch primitives for random number generation using the Philox algorithm on CUDA devices, and provides functions to modify, save, and utilize random number generator (RNG) state across different devices and operation executions. It includes definitions and registrations of primitives like `philox_rand`, which stands out as a stateless random number generator based on the Philox algorithm, and higher-order operations such as `run_and_save_rng_state_op` and `run_with_rng_state_op` that ensure the correct RNG state is used and preserved during function calls. Additionally, the file sets up utilities to manage and trace RNG states within PyTorch's advanced tensor operation contexts, employing proxy and dispatch mechanisms to handle backend-specific executions and support graph-execution compatibilities, particularly within CUDA environments.",
            "uuid": "c6b2fba3-d720-43d9-82d1-4590753ae847"
        },
        "titan_premier": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 244,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 6.582741,
            "file_summary": "The file `rng_prims.py` is a Python file that contains functions for registering RNG (Random Number Generator) prims. The main functionalities of the file include:\n\n1. Registering the Philox-based stateless rand operator using the `register_philox_rand` function.\n2. Registering the `run_and_save_rng_state` and `run_with_rng_state` operators using the `register_run_and_save_rng_state_op` and `register_run_with_rng_state_op` functions, respectively.\n3. Registering the `graphsafe_run_with_rng_state` operator using the `register_graphsafe_run_with_rng_state_op` function.\n\nThe file also contains helper functions such as `philox_rand_offset_meta`, `philox_rand_offset`, and `get_device` to support the main functionalities.\n\nOverall, the file is responsible for registering various RNG operators and providing helper functions for working with RNG states.",
            "uuid": "e5ac2f5c-c3a0-4532-a5a0-1219f794318d"
        },
        "nova-lite-v1": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 4563,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 2.982121,
            "file_summary": "The main functionality of the \"rng_prims.py\" file is to define and register custom random number generator (RNG) operations for PyTorch, specifically focusing on Philox-based stateless random operations. The file includes functions to register RNG primitives, handle different backend devices (CUDA, CPU, XPU, HPU), and manage RNG states for various operations, ensuring compatibility and functionality across different hardware accelerators.",
            "uuid": "14441ee3-85c7-4249-b2d4-92e20264e9ea"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 3657,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 2.747162,
            "file_summary": "The main functionality of the \"rng_prims.py\" file is to define and register Philox-based random number generation (RNG) operators using custom operators in PyTorch. The file contains functions for generating random values based on the Philox algorithm, handling RNG states for different devices, and registering higher-order RNG operators for running and saving RNG states. Additionally, it includes functionality for graph-safe RNG operations on CUDA devices and registration of the RNG primitives.",
            "uuid": "a502465c-bf41-44cc-bd58-f17bde1b2957"
        },
        "nova-pro-v1": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 4531,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 1.335866,
            "file_summary": "The main functionality of the `rng_prims.py` file is to register custom primitives for random number generation (RNG) operations in PyTorch, specifically focusing on Philox-based stateless RNG and operations to run and save RNG states across different devices.",
            "uuid": "d258c814-a6ad-4b8b-81b6-b7642f356a56"
        },
        "gpt-4.1": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 3699,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 2.461093,
            "file_summary": "The file defines and registers a set of PyTorch \"random number generator primitives\" (RNG prims) focused on stateless and functionalized random number generation for various backends (especially CUDA). It introduces higher-order operators for running functions with controlled RNG states (saving, restoring, and graph-safe execution), supporting use cases like functional programming, distributed computation, and deterministic behavior. The main functionality is to create abstractions and operators for reproducible and backend-agnostic RNG handling in PyTorch, particularly for advanced features like graph tracing and functionalization.",
            "uuid": "733280ff-6a9e-4505-9c73-1779a2ab6ac3"
        },
        "Claude3.5": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 5087,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 5.010065,
            "file_summary": "The main functionality of the \"rng_prims.py\" file is to register and implement random number generation (RNG) primitives and operations for PyTorch, particularly focusing on Philox-based random number generation for CUDA devices. It defines several higher-order operators and their implementations for different dispatch keys, including functions to run operations with specific RNG states and to save RNG states during execution.",
            "uuid": "225ee491-4634-4781-8dbf-5d3303411f45"
        },
        "gemini-2.5": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 5025,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.946648,
            "file_summary": "This Python file defines and registers custom PyTorch operations for random number generation (RNG), including a Philox-based stateless random number generator (`philox_rand`). It also introduces higher-order operators like `run_and_save_rng_state` and `run_with_rng_state` to manage RNG states across different devices (CPU, CUDA, HPU, XPU) during operations. Furthermore, a graph-safe version of the RNG state management operator, `graphsafe_run_with_rng_state`, is provided specifically for CUDA to ensure compatibility with graph-based execution.",
            "uuid": "ad1f598b-9a83-40e4-8795-3bb8f25a9daf"
        },
        "Claude3.7": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 5108,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.92188,
            "file_summary": "This file implements random number generation (RNG) primitives for PyTorch, focusing on stateless and device-specific RNG operations. It registers several higher-order operators for managing RNG state, including functions to run operations while saving or restoring RNG state, with special handling for different device backends (CUDA, CPU, HPU, XPU). The file also implements Philox-based random number generation specifically for CUDA devices, providing functionality for deterministic random number generation in PyTorch.",
            "uuid": "e27deea5-7c53-46e8-a3ac-fee2735c5b48"
        },
        "gpt-4o": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 3691,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 2.509754,
            "file_summary": "The file `rng_prims.py` implements randomized operations designed to work with the PyTorch framework, focusing on the registration and functionalization of Philox-based random number generation (RNG) primitives across different hardware backends. It provides utilities for saving, restoring, and managing RNG states in CUDA, CPU, XPU, and HPU platforms, enabling deterministic and stateless randomization compatible with distributed environments and higher-order operator frameworks. Additionally, the file includes support for graph-safe RNG state manipulation specific to CUDA.",
            "uuid": "379fe920-9087-475b-8ff0-9f3df7926628"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 5299,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 4.667284,
            "file_summary": "This Python file defines and registers custom PyTorch operators related to random number generation (RNG). It includes a stateless Philox-based random number primitive and higher-order operators to execute code blocks while saving or temporarily setting the RNG state. These operators support various devices and provide graph-safe state management for CUDA.",
            "uuid": "ffff18fa-7326-462f-b5c1-35de77b80a94"
        },
        "mistral-small": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 5425,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 7.506074,
            "file_summary": " The main functionality of the provided Python file, named \"rng_prims.py\", is to register custom random number generator (RNG) operations for PyTorch. These operations are based on the Philox algorithm and are designed to work with different devices such as CPU and CUDA. The file defines several functions and classes to achieve this:\n\n1. `register_rng_prim(name, schema, impl_aten, impl_meta, doc, tags=None)`: This function is used to register a new RNG operation with PyTorch. It takes the name, schema, implementation, metadata, documentation, and tags as arguments.\n\n2. `register_philox_rand()`: This function registers the Philox-based stateless random number generator operation. It defines the schema, implementation, and metadata for the operation.\n\n3. `run_and_save_rng_state` and `run_with_rng_state`: These are higher-order operators that allow running an operation with a specific RNG state and saving/restoring the RNG state, respectively.\n\n4. `graphsafe_run_with_rng_state`: This is a higher-order operator that allows running an operation with a specific RNG state in a graph-safe manner, which means it can be used in PyTorch's graph mode.\n\n5. `register_graphsafe_run_with_rng_state_op()`: This function registers the `graphsafe_run_with_rng_state` operator.\n\n6. `register_rng_prims()`: This function is called at the end of the file to register all the defined RNG operations.\n\nIn summary, the main functionality of this file is to provide custom RNG operations for PyTorch that are based on the Philox algorithm and can work with different devices and in graph mode.",
            "uuid": "31057fca-1da8-4bed-a927-e70d1360dd5d"
        },
        "mistral-7b": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "total_tokens": 5123,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 5.032265,
            "file_summary": " This Python file defines functions for registering custom random number generator (RNG) primitives in PyTorch. The main functionality of the file is to register a Philox-based RNG operator named \"philox\\_rand\" with the given schema, implementation, and metadata. The registration process involves defining meta functions, creating TensorMeta and Tensor objects, and registering the operator using the torch.library.custom\\_op function. The file also includes helper functions for handling device types and registering other related operators.",
            "uuid": "90fb332c-7c1d-45c1-958a-35016ce4c1c9"
        },
        "gpt-4.5": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "file_summary": "The provided Python file primarily defines and registers PyTorch primitives for handling random number generation (RNG) on various backends (CUDA, CPU, HPU, XPU), specifically utilizing the Philox-based stateless RNG algorithm. It offers mechanisms to execute functions with controlled RNG states, retrieve and restore RNG states, as well as a \"graph-safe\" variant specialized for CUDA backend. Overall, its role is enabling deterministic and stateless handling of RNG operations within PyTorch, supporting traceability and reproducibility.",
            "total_tokens": 3690,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 12.612486,
            "uuid": "95c239ac-8066-42ef-aaf4-32d29fb66dc2"
        },
        "cohere-v1": {
            "file_name": "/torch/_prims/rng_prims.py",
            "number_of_lines": 390,
            "project_name": "pytorch",
            "file_summary": "The code defines functions to register custom random number generation (RNG) primitives in PyTorch. These functions allow for the registration of RNG operators with different device types, including CPU, CUDA, and custom backends like HPU and XPU. The RNG operators are used for generating random values in a stateless manner, and the code also provides higher-order operators to manage and restore RNG states for deterministic execution.",
            "total_tokens": 4445,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 7.558784,
            "uuid": "646f1e73-1b5e-4428-b0f3-c3acf673b5e3"
        }
    },
    "/torchgen/gen_vmap_plumbing.py": {
        "mistral-small": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 3383,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 3.983035,
            "file_summary": " The main functionality of the Python file \"gen_vmap_plumbing.py\" is to generate C++ code for PyTorch's Functorch library. This code is responsible for handling batch rule plumbing, which is a part of PyTorch's vectorized map (vmap) functionality. The generated code includes functions for unwrapping tensors, handling cases where all batch dimensions are none, and generating vmap plumbing for both functional and in-place operations. The file uses PyTorch's type system and code generation tools to automatically generate the necessary C++ code based on the input function's schema.",
            "uuid": "10791606-96b3-4eff-8c3d-9a08b3dc69a5"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 2509,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 2.582625,
            "file_summary": "The main functionality of the `gen_vmap_plumbing.py` file is to generate plumbing code for handling batch processing of native functions in PyTorch using the `torchgen` library. The file contains functions for generating code that handles unwrapping tensor inputs, checking conditions, and returning processed results in batched form. Additionally, it includes functions for generating plumbing code specific to in-place operations and cases where all batch dimensions are none.",
            "uuid": "6bb3afd0-1922-4925-adb7-923c19ab00a4"
        },
        "nova-pro-v1": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 3030,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 1.191676,
            "file_summary": "The main functionality of the `gen_vmap_plumbing.py` file is to generate C++ code for handling vectorized map (vmap) operations on PyTorch native functions, specifically for in-place and functional operations involving tensors, by unwrapping tensor arguments, applying batch rules, and handling returns appropriately.",
            "uuid": "c16b9801-b1c1-4898-82a4-1ba092bc7c5d"
        },
        "Claude3.5": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 3413,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.579376,
            "file_summary": "The main functionality of the \"gen_vmap_plumbing.py\" file is to generate plumbing code for vmap (vectorized map) operations in PyTorch. It analyzes PyTorch function schemas and generates C++ code templates for handling batched tensor operations, including unwrapping tensors, handling different input types, and wrapping return values. The file provides functions to generate plumbing code for various scenarios, such as in-place operations, functions with no returns, and general vmap plumbing for PyTorch native functions.",
            "uuid": "88fa7106-561f-47ea-85eb-01c8d251018f"
        },
        "mistral-7b": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 3375,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 3.269238,
            "file_summary": " The given Python file, 'gen\\_vmap\\_plumbing.py', is a script used to generate C++ code for PyTorch's FunctorCh library. The main functionality of the file is to define and implement a `ComputeBatchRulePlumbing` dataclass with a method `__call__` that generates C++ code for specific PyTorch functions (`NativeFunction` objects) using the `gen_vmap_plumbing` function. This generated code is used to implement the batching rules for PyTorch functions during the forward and backward passes.",
            "uuid": "9982e134-060d-441d-8b96-d48386cc7c05"
        },
        "nova-lite-v1": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 3052,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 0.888966,
            "file_summary": "The main functionality of the `gen_vmap_plumbing.py` file is to generate C++ code for vectorized map (vmap) plumbing in PyTorch, specifically for handling tensor inputs and outputs in functional and in-place operations. It includes generating code for unwrapping tensors, handling cases where batch dimensions are None, and creating the necessary plumbing for vmap operations based on the function schema and its return types.",
            "uuid": "3ec8a246-ea59-42bf-9930-49414141444a"
        },
        "gpt-4.1": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 2541,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 3.851466,
            "file_summary": "The file **gen_vmap_plumbing.py** generates C++ code (\"plumbing\") for handling batched operations (vmap) in PyTorch's dispatch system by producing wrapper functions for operators with batched tensor inputs and outputs. It analyzes the operator schemas, determines how to unwrap and rewrap batched arguments/returns, and generates different wrapper code depending on operator mutability, in-place status, and types. The resulting code supports efficient, correct propagation of batch dimensions through operator calls under the functorch system.",
            "uuid": "075be21c-10af-4627-9c2f-264cedf4b32e"
        },
        "gpt-4o": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 2535,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 3.064446,
            "file_summary": "The `gen_vmap_plumbing.py` file provides functionality to generate C++ template code for \"vmap plumbing\" in PyTorch's Function Transformations library, focusing on dynamic layer batching. It includes methods for unwrapping tensor arguments, handling cases where batch dimensions are absent, and generating specialized functions for functional operations, in-place operations, and operations with no return values. Its main role is to create consistent, automated plumbing code for managing batched tensor operations efficiently in functorch.",
            "uuid": "92077065-2854-40b5-ab20-11ae168ac014"
        },
        "gemini-2.5": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 3585,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 8.451898,
            "file_summary": "This Python script automatically generates C++ code that provides the \"plumbing\" for PyTorch's `vmap` (vectorized map) functionality, specifically for batched tensor operations. It defines helper functions to unwrap batched tensor inputs, handle optional tensors, and then re-wrap the results after applying a core batching rule. The script produces C++ template functions tailored to different native PyTorch function signatures, including in-place operations and functions with varying return types, to integrate them into the `vmap` system.",
            "uuid": "ba0c0867-5f0b-4a4e-a2ad-1caa0880eb53"
        },
        "Claude3.7": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 3391,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 7.31179,
            "file_summary": "This Python file generates C++ plumbing code for vectorized map (vmap) operations in PyTorch's functorch module. It analyzes PyTorch function schemas and creates template code that handles tensor batching, unwrapping tensors at different batch dimensions, and properly wrapping return values. The file contains specialized handling for different function types (functional, in-place, etc.) and tensor argument patterns to enable automatic vectorization of PyTorch operations.",
            "uuid": "5b5e45d8-35e5-43f0-99ca-efc1aaa4c408"
        },
        "Llama3.1": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 2511,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 6.434709,
            "file_summary": "The main functionality of this Python file is to generate C++ code for vectorized map (vmap) plumbing in PyTorch, specifically for functions that operate on tensors. The file defines several functions that generate C++ code for different types of vmap plumbing, including in-place plumbing, plumbing with no returns, and plumbing with returns. The generated code is used to implement batched tensor operations in PyTorch.",
            "uuid": "95cf87b3-c720-4d4b-9251-63516fec12a3"
        },
        "titan_premier": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 110,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 3.052432,
            "file_summary": "The main functionality of the file gen_vmap_plumbing.py is to generate C++ code for vmap plumbing. It checks if the function schema has at least one tensor input and if all returns are either Tensor or TensorList. If these conditions are met, it generates the C++ code for the function. The code also handles cases where all batch dimensions are None and generates the necessary code to unwrap tensors and optional tensors. The generated code is then wrapped in a namespace and included in a header file.",
            "uuid": "f1fa59a8-988c-41b7-b133-ed476ab6206d"
        },
        "Llama3.3": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 2526,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 1.992493,
            "file_summary": "The main functionality of the provided Python file, `gen_vmap_plumbing.py`, is to generate C++ code for plumbing related to vectorized mapping (vmap) in PyTorch, specifically for functions that operate on tensors. The file uses various utility functions to handle different scenarios, such as unwrapping tensor arguments, handling in-place operations, and generating return statements. The code ultimately generates C++ code that can be used to create vmap plumbing for PyTorch functions.",
            "uuid": "13b63476-e6ec-4751-99f6-ecbdf584a711"
        },
        "gemini-2.5-flash": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 3635,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 3.175821,
            "file_summary": "This Python script is part of the PyTorch code generation system (`torchgen`). It analyzes PyTorch native functions to generate C++ \"plumbing\" code that enables `vmap` (vectorized map) support for these operations. The generated code handles unwrapping batched tensors, calling the core function, and re-wrapping the results with batch dimensions.",
            "uuid": "cf2866a6-639d-447f-abe9-52b640a49cd4"
        },
        "gpt-4": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "total_tokens": 2511,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 6.194867,
            "file_summary": "The provided Python script is designed for generating vectorization mappings (vmap) plumbing code for PyTorch functions. It includes definitions of utilities to handle both tensor and non-tensor types during the vectorization of PyTorch operations. The core functionality includes generating special plumbing code for either batched operations or for operations requiring specific conditions on their arguments during execution, useful when using batch dimension vectorization techniques in the context of dynamic layers and dispatch keys.",
            "uuid": "9459224d-f955-4be6-9b3f-d374f45062ec"
        },
        "gpt-4.5": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "file_summary": "The provided Python file defines code generation utilities for creating vmap plumbing C++ templates for PyTorch's dispatcher system, specifically tailored for handling batched tensor operations. It inspects function schemas, unpacks tensor and optional tensor arguments, generates conditional logic for unbatched scenarios, wraps/unwraps batched tensors, and handles different schema types such as in-place function calls. Ultimately, it generates C++ functions to support vectorized batch processing within the PyTorch dispatcher system.",
            "total_tokens": 2531,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 13.231002,
            "uuid": "9f3b21f9-ac7d-4c6a-a3f7-26bd6513f0b1"
        },
        "cohere-v1": {
            "file_name": "/torchgen/gen_vmap_plumbing.py",
            "number_of_lines": 276,
            "project_name": "pytorch",
            "file_summary": "The provided Python script, gen_vmap_plumbing.py, is responsible for generating plumbing code for function dispatch in the context of vmap batching in PyTorch. It defines functions to unwrap tensor inputs, generate case statements for handling different batching scenarios, and ultimately produces template function definitions that handle the batching logic for various native functions. The generated code facilitates the efficient application of functions to batched inputs.",
            "total_tokens": 3015,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 4.296372,
            "uuid": "39c9010e-863e-4b10-86fd-ddceeed369b8"
        }
    },
    "/torch/distributed/_tools/sac_estimator.py": {
        "gpt-4o": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 9883,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 7.192063,
            "file_summary": "The `sac_estimator.py` file provides a Python implementation focused on estimating memory and runtime trade-offs for Selective Activation Checkpointing (SAC). It includes the `SACEstimator` class, which leverages PyTorch's `TorchDispatchMode` to gather statistics, optimize checkpointing strategies, and compute memory-recomputation trade-off curves using a piecewise linear fitting model. It supports detailed analysis and modulewise activation checkpointing metrics, facilitating more efficient handling of computational resources during model execution.",
            "uuid": "5f01f857-7b27-40e4-9745-00bb8d598c92"
        },
        "Llama3.3": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 9855,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 10,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 2.734751,
            "file_summary": "The main functionality of the given Python file is to estimate the memory and recomputation time trade-offs for applying Selective Activation Checkpointing (SAC) in PyTorch models. The `SACEstimator` class provides a context manager to estimate these trade-offs and displays detailed statistics and metadata information for operators of each module. It also constructs a per-module trade-off graph of discarded memory vs recomputation time for the obtained greedy order.",
            "uuid": "e2115052-e51c-4630-a2e2-2427de8d9e77"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 12531,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 4.060165,
            "file_summary": "This Python file defines the `SACEstimator` class, a `TorchDispatchMode` designed to estimate memory and recomputation time trade-offs for Selective Activation Checkpointing (SAC) within PyTorch modules. It operates under `FakeTensorMode` to intercept operations and collect per-operator memory usage and runtime estimates using a `RuntimeEstimator`. The estimator analyzes these statistics per module to determine a greedy order for checkpointing operations and provides detailed trade-off statistics.",
            "uuid": "5400fd10-971b-4586-877f-cfee944f8e42"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 9880,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 5.800944,
            "file_summary": "The provided Python file `sac_estimator.py` defines classes and functions for estimating memory and recomputation time trade-offs for Selective Activation Checkpointing (SAC) in PyTorch. The file contains data classes for storing SAC statistics, metadata information, trade-off statistics, and greedy order metadata. It provides functionality to estimate SAC trade-offs, display SAC statistics for modules, and fit a piecewise linear function to the trade-off curve of discarded memory vs. recomputation time. The code also includes context managers for setting the estimation mode type and handling hooks for module metadata preparation and SAC statistics calculation.",
            "uuid": "00ad3810-f193-4825-a5fc-dec03f6c49b3"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 13703,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 13.080517,
            "file_summary": " The main functionality of the file is to analyze a given Python file and summarize its main functionality in up to three sentences. The user input is given in a form of a dictionary with two fields: 'file_name' and 'file_content'. The file_name field contains the name of the Python file, and the file_content field contains the content of the file as a string.\n\nThe file uses the Python dataclasses and typing modules to define several classes and functions that are used to analyze the given Python file. The main class is SACEstimator, which is a senior developer and an expert in Python. This class is used to analyze the given Python file and summarize its main functionality.\n\nThe SACEstimator class has several methods, including __init__, __torch_dispatch__, __getstate__, __setstate__, __call__, __enter__, and __exit__. The __init__ method initializes several attributes of the class, including self.sac\\_mod\\_stats, self.sac\\_mod\\_tradeoff\\_stats, self.sac\\_mod\\_greedy\\_order\\_meta, self.\\_mod\\_tracker, self.\\_sac\\_metadata, self.\\_sac\\_mod\\_metadata, self.\\_leaf\\_modules, self.\\_saved\\_tensor\\_hook\\_ctx, and self.\\_estimate\\_runtime.\n\nThe __torch_dispatch__ method is used to estimate the memory and recomputation time trade-offs for applying Selective Activation Checkpointing (SAC) to the given Python file. This method uses the TorchDispatchMode context manager to estimate the memory and runtime trade-offs of functions or nn.Module objects for SAC. It provides detailed statistics and metadata information for operators of each module and provides a greedy order for selecting the operators to be recomputed/checkpointed.\n\nThe __call__ method sets the estimate mode type for the SAC estimator. Currently supported modes are \"operator-level-benchmark\" and \"operator-level-cost-model\". The __enter__ and __exit__ methods are used to register user hooks for the mod\\_tracker and saved\\_tensor\\_hook\\_ctx objects.\n\nThe main functionality of the file is to analyze the given Python file and summarize its main functionality in up to three sentences. This is done by using the SACEstimator class to estimate the memory and recomputation time trade-offs for applying SAC to the given Python file. The results of this analysis are then used to summarize the main functionality of the file in up to three sentences.",
            "uuid": "55e33184-2fc2-468c-9f92-7babbeedc7cc"
        },
        "gpt-4": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 9881,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 7.977128,
            "file_summary": "The Python file defines a module for estimating the memory and runtime trade-offs involved in applying Selective Activation Checkpointing (SAC) in PyTorch neural networks. It includes dataclasses and functions to store and manage metadata about operations, modules, and their SAC statistics, providing a framework to compute optimal recompute/store strategies for operations to optimize memory use versus recomputation time. It uses hooks to gather information during forward passes of modules, calculates memory savings per second for operations, and fits piecewise linear functions to model trade-offs, facilitating detailed analysis and insights on memory-runtime trade-offs for different module configurations.",
            "uuid": "722018f1-fe78-4d4d-8f86-53a2d40d455e"
        },
        "Llama3.1": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 9866,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 49.913042,
            "file_summary": "The main functionality of the file is to estimate the memory and recomputation time trade-offs for applying Selective Activation Checkpointing (SAC) to a PyTorch model. The file defines a class `SACEstimator` that provides a context manager to estimate the memory and runtime trade-offs of functions or `torch.nn.Module`s for SAC. It also provides detailed statistics and metadata information for operators of each module and provides a greedy order for selecting the operators to be recomputed/checkpointed.",
            "uuid": "48b819d1-605b-4e2e-9550-24c3a6748272"
        },
        "Claude3.5": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 13048,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 6.051612,
            "file_summary": "The main functionality of the \"sac_estimator.py\" file is to implement a Selective Activation Checkpointing (SAC) Estimator. This class estimates memory and recomputation time trade-offs for applying SAC to PyTorch modules. It provides detailed statistics and metadata for operators within each module and generates a greedy order for selecting operators to be recomputed or checkpointed. The estimator also constructs per-module trade-off graphs of discarded memory versus recomputation time for the obtained greedy order.",
            "uuid": "31d729f3-f3a2-48f2-a080-9042928a1751"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 13028,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 106.698486,
            "file_summary": "The `sac_estimator.py` file implements a Selective Activation Checkpointing (SAC) estimator that analyzes PyTorch modules to determine optimal memory-runtime trade-offs for activation checkpointing. It tracks operator execution, memory usage, and runtime to build a detailed model of which operations should be recomputed versus stored during backpropagation, helping to optimize memory usage while minimizing computational overhead in deep learning models.",
            "uuid": "5a1ad46d-1dcd-48b5-918a-e6da547ee9b8"
        },
        "nova-pro-v1": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 11793,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 3.602592,
            "file_summary": "The main functionality of the `sac_estimator.py` file is to estimate the memory and recomputation time trade-offs for applying Selective Activation Checkpointing (SAC) in PyTorch models, providing detailed statistics and metadata for operators within each module, and constructing per-module trade-off graphs. It supports two estimation modes: \"operator-level-benchmark\" and \"operator-level-cost-model\".",
            "uuid": "12b4da9c-f3f8-49f3-8ef6-14dedb99a2ea"
        },
        "nova-lite-v1": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 11830,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 2.203567,
            "file_summary": "The file \"sac_estimator.py\" defines a class `SACEstimator` that estimates the memory and recomputation time trade-offs for applying Selective Activation Checkpointing (SAC) in PyTorch models. It provides detailed statistics and metadata for operators within each module, and constructs a trade-off graph of discarded memory versus recomputation time. The class supports two estimation modes: \"operator-level-benchmark\" and \"operator-level-cost-model\" (roofline model). It also includes utility functions to display the SAC statistics and trade-off statistics in tabular or plain text formats.",
            "uuid": "66aa8ff8-5619-47b4-a8e6-c8ea6a89b67f"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 13414,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 10.047209,
            "file_summary": " The given Python file, `sac_estimator.py`, is a script implementing Selective Activation Checkpointing (SAC) estimation for PyTorch models. The main functionality of the file can be summarized as follows:\n\n1. The script defines classes and functions to estimate the memory and runtime trade-offs for applying SAC to PyTorch models. It uses the `TorchDispatchMode` context manager to intercept forward passes through PyTorch modules and collect statistics.\n2. It provides methods to display the collected statistics for each module, as well as the trade-off statistics between discarded memory and recomputation time.\n3. The script supports two estimation modes: \"operator-level-benchmark\" and \"operator-level-cost-model\". The former uses operator benchmarking to estimate runtime, while the latter uses a roofline cost model.\n4. The script also includes utility functions and classes for handling metadata, statistics, and trade-off statistics.\n\nIn summary, the main functionality of the file is to provide a Python script for estimating the memory and runtime trade-offs for applying SAC to PyTorch models, using either operator benchmarking or a roofline cost model.",
            "uuid": "96d7b1b3-3c4d-4248-a983-eed02c8d0ae3"
        },
        "gpt-4.1": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 9911,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 6.024272,
            "file_summary": "The file defines a SACEstimator class which estimates the memory and recomputation time trade-offs when applying Selective Activation Checkpointing (SAC) in PyTorch models, specifically using a TorchDispatchMode context and operating under FakeTensorMode. It tracks module and operator-level statistics such as runtime, memory, view-like, random, and in-place properties, constructs a greedy schedule for storing/recomputing activations, and fits piecewise linear trade-off curves (memory saved vs compute time incurred) for checkpointing decisions. The file also provides display utilities for summarizing module-wise SAC statistics and trade-offs, supporting both tabular and plain text outputs.",
            "uuid": "c951ea80-720b-464e-91e1-6f4c36dbd8c9"
        },
        "gemini-2.5": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 12363,
            "model": {
                "known_name": "gemini-2.5",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00125,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 5.751111,
            "file_summary": "This Python file defines the `SACEstimator` class, a tool designed to analyze PyTorch models and estimate the trade-offs between memory savings and recomputation time when applying Selective Activation Checkpointing (SAC). It functions as a `TorchDispatchMode` context manager, intercepting PyTorch operations to gather detailed statistics like runtime, memory usage, and operator properties for each module. Ultimately, the `SACEstimator` helps determine an optimal strategy for checkpointing by providing per-module statistics, a greedy ordering of operations for recomputation, and trade-off curves.",
            "uuid": "f96b020b-468c-4efc-b0d7-cc57431dfa5b"
        },
        "titan_premier": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "total_tokens": 269,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 8.011576,
            "file_summary": "The file \"sac_estimator.py\" is a Python module that provides a class called \"SACEstimator\". This class is designed to estimate the memory and recomputation time trade-offs for applying Selective Activation Checkpointing (SAC) to a given function or module.\n\nThe \"SACEstimator\" class uses a context manager to track the metadata of each operator in the module during the forward pass. It then computes the SAC statistics and trade-off statistics for each module. The SAC statistics include the operator names, runtimes, memory usage, view-like operators, random operators, and saved autograd operators. The trade-off statistics include the number of segments, slopes, intercepts, fit breaks, and a trade-off curve.\n\nThe \"SACEstimator\" class also provides several methods to display the SAC and trade-off statistics in a tabular or plain text format. It also provides a method to fit a piecewise linear function to the trade-off curve and save the trade-off graphs to file.\n\nOverall, the \"sac_estimator.py\" file provides a tool for estimating the memory and recomputation time trade-offs of applying SAC to a given function or module.",
            "uuid": "8ce6e402-c5c2-4a5f-82cb-57b5bc7ee483"
        },
        "gpt-4.5": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "file_summary": "The provided Python file implements a Selective Activation Checkpointing (SAC) estimation tool for PyTorch modules, designed to optimize the trade-off between computational memory usage and recomputation runtime. It performs detailed operation-level analysis and provides a greedy estimation approach to decide which activations to store and which to recompute during neural network training. This system supports visualization and quantification of memory-runtime tradeoffs through piecewise-linear fitting and provides module-level detailed statistics.",
            "total_tokens": 9872,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 18.379591,
            "uuid": "1d58af02-7459-456c-8f43-2b313bb443e0"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/_tools/sac_estimator.py",
            "number_of_lines": 963,
            "project_name": "pytorch",
            "file_summary": "The code is a Python script that provides a context manager called SACEstimator for estimating the memory and recomputation time trade-offs of functions or torch.nn.Module for Selective Activation Checkpointing (SAC). It analyzes PyTorch operations and modules to gather statistics and construct a trade-off graph for activation checkpointing. The script supports two estimation modes: operator-level-benchmark and operator-level-cost-model, allowing users to choose how runtimes are estimated.",
            "total_tokens": 11262,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 2.744333,
            "uuid": "b10b22f1-7762-4219-8d9b-01058a657b94"
        }
    },
    "/torch/onnx/operators.py": {
        "gpt-4o": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The file `operators.py` provides utility functions for operations that assist in exporting PyTorch models via ONNX, specifically focusing on dynamic size operations to ensure traceability. It includes two key functions: `shape_as_tensor`, which extracts the shape of a tensor as a tensor, and `reshape_from_tensor_shape`, which reshapes a tensor dynamically based on a target shape. Both functions are kept for backward compatibility as they are now implemented directly in ATen.",
            "total_tokens": 457,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 3.296443,
            "uuid": "8152eeea-559d-4280-9ff4-06a4faf37f26"
        },
        "Llama3.1": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"operators.py\" file is to provide operators for exporting models via ONNX, specifically for handling dynamic size operations. The file contains two functions: `shape_as_tensor` and `reshape_from_tensor_shape`, which are used to get the shape of a tensor as a tensor and reshape a tensor to a given shape, respectively. These functions are kept for backward-compatibility, as they are now implemented directly in ATen.",
            "total_tokens": 458,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 5.665142,
            "uuid": "8b7b1354-9251-4929-ad6a-6f240589c418"
        },
        "nova-pro-v1": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The `operators.py` file provides backward-compatible operators, such as `shape_as_tensor` and `reshape_from_tensor_shape`, to facilitate exporting models via ONNX by making dynamic size operations traceable. These operators are now implemented directly in ATen, but the file is maintained for compatibility purposes.",
            "total_tokens": 451,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 2.019123,
            "uuid": "1f8b77a7-51ac-428e-9bc5-abd451db2196"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"operators.py\" file is to provide a set of functions for exporting models via onnx, specifically focusing on making dynamic size operations traceable. The file includes functions like `shape_as_tensor` to get the shape of a tensor as a tensor and `reshape_from_tensor_shape` to reshape a tensor to a given shape, all kept for backward-compatibility even though similar functions are now implemented directly in ATen. The file emphasizes the importance of these functions in ensuring traceability during model export.",
            "total_tokens": 467,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 3.964909,
            "uuid": "d8629331-975f-4590-87f3-e300e641c73b"
        },
        "gpt-4": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The Python file \"operators.py\" is designed to facilitate the export of models through ONNX by making dynamic size operations traceable. It consists primarily of two functions: `shape_as_tensor`, which retrieves the shape of a tensor as a tensor itself, and `reshape_from_tensor_shape`, used to reshape a tensor to a given shape for traceability in model exporting. The file is also maintained for backward-compatibility purposes with some functionalities now being directly implemented in ATen.",
            "total_tokens": 457,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 6.321315,
            "uuid": "0ff9efef-c10b-4205-a474-97240916d8f5"
        },
        "gpt-4.1": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The file defines two operators, shape_as_tensor and reshape_from_tensor_shape, which make tensor shape operations traceable for ONNX export by delegating to internal PyTorch (ATen) implementations. These functions enable dynamic size operations to be captured in model export workflows, particularly useful for exporting PyTorch models to ONNX format. The file is maintained for backward compatibility, as the actual functionality is now directly provided by PyTorch core.",
            "total_tokens": 452,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 5.04433,
            "uuid": "c5e96360-9da4-4acf-98b2-5744abcf5d96"
        },
        "mistral-small": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": " The main functionality of the file \"operators.py\" is to provide operators that aid in exporting models via ONNX. The two primary operators are `shape_as_tensor` and `reshape_from_tensor_shape`, which are used to make all dynamic size operations traceable. These functions are kept for backward-compatibility as they have been implemented directly in ATen.",
            "total_tokens": 532,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.136143,
            "uuid": "64dd0650-fbdc-418b-851d-634e487b8d23"
        },
        "Claude3.7": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The file \"operators.py\" provides compatibility functions for ONNX model export, specifically maintaining backward compatibility for shape manipulation operations that have since been implemented directly in ATen. It defines two main functions: `shape_as_tensor` which gets a tensor's shape as a tensor, and `reshape_from_tensor_shape` which reshapes a tensor according to a shape tensor, both of which are now simply wrappers around native PyTorch implementations.",
            "total_tokens": 529,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.031604,
            "uuid": "621fb074-2fdd-45ad-8bbb-16ad77bc23de"
        },
        "gpt-4.5": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The Python file provides operators specifically to help export models via ONNX by making dynamic tensor-size operations traceable, wrapping two core functionalities (`shape_as_tensor` and `reshape_from_tensor_shape`). These operators are now directly implemented in PyTorch's ATen backend, so the functions in this file are maintained solely for backward compatibility. Essentially, the file defines legacy wrappers for PyTorch internal utilities that manage tensor shapes when exporting dynamic models.",
            "total_tokens": 452,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 8.48261,
            "uuid": "24b09617-451a-4d3c-9d1a-036d0436edf8"
        },
        "mistral-7b": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": " This Python file, named \"operators.py,\" contains two functions, `shape_as_tensor` and `reshape_from_tensor_shape`, which are used to make dynamic size operations traceable when exporting PyTorch models to Open Neural Network Exchange (ONNX) format. These functions were previously implemented in this file but have since been moved to PyTorch's ATen library. The file is kept for backward-compatibility purposes.",
            "total_tokens": 552,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.609746,
            "uuid": "c49b7a98-fa45-4dda-8fef-d9849b669920"
        },
        "titan_premier": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The file \"operators.py\" is a Python file that contains two functions: \"shape_as_tensor\" and \"reshape_from_tensor_shape\". The \"shape_as_tensor\" function takes a tensor as input and returns a tensor of shape [len(x.shape)] containing the size of each dimension of x. The \"reshape_from_tensor_shape\" function reshapes a tensor to the given shape, which is useful for making dynamic size operations traceable when exporting models via ONNX. The file is kept for backward-compatibility, as these functions are now implemented directly in ATen.",
            "total_tokens": 129,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 7.802581,
            "uuid": "4f8e885c-0abf-44d5-862f-9b4fe9b2bd8f"
        },
        "Llama3.3": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the `operators.py` file is to provide operators that aid in exporting models via ONNX, specifically making dynamic size operations traceable. The file contains two key functions: `shape_as_tensor` and `reshape_from_tensor_shape`, which are used to get the shape of a tensor as a tensor and reshape a tensor to a given shape, respectively. Although these functions are now implemented directly in ATen, this file is maintained for backward-compatibility purposes.",
            "total_tokens": 464,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 1.31987,
            "uuid": "7735ced2-6ec9-436a-bd9b-a997bb209975"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "This file defines two functions, `shape_as_tensor` and `reshape_from_tensor_shape`, which are aliases for internal PyTorch operations. These functions were originally created to help make dynamic tensor shape operations traceable for ONNX export. The file is now primarily kept for backward compatibility as the core functionality is implemented directly in PyTorch.",
            "total_tokens": 904,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 8.340036,
            "uuid": "5a5cd799-3e3b-42bd-be45-385407084514"
        },
        "nova-lite-v1": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"operators.py\" file is to provide backward-compatible operators for exporting models using ONNX, specifically for converting tensor shapes to tensors and reshaping tensors from tensor shapes. These functions, `shape_as_tensor` and `reshape_from_tensor_shape`, are retained for compatibility even though their implementations have been integrated into ATen.",
            "total_tokens": 462,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 2.011175,
            "uuid": "ad31eefc-bd8d-4935-a525-4a64a05898ab"
        },
        "Claude3.5": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The file \"operators.py\" primarily serves as a compatibility layer for ONNX model export operations in PyTorch. It defines two key functions, 'shape_as_tensor' and 'reshape_from_tensor_shape', which are now directly implemented in ATen but are kept here for backward compatibility. The file aims to make dynamic size operations traceable when exporting models via ONNX.",
            "total_tokens": 520,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 5.388524,
            "uuid": "f28ac19a-a6ef-4a4a-b328-b732523b7952"
        },
        "cohere-v1": {
            "file_name": "/torch/onnx/operators.py",
            "number_of_lines": 48,
            "project_name": "pytorch",
            "file_summary": "The operators.py file defines two functions, shape_as_tensor and reshape_from_tensor_shape, which are used for ONNX model export. These functions help in making dynamic size operations traceable and are implemented for backward compatibility. They are part of the torch library and are used to manipulate the shape of tensors.",
            "total_tokens": 477,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 6.341549,
            "uuid": "8bffb94b-c8b5-4e7a-abd6-9e9b06883cec"
        }
    },
    "/torch/distributed/optim/apply_optimizer_in_backward.py": {
        "titan_premier": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the file \"apply_optimizer_in_backward.py\" is to apply an optimizer to a set of parameters during the backward pass of the autograd engine. It allows for the optimization process to occur earlier in the computation graph, potentially reducing memory usage and improving performance. The file includes functions to apply the optimizer to parameters, retrieve the optimizers applied to a module's parameters, and register hooks to run the optimizer after the gradient for a parameter is accumulated. The file also uses weak references to store relevant metadata for the Tensor/Parameter, allowing for efficient memory management.",
            "total_tokens": 126,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 166.227036,
            "uuid": "a63a1b19-2050-4546-ab7e-4551c41b9cc6"
        },
        "gpt-4": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The Python script `apply_optimizer_in_backward.py` implements functionality to apply optimizers directly in the backward pass of a neural network module using PyTorch. It allows specifying different optimizers for different parameters right after their respective gradients are accumulated during backward propagation. The script also includes utilities for managing optimizer states and hooks, ensuring that optimizers are applied once per gradient accumulation, thus potentially optimizing the update process in deep learning training routines.",
            "total_tokens": 1312,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 4.952679,
            "uuid": "12707c87-3b5d-4421-af28-a6b885e17423"
        },
        "Llama3.1": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The main functionality of this file is to provide a way to apply an optimizer to a PyTorch model's parameters during the backward pass, allowing the optimizer to update the parameters immediately after the gradients have been accumulated. This is achieved through the `_apply_optimizer_in_backward` function, which takes an optimizer class, a list of parameters, and optimizer keyword arguments as input. The file also provides a `_get_in_backward_optimizers` function to retrieve the list of in-backward optimizers applied to a model's parameters.",
            "total_tokens": 1337,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 4.118837,
            "uuid": "7d530aa8-87cd-48af-9bb1-b3daf5f8f676"
        },
        "Claude3.5": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"apply_optimizer_in_backward.py\" file is to implement a mechanism for applying optimizers during the backward pass of a neural network. It provides a function `_apply_optimizer_in_backward` that allows users to specify different optimizers for different parameters, which are then automatically applied after the gradients are computed during backpropagation. Additionally, the file includes a utility function `_get_in_backward_optimizers` to retrieve the list of optimizers applied to a module's parameters using this technique.",
            "total_tokens": 1654,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.931855,
            "uuid": "a4256d84-3297-4ff0-99be-0077c8a4c89a"
        },
        "gpt-4o": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The file defines a mechanism for applying optimizers during the backward pass in PyTorch, allowing optimizers to act directly on parameters after their gradients are calculated. It provides functions to register optimizers and hooks on parameters (`_apply_optimizer_in_backward`) and retrieve a list of such optimizers from a given module (`_get_in_backward_optimizers`). This approach modifies the typical training workflow by automating the application of optimization steps within the backward pass itself.",
            "total_tokens": 1318,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 3.526714,
            "uuid": "fc1a4b56-cc47-4678-b346-ab179de25620"
        },
        "gpt-4.5": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The provided Python file defines a mechanism to apply an optimizer directly during the backward gradient computation phase for PyTorch parameters, enabling automatic optimization immediately after gradient accumulation. By registering autograd hooks, optimizers execute their updates right as gradients become ready, after which gradients are cleared automatically. It includes helper functions for associating specific optimizer instances to each parameter, tracking these associations via weak references, and retrieving optimizers for modules as needed.",
            "total_tokens": 1312,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 8.820504,
            "uuid": "f851b50c-2864-4661-8bf1-9ad03dd807ca"
        },
        "nova-pro-v1": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the file \"apply_optimizer_in_backward.py\" is to apply specified optimizers to given parameters during the backward pass in PyTorch, allowing for optimizers to be executed immediately after gradients are accumulated. It provides a mechanism to register hooks that trigger optimizer steps and retrieves the optimizers applied in this manner. The file utilizes weak references to manage metadata associated with tensors and parameters without affecting their lifetime.",
            "total_tokens": 1556,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 1.29235,
            "uuid": "c84557fb-165b-43f4-8cdb-2e33e13ad71b"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": " The main functionality of the file \"apply_optimizer_in_backward.py\" is to apply an optimizer to specific parameters in a PyTorch model during the backward pass. This is done using the `_apply_optimizer_in_backward` function, which registers a hook that runs the optimizer after the gradient for the specified parameter is accumulated. The file also provides a function `_get_in_backward_optimizers` to retrieve the in-backward optimizers applied to a model's parameters.",
            "total_tokens": 1729,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.986453,
            "uuid": "22b9161f-b070-4401-ba79-0c1253320b98"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"apply_optimizer_in_backward.py\" file is to provide a function called \"_apply_optimizer_in_backward\" that applies a specified optimizer to parameters during the backward pass in PyTorch. The function takes input parameters such as the optimizer class, parameters to apply the optimizer to, optimizer arguments, and a flag to register a hook to run the optimizer after gradient accumulation. Additionally, the file includes a helper function \"_get_in_backward_optimizers\" to retrieve a list of optimizers applied to a given module's parameters for specific use cases like checkpointing.",
            "total_tokens": 1341,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 2.919349,
            "uuid": "aa7ffc2d-07c6-4b82-a1f6-8ff5f114c093"
        },
        "gpt-4.1": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The main functionality of this file is to enable the application of optimizer steps directly during the backward pass in PyTorch, by registering hooks on parameters so their optimizers are applied immediately after their gradients are accumulated. It provides a function to set up this mechanism per-parameter with different optimizers and configurations, which is useful for advanced training patterns or distributed training. Additionally, the file includes a method to retrieve the list of these in-backward optimizers associated with a given module.",
            "total_tokens": 1322,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 3.46413,
            "uuid": "7fc1822f-848a-4bc3-a54b-acef4485b68d"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "This file implements functionality for applying optimizers during the backward pass in PyTorch, allowing parameter updates to occur immediately after gradients are computed without requiring a separate optimizer step call. It provides the `_apply_optimizer_in_backward` function which registers hooks on parameters to trigger optimization when gradients are ready, and includes a helper function `_get_in_backward_optimizers` to retrieve all optimizers applied to a module's parameters.",
            "total_tokens": 1631,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.307516,
            "uuid": "9a99529d-68fc-4647-8dd8-44105cdd0e67"
        },
        "Llama3.3": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the given Python file is to apply an optimizer to a set of parameters in a PyTorch model during the backward pass, allowing for per-parameter optimization. The `_apply_optimizer_in_backward` function takes an optimizer class, parameters, and optimizer keyword arguments, and applies the optimizer to each parameter after the gradient has been accumulated. This allows for custom optimization strategies, such as applying different optimizers to different parts of the model, and is useful for techniques like checkpointing and distributed training.",
            "total_tokens": 1333,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 1.092634,
            "uuid": "096a8519-50af-4c01-9af1-517cd72207e0"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": " The given Python file, \"apply\\_optimizer\\_in\\_backward.py\", defines a function `_apply_optimizer_in_backward` that applies an optimizer to a PyTorch model's parameters during the backward pass. The optimizer is registered as a hook on the parameter's autograd function, ensuring it is applied after the gradient has been accumulated. This allows for multiple optimizers to be applied to different parameters within a single PyTorch model.",
            "total_tokens": 1722,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.261026,
            "uuid": "d3dbc92c-a51b-4560-8d70-dded50a967ca"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "This Python file provides functionality to apply PyTorch optimizers to specific parameters during the backward pass, rather than after the entire backward computation. It registers hooks on parameter gradient accumulation functions to trigger the optimizer step once the gradient is computed for that parameter. This allows for fine-grained control over parameter updates, potentially using different optimizers or settings for different parameters within a model.",
            "total_tokens": 1769,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 5.640592,
            "uuid": "ff5da1b9-ba5d-4ccb-9992-a1d02aa967cb"
        },
        "nova-lite-v1": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "This Python file defines a utility function `_apply_optimizer_in_backward` for applying an optimizer to PyTorch model parameters during the backward pass of the autograd computation graph. It allows specifying different optimizers for different parameters and ensures that the optimizers are executed after gradients are accumulated but before they are used by other optimizers. The file also includes a helper function `_get_in_backward_optimizers` to retrieve the optimizers applied to a given model's parameters.",
            "total_tokens": 1568,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 0.929548,
            "uuid": "1d427e7e-99ff-48f0-b3f0-a1886c993f3d"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/optim/apply_optimizer_in_backward.py",
            "number_of_lines": 122,
            "project_name": "pytorch",
            "file_summary": "The Python script applies one or more optimizers to specific parameters of a model. It uses a weak dictionary to store meta-data for tensors and parameters, and defines functions to manage and execute these optimizers during the backward pass. The optimizers are applied after gradients are accumulated, and the function `_apply_optimizer_in_backward` registers hooks to fire the optimizers accordingly.",
            "total_tokens": 1478,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.188565,
            "uuid": "2ff34c0d-8d51-4ef7-ab7c-3273b61460dd"
        }
    },
    "/torch/onnx/_internal/exporter/_flags.py": {
        "mistral-7b": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": " This Python file, named \"_flags.py\", defines an internal flag for ONNX export and provides a decorator function `set_onnx_exporting_flag` to set and reset the flag. When a function is decorated with this flag, it will be executed only when ONNX export is enabled.",
            "total_tokens": 374,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 0.954266,
            "uuid": "89f80541-e923-419b-9c36-8cb47bcc4862"
        },
        "Claude3.7": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "This file provides functionality for tracking whether ONNX export is in progress through a global flag `_is_onnx_exporting`. It defines a decorator `set_onnx_exporting_flag` that temporarily sets this flag to True during the execution of the decorated function and ensures it's reset to False afterward, even if exceptions occur. The module is designed to help other parts of the codebase determine if they're being executed during an ONNX export process.",
            "total_tokens": 404,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 3.360393,
            "uuid": "7c7a35be-4ec3-4ff1-9387-bfe84b22166b"
        },
        "gpt-4o": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The file `_flags.py` provides functionality to manage an internal flag (`_is_onnx_exporting`) that indicates whether an ONNX export operation is ongoing. It includes a decorator function `set_onnx_exporting_flag` that temporarily sets this flag to `True` while executing a wrapped function and resets it to `False afterwards, even if an exception occurs. This mechanism is useful for tracking the export state during ONNX-related operations.",
            "total_tokens": 347,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 2.443967,
            "uuid": "a5f23703-3c92-4b30-9d17-20849069272b"
        },
        "gpt-4.1": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The file defines and manages an internal flag _is_onnx_exporting, which indicates whether ONNX export is currently in progress. It provides a decorator set_onnx_exporting_flag that temporarily sets this flag to True during the execution of a wrapped function and reliably resets it to False afterward, even if an exception is raised. This mechanism is intended for internal use to control behavior specific to ONNX export operations.",
            "total_tokens": 341,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 2.883622,
            "uuid": "c717d26b-8b1b-420a-83e2-5b1b4c45a02b"
        },
        "nova-pro-v1": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The file `_flags.py` defines internal flags and utilities specifically for ONNX export operations. It includes a global flag `_is_onnx_exporting` to indicate whether ONNX exporting is currently active, and a decorator `set_onnx_exporting_flag` to set this flag during the execution of decorated functions, ensuring it is reset afterward.",
            "total_tokens": 346,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 0.932959,
            "uuid": "72eb2875-776a-483d-a554-a8c85e3b6c60"
        },
        "gpt-4": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The Python file \"_flags.py\" primarily handles the state of an internal flag (`_is_onnx_exporting`) used for controlling whether an ONNX export is ongoing. It includes a decorator `set_onnx_exporting_flag` that, when applied to a function, sets this flag to True before executing the function and resets it to False afterwards, even if an exception occurs. This functionality aids in maintaining and adjusting the environment state specifically during the ONNX export processes.\n",
            "total_tokens": 345,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 6.209977,
            "uuid": "755b05f1-4ad0-4673-82de-d112282fd648"
        },
        "mistral-small": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": " The main functionality of the file \"_flags.py\" is to manage an internal flag for ONNX export. It defines a global variable `_is_onnx_exporting` which is set to True when the decorated function is called using the `set_onnx_exporting_flag` decorator, and then reset to False after the function execution. This flag can be used to conditionally execute certain code paths during ONNX export.",
            "total_tokens": 398,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 1.749047,
            "uuid": "46187370-363f-415c-8454-e1a299e30129"
        },
        "titan_premier": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the _flags.py file is to manage an internal flag for ONNX export. The file contains a global variable _is_onnx_exporting, which is initially set to False. The file also includes a decorator function, set_onnx_exporting_flag, that can be used to wrap other functions. When a function is wrapped with this decorator, the _is_onnx_exporting flag is set to True before the function is called and then reset to False after the function completes, regardless of whether an exception occurs. This decorator can be used to temporarily enable ONNX export mode for specific functions.",
            "total_tokens": 141,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 915.828038,
            "uuid": "a0dabf55-14c4-4dd5-9164-9296b26ec16c"
        },
        "nova-lite-v1": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the `_flags.py` file is to provide a mechanism for setting a global flag indicating whether the ONNX export process is currently active. This is achieved through a decorator, `set_onnx_exporting_flag`, which temporarily sets the `_is_onnx_exporting` flag to `True` during the execution of the decorated function and ensures it is reset to `False` afterward, regardless of whether an exception occurs. This allows functions to behave differently when called within the context of an ONNX export.",
            "total_tokens": 382,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 0.78536,
            "uuid": "4f55fe9a-9240-4227-88bc-52a1e5e05e40"
        },
        "Llama3.1": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The main functionality of this Python file is to manage an internal flag `_is_onnx_exporting` that tracks whether the system is currently exporting to ONNX format. The file provides a decorator `set_onnx_exporting_flag` that sets this flag to `True` when a decorated function is called and resets it to `False` when the function finishes executing, even if an exception occurs. This allows other parts of the system to check the state of this flag to determine if they should behave differently during ONNX export.",
            "total_tokens": 362,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 4.028727,
            "uuid": "d15d723e-8f68-49f7-b823-926f9cbea78f"
        },
        "gpt-4.5": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The provided Python file defines internal functionality to manage a global flag `_is_onnx_exporting`, indicating if an ONNX export is currently happening. It provides the decorator `set_onnx_exporting_flag` that sets this flag to `True` before a function runs and resets it to `False` afterward, even if exceptions occur. This mechanism allows the codebase to track ONNX export operations consistently and safely.",
            "total_tokens": 342,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 7.139382,
            "uuid": "a1d60f42-7fd1-43e3-88ba-ae27308bbae9"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"_flags.py\" file is to provide internal flags for controlling the ONNX export process. It defines a function \"set_onnx_exporting_flag\" which wraps another function and sets a global flag \"_is_onnx_exporting\" to True during the execution of the wrapped function, resetting it back to False after the function execution, ensuring it is always reset even if an exception occurs.",
            "total_tokens": 333,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 2.323177,
            "uuid": "a7ba5e86-70f2-4717-aa00-4eb9c1e72640"
        },
        "Claude3.5": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The \"_flags.py\" file defines internal flags for ONNX export in Python. It primarily provides a decorator function `set_onnx_exporting_flag` that sets a global flag `_is_onnx_exporting` to True during the execution of the decorated function and resets it to False afterward, ensuring the flag is reset even if an exception occurs. This functionality is likely used to control behavior specific to ONNX export operations within a larger codebase.",
            "total_tokens": 406,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.829713,
            "uuid": "c980cbe8-da1f-45b4-ae67-4f805e018c58"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "This Python file defines an internal global flag `_is_onnx_exporting` used to indicate whether code is currently running within an ONNX export process. It provides a decorator `set_onnx_exporting_flag` that temporarily sets this flag to `True` for the duration of the decorated function's execution. The flag is automatically reset to `False` afterwards, even if errors occur.",
            "total_tokens": 748,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 4.635208,
            "uuid": "01974073-e449-4c35-a90d-d112a6fc1b7e"
        },
        "Llama3.3": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the `_flags.py` file is to manage an internal flag `_is_onnx_exporting` that tracks whether the system is currently exporting to ONNX format. The `set_onnx_exporting_flag` function is a decorator that sets this flag to `True` when a decorated function is called and resets it to `False` after the function execution is complete, even if an exception occurs. This allows other parts of the system to check the state of this flag and adjust their behavior accordingly during ONNX export.",
            "total_tokens": 364,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 0.989212,
            "uuid": "d1054f94-523b-4f73-9f96-6236f3b8af65"
        },
        "cohere-v1": {
            "file_name": "/torch/onnx/_internal/exporter/_flags.py",
            "number_of_lines": 26,
            "project_name": "pytorch",
            "file_summary": "The _flags.py file manages a global flag called _is_onnx_exporting, which is used to control the behavior of ONNX exporting. It defines a decorator set_onnx_exporting_flag that modifies the behavior of the decorated function, setting the flag to True during its execution and ensuring it resets to False afterward. This mechanism appears to be part of a larger system for enabling or tracking ONNX export functionality.",
            "total_tokens": 346,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.193559,
            "uuid": "d2333049-598f-4bcd-adb1-30378b9b9e8d"
        }
    },
    "/torch/nn/quantized/_reference/modules/utils.py": {
        "nova-pro-v1": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The `utils.py` file serves as a transitional compatibility layer during the migration of quantized reference modules from their current location to `torch/ao/nn/quantized/reference`. It imports essential functions and classes from the new module location to ensure continued functionality, while directing any new additions to the appropriate files in the new directory.",
            "total_tokens": 320,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 0.89012,
            "uuid": "61a39781-6691-493c-b95c-8331b1505cb8"
        },
        "gpt-4": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The Python file \"utils.py\" seems to be a part of a migration process to restructure quantized reference modules into a new directory structure under `torch/ao/nn/quantized/reference`. The file currently imports functionality such as weight quantization methods and a class `ReferenceQuantizedModule` from a new location indicated by its path. The file is maintained for compatibility during this transition and serves as an intermediary for importing newly located methods and classes.",
            "total_tokens": 327,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 5.424192,
            "uuid": "5446a8c5-9472-4e4d-a4cd-cf5f223a04a5"
        },
        "gpt-4.1": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "This file serves as a compatibility wrapper for quantized reference modules during their migration to a new location in the PyTorch library. It imports key utility functions and classes related to quantized modules from torch.ao.nn.quantized.reference.modules.utils and re-exports them for continued access. The file instructs developers to add new functionalities to the new directory structure and only update imports here as necessary.",
            "total_tokens": 315,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 2.801406,
            "uuid": "2078aa27-e0b7-481b-86e9-cb0162b3d88c"
        },
        "nova-lite-v1": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"utils.py\" file is to provide utility functions for quantized neural network modules, specifically for handling weight quantization parameters and operations. It serves as a transitional module during the migration of its contents to a new location within the torch library. The file imports several functions and a class from a submodule to facilitate quantized operations in neural networks.",
            "total_tokens": 325,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 0.762238,
            "uuid": "7b826041-88e9-4b4f-ab4c-a12aba82438a"
        },
        "gpt-4.5": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The provided Python file serves as a compatibility wrapper during the migration of quantized reference modules to the new location (`torch/ao/nn/quantized/reference`). Its primary purpose is importing relevant utilities and components from the new module path to ensure backward compatibility. Any new functionality should be directly added to the new module location, while this file remains a temporary import redirector.",
            "total_tokens": 311,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 10.634232,
            "uuid": "d5620181-f75d-4c25-ba75-2ee11eea35e6"
        },
        "Llama3.3": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the `utils.py` file is to provide a collection of utility functions and classes for working with quantized reference modules in PyTorch. The file imports various functions and a class from `torch.ao.nn.quantized.reference.modules.utils`, which are used for quantizing and dequantizing weights, saving weight parameters, and creating reference quantized modules. The file serves as a compatibility layer during the migration of these utilities to a new location within the PyTorch library.",
            "total_tokens": 341,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 0.931303,
            "uuid": "29587c91-5c4b-4092-b027-e2e69c01798a"
        },
        "gpt-4o": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "This Python file serves as a compatibility bridge during the migration of quantized reference modules to `torch/ao/nn/quantized/reference`. It re-exports several utility functions and classes related to quantization, such as methods for handling, quantizing, and saving weight parameters, from the new module location. Its primary role is to ensure backward compatibility while the migration process is ongoing.",
            "total_tokens": 314,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 2.857494,
            "uuid": "d8b36921-570b-4aef-bfac-0f86109396f8"
        },
        "Claude3.5": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"utils.py\" file is to serve as a compatibility layer during the migration process of quantized reference modules. It imports utility functions and a base class from the new location 'torch.ao.nn.quantized.reference.modules.utils', while maintaining backward compatibility for existing code that may still reference this file.",
            "total_tokens": 351,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.914992,
            "uuid": "0644a9d2-43d1-4002-be97-33e654017ec2"
        },
        "Claude3.7": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "This file serves as a compatibility layer during the migration of quantized reference modules from the current location to `torch/ao/nn/quantized/reference`. It imports and re-exports utility functions and classes related to quantized neural network modules, maintaining backward compatibility while the migration process is ongoing.",
            "total_tokens": 340,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 2.918737,
            "uuid": "0fadb4a7-ef7c-4c3a-8dde-c5493c4c4735"
        },
        "Llama3.1": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the file \"utils.py\" is to provide utility functions and classes for quantized reference modules in PyTorch, specifically for weight quantization and dequantization. The file imports various functions and a class from another module, suggesting that it serves as a compatibility layer during a migration process. The functions and class imported are related to quantization and dequantization of weights, and saving weight quantization parameters.",
            "total_tokens": 328,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 3.305456,
            "uuid": "084e7fb6-2118-4ce7-b8f4-b6f57b5ba2d2"
        },
        "mistral-7b": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": " The given Python file, named \"utils.py\", is part of the Torch AutoGrad library for quantized reference modules. It imports functions and classes from the \"utils\" submodule of the \"torch/ao/nn/quantized/reference/modules\" package. The main functionality of this file includes quantizing and dequantizing weights, saving weight quantization parameters, and defining a ReferenceQuantizedModule class, which are essential for implementing quantized neural networks in the Torch AutoGrad library.",
            "total_tokens": 387,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 2.920973,
            "uuid": "d98cb05c-c189-4eda-b370-2222868daa06"
        },
        "titan_premier": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"utils.py\" file is to provide a set of utility functions for working with quantized neural network modules. These functions include _get_weight_qparam_keys, _quantize_and_dequantize_weight, _quantize_weight, _save_weight_qparams, and ReferenceQuantizedModule. The file is currently in the process of being migrated to a different location, and any new additions should be made to the appropriate file under the `torch/ao/nn/quantized/reference` directory while adding an import statement to this file.",
            "total_tokens": 128,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 3.040245,
            "uuid": "e5f2cbf6-b799-4648-ad00-aee15eb4aa2a"
        },
        "mistral-small": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": " The main functionality of the file \"utils.py\" is to serve as a temporary compatibility module for Quantized Reference Modules during the migration process to \"torch/ao/nn/quantized/reference\". It imports and provides various utility functions related to weight quantization, such as _get_weight_qparam_keys, _quantize_and_dequantize_weight, _quantize_weight, _save_weight_qparams, and the ReferenceQuantizedModule class.",
            "total_tokens": 381,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.802683,
            "uuid": "1d0f2e4c-e087-4138-ab57-15f7c923c827"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the 'utils.py' file is to define a module for Quantized Reference Modules within the torch framework. The module includes functions for handling quantized weights such as getting weight quantization parameter keys, quantizing and dequantizing weights, saving weight quantization parameters, and defining a ReferenceQuantizedModule. The module is undergoing migration to a new location in `torch/ao/nn/quantized/reference` for compatibility purposes.",
            "total_tokens": 326,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 2.259571,
            "uuid": "f690a335-2c06-4aed-92d8-c404415a3ac0"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "This Python file acts as a compatibility layer during the migration of quantized reference modules in PyTorch. It re-exports utility functions and the `ReferenceQuantizedModule` class from their new location in `torch.ao.nn.quantized.reference`. The file's primary function is to maintain backward compatibility for code importing these components from the old path.",
            "total_tokens": 691,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 2.817237,
            "uuid": "f68083ea-449c-44da-97fe-571d297bc541"
        },
        "cohere-v1": {
            "file_name": "/torch/nn/quantized/_reference/modules/utils.py",
            "number_of_lines": 19,
            "project_name": "pytorch",
            "file_summary": "The Python script named utils.py contains functions related to quantized reference modules used in neural networks. It appears to be in the process of migration and maintains compatibility by importing relevant functions from another module. The main functionality seems to be focused on quantizing and managing weights for neural network modules.",
            "total_tokens": 314,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 0.995471,
            "uuid": "bfb32e06-4c5c-44b9-baf2-9b97b9ad1532"
        }
    },
    "/torch/distributed/fsdp/_exec_order_utils.py": {
        "gpt-3.5-turbo": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The file \"_exec_order_utils.py\" contains classes and methods for tracking execution order, validating order consistency across ranks, and prefetching handles for forward and backward operations in Distributed FSDP. It includes functionalities to initialize data structures, record pre-forward and post-forward orders, check and warn about any deviations, and advance data structures per iteration, mainly focusing on handling distributed training and parameter synchronization in PyTorch.",
            "total_tokens": 3475,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 2.793986,
            "uuid": "c342d9e3-2684-4fc5-9149-dbf5b05ddf56"
        },
        "Llama3.1": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The main functionality of this file is to track and validate the execution order of forward and backward passes in a distributed training setting. It provides data structures and methods to record and check the order of handles (which represent parameters in a neural network) across different ranks in a distributed process group. The file also includes functionality for prefetching handles based on the recorded order.",
            "total_tokens": 3471,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 3.635286,
            "uuid": "eb6dc286-1d06-4f28-a276-ed66daf26094"
        },
        "gpt-4": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The Python file \"_exec_order_utils.py\" is centered around managing and verifying the execution order of handlers, specifically in the context of a distributed system using PyTorch's Fully Sharded Data Parallelism (FSDP). It includes classes and methods to track and validate the execution order upon the forward and backward passes of model parameters, ensuring consistency across different iterations and ranks within a distributed setting. Prefetching of parameter handlers during forward and backward passes for optimization, as well as detailed checking and warnings for any deviations in the execution order, are primary functionalities within this module.",
            "total_tokens": 3508,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 8.028978,
            "uuid": "93381e87-7009-4682-83c4-25adf91c4cc2"
        },
        "Claude3.7": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "This file implements utilities for tracking and validating execution order in PyTorch's Fully Sharded Data Parallel (FSDP) training, particularly focusing on parameter prefetching optimization. It provides mechanisms to record pre-forward and post-forward execution orders of FlatParamHandles, validate consistency across distributed ranks, and determine which parameters to prefetch for optimized backward and forward passes.",
            "total_tokens": 4352,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 4.501208,
            "uuid": "5fac4361-d054-447d-9199-38245658449e"
        },
        "Llama3.3": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the `_exec_order_utils.py` file is to track and validate the execution order of PyTorch modules in a distributed setting, ensuring that all ranks are executing the same parameters in the same order. The file provides a class `_ExecOrderData` that records the pre-forward and post-forward order of handles and checks for any deviations in the execution order across ranks. The class also provides methods to prefetch handles for backward and forward passes, and to advance the internal data structures per iteration.",
            "total_tokens": 3500,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 1.397159,
            "uuid": "1355f9de-4040-417d-bb40-befcd180fd7a"
        },
        "Claude3.5": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"_exec_order_utils.py\" file is to track and validate the execution order of FSDP (Fully Sharded Data Parallel) handles during model training. It provides utilities for recording pre-forward and post-forward orders, checking consistency across ranks, and managing prefetching for both backward and forward passes. The file also includes mechanisms for issuing warnings when deviations in execution order are detected, which is crucial for ensuring correct results and preventing potential hangs in distributed training scenarios.",
            "total_tokens": 4378,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 5.670815,
            "uuid": "b9959124-4aa9-486d-9619-2a0155d0e4a4"
        },
        "nova-lite-v1": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the `_exec_order_utils.py` file is to manage and validate the execution order of distributed operations in PyTorch's Fully Sharded Data Parallel (FSDP) framework. It tracks the order of parameter all-gathers during the forward and backward passes, ensuring consistency across different iterations and ranks. If discrepancies are detected, it issues warnings to alert the user about potential issues with the collective operations.",
            "total_tokens": 4083,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 1.097061,
            "uuid": "12df95bc-3126-4a4b-aebe-a64b1115abd2"
        },
        "titan_premier": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the file is to provide data structures and methods for tracking the execution order of a model during distributed training with Fully Sharded Data Parallel (FSDP). It includes methods for recording the pre-forward and post-forward order of model parameters, checking the execution order across ranks, and issuing warnings if the order deviates from the first iteration. The file also includes methods for getting the handles to backward and forward prefetch, which are used for optimizing communication in distributed training.",
            "total_tokens": 99,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 2.905601,
            "uuid": "b8d02e74-bb89-4ea3-8b27-dbe310948f88"
        },
        "gemini-2.5-flash": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "This Python file defines utilities for tracking and validating the execution order of `FlatParamHandle`s within PyTorch's FSDP. It records the order of handle usage during the forward pass to support parameter prefetching for both forward and backward passes. The file also includes logic to check for inconsistencies in the execution order across distributed ranks or across iterations, raising errors or warnings.",
            "total_tokens": 4771,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 4.045248,
            "uuid": "e31758c8-9b24-4c8a-b6d0-595b818acdf6"
        },
        "nova-pro-v1": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the `_exec_order_utils.py` file is to manage and validate the execution order of operations in a distributed training setup using PyTorch, specifically for forward and backward prefetching, and to ensure consistency across different ranks in a distributed environment.",
            "total_tokens": 4051,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 1.019951,
            "uuid": "a52bbe56-2f7e-49c1-9209-16999250b4fc"
        },
        "gpt-4.1": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The file _exec_order_utils.py provides utilities for tracking, validating, and managing the execution order of parameter all-gathers in Fully Sharded Data Parallel (FSDP) modules in PyTorch's distributed training. It defines data structures for recording the forward and backward execution order of parameter handles, checks consistency of all-gather operations across distributed ranks (with optional strictness based on debug level), and supports prefetching logic to optimize communication overlap. Additionally, it issues warnings or errors when deviations from expected execution order are detected, helping to ensure correctness and performance in distributed training workflows.",
            "total_tokens": 3529,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 4.186531,
            "uuid": "1e25e0f5-2b79-404b-b3e1-db483b4d99f4"
        },
        "gpt-4o": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The `_exec_order_utils.py` file defines tools for tracking and validating the execution order of operations in PyTorch's Fully Sharded Data Parallel (FSDP) training framework. It includes classes and mechanisms to monitor pre- and post-forward execution order across iterations, ensuring that distributed ranks maintain consistent order for correct results and debugging during collective operations. The file also provides functionality for handling deviations in execution order via warnings or errors depending on the setup and execution context.",
            "total_tokens": 3504,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 3.295075,
            "uuid": "49913d87-116c-4189-9aa4-99af496a02e9"
        },
        "mistral-small": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": " The provided Python file, \"_exec_order_utils.py\", is part of the PyTorch library and is used for managing the execution order of parameters in distributed data parallel training. The main functionality of the file is to define the `_ExecOrderData` class, which tracks the pre-forward and post-forward order of parameters, and provides methods for recording and checking these orders. This class is used to ensure that all ranks in a distributed training setup are all-gathering the same parameters and in the same order, which is crucial for correctness and performance. The file also includes various utility functions for handling handles, indices, and names of parameters.",
            "total_tokens": 4509,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 2.687016,
            "uuid": "87a8219e-830d-49db-b326-7a6d6b234fd9"
        },
        "gpt-4.5": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "This Python module supports execution order tracking and validation for PyTorch's Fully Sharded Data Parallel (FSDP), primarily designed to optimize parameter prefetching in the forward and backward pass. It records and checks the order in which module parameters are accessed or gathered during training iterations, ensuring consistency across iterations and across distributed nodes. Deviations from expected execution order result in runtime warnings or errors, assisting developers to identify issues that may lead to incorrect results or hangs during distributed training.",
            "total_tokens": 3507,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 10.74327,
            "uuid": "153548cc-88a4-48dd-8c8d-c54a75e4ac40"
        },
        "mistral-7b": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": " This Python file defines a class named `_ExecOrderData` which is used to track the execution order of parameters in a distributed deep learning model using PyTorch. The class initializes data structures for recording the pre-forward and post-forward orders of handles, and provides methods for checking the execution order across ranks and recording handles in the pre-forward order. The class also includes methods for getting handle indices and names from handle indices, and for getting the handles to forward and backward prefetch.",
            "total_tokens": 4479,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 1.671892,
            "uuid": "73799219-04fa-478d-81ea-dee2980c42a2"
        },
        "cohere-v1": {
            "file_name": "/torch/distributed/fsdp/_exec_order_utils.py",
            "number_of_lines": 365,
            "project_name": "pytorch",
            "file_summary": "The code defines classes to manage the execution order of distributed training, enabling forward prefetching and backward prefetching. It tracks parameter handles, validates execution order across ranks, and issues warnings for order deviations. The classes provide methods to record pre-forward and post-forward orders, check order consistency, and advance the internal state for each iteration.",
            "total_tokens": 3813,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.64493,
            "uuid": "f80a8225-ac93-4325-91ae-b9c6df6a2807"
        }
    },
    "/torchgen/dest/lazy_ir.py": {
        "nova-lite-v1": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"lazy_ir.py\" file is to generate lazy intermediate representation (IR) code for non-native PyTorch operations. It defines classes and functions to convert operation schemas into C++ classes that represent the lazy IR nodes, including handling of tensor arguments, shape inference, and fallback to eager execution when necessary. The generated code is designed to be backend-agnostic and can be specialized for specific backends by subclassing the provided base classes.",
            "total_tokens": 8024,
            "model": {
                "known_name": "nova-lite-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-lite-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 7.8e-05,
                "price_per_1000_output_tokens": 0.000312,
                "exclude_files": []
            },
            "latency": 1.47915,
            "uuid": "ab083f02-73e6-4cca-808e-f32449801926"
        },
        "gemini-2.5-flash": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "This Python file contains code generation logic for PyTorch's Lazy Tensor Core (LTC) backend. It defines classes and functions to generate C++ code for lazy Intermediate Representation (IR) nodes and their corresponding native function wrappers. The generated code handles node construction, shape inference, lowering to backend-specific operations, and dispatching from the PyTorch frontend.",
            "total_tokens": 8896,
            "model": {
                "known_name": "gemini-2.5-flash",
                "provider_name": "GOOGLE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 15,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0006,
                "exclude_files": []
            },
            "latency": 4.231289,
            "uuid": "3df68c6f-efad-42eb-912c-24c4dfa3ca23"
        },
        "Llama3.1": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The main functionality of this Python file is to generate code for lazy tensor operations in PyTorch. It defines several classes and functions that are used to generate C++ code for lazy tensor operations, including node constructors, shape inference, and lowering functions. The file appears to be part of a larger code generation system for PyTorch.",
            "total_tokens": 6663,
            "model": {
                "known_name": "Llama3.1",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-1-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0009,
                "price_per_1000_output_tokens": 0.0009,
                "exclude_files": []
            },
            "latency": 4.805734,
            "uuid": "3f6b4cd9-6f81-4c8d-8785-18b8c886c8bd"
        },
        "Llama3.3": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the provided Python file is to generate code for lazy tensor operations in PyTorch, specifically for creating and lowering lazy IR nodes. The file defines several classes and functions that work together to generate the necessary code, including `GenLazyIR`, `GenTSLazyIR`, and `GenLazyNativeFuncDefinition`. These classes use various inputs, such as function schemas and backend metadata, to produce C++ code that implements lazy tensor operations. The generated code includes node constructors, shape inference functions, and lowering functions that enable the execution of lazy tensor operations on different backends.",
            "total_tokens": 6715,
            "model": {
                "known_name": "Llama3.3",
                "provider_name": "AWS",
                "aws_model_id": "us.meta.llama3-3-70b-instruct-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00072,
                "price_per_1000_output_tokens": 0.00072,
                "exclude_files": []
            },
            "latency": 3.381791,
            "uuid": "a9c50b85-12d8-4806-8589-d24413362ad0"
        },
        "titan_premier": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the given Python file is to generate code for lazy IR nodes and shape inference functions. It defines several classes and functions to achieve this.\n\nThe `GenLazyIR` class is a base class that generates code for lazy IR nodes. It has a method called `gen` that generates the code for a lazy IR node based on a given schema. The schema contains information about the node's name, arguments, and other properties.\n\nThe `GenTSLazyIR` class is a subclass of `GenLazyIR` that generates code for lazy IR nodes in TypeScript. It overrides the `lowering_function`, `create_function`, and `can_be_reused_function` methods to generate TypeScript-specific code.\n\nThe `GenLazyNativeFuncDefinition` class generates code for lazy IR nodes in C++. It has methods to generate code for lazy tensor declarations, force eager fallback, metrics, device, shape inference, and bridge code.\n\nThe `ComputeShapeSignature` class generates the signature for shape inference functions.\n\nThe `GenLazyShapeInferenceDefinition` class generates code for shape inference functions.\n\nThe `generate_non_native_lazy_ir_nodes` function generates code for non-native lazy IR nodes.\n\nOverall, the file generates code for lazy IR nodes and shape inference functions in both TypeScript and C++.",
            "total_tokens": 296,
            "model": {
                "known_name": "titan_premier",
                "provider_name": "AWS",
                "aws_model_id": "amazon.titan-text-premier-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": false,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 8.550041,
            "uuid": "abde854c-a5e8-4c55-94d5-5bff5d839c6e"
        },
        "mistral-small": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": " The main functionality of the given Python file is to analyze and summarize the functionality of a provided Python file containing a senior developer's Python code. The file uses the `sys` module to simulate user input and extract the file name and content from the provided JSON. It then uses the `LazyIr` class, which is a part of the `torchgen` library, to analyze the provided Python file and generate a summary of its main functionality in up to three sentences. The `LazyIr` class is a tool for generating lazy IR code for PyTorch operations, and it uses various techniques such as abstract syntax tree (AST) parsing and code generation to analyze the provided code and generate a summary. The summary generated by the `LazyIr` class includes information about the operation being performed, the input and output types, and any relevant properties or attributes of the operation. The file also includes various helper functions and classes that are used by the `LazyIr` class to perform its analysis and generate the summary. Overall, the main functionality of the file is to provide a tool for analyzing and summarizing the functionality of PyTorch operations written in Python.",
            "total_tokens": 9147,
            "model": {
                "known_name": "mistral-small",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-small-2402-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.001,
                "price_per_1000_output_tokens": 0.003,
                "exclude_files": []
            },
            "latency": 5.26419,
            "uuid": "4d6376f7-ded2-4d46-ba44-83bb4f1d5aca"
        },
        "Claude3.5": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The main functionality of the \"lazy_ir.py\" file is to generate lazy IR (Intermediate Representation) nodes and related code for a lazy tensor computation backend. It provides classes and functions to:\n\n1. Generate IR node classes for lazy tensor operations, including their constructors, lowering functions, and other helper methods.\n\n2. Create native function definitions for lazy tensor operations, including shape inference, device handling, and conversion between lazy and eager tensors.\n\n3. Generate shape inference functions for lazy tensor operations, which are used to determine the output shapes of operations without actually executing them.",
            "total_tokens": 8942,
            "model": {
                "known_name": "Claude3.5",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 20,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 6.012192,
            "uuid": "3c7e3d4d-c17a-4584-b03f-84017ba4fa4b"
        },
        "nova-pro-v1": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The `lazy_ir.py` file is designed to generate code for lazy intermediate representation (IR) nodes in PyTorch, specifically for handling lazy tensor computations. It includes functions and classes to create node definitions, handle shape inference, and generate fallback code for eager execution when necessary.",
            "total_tokens": 7985,
            "model": {
                "known_name": "nova-pro-v1",
                "provider_name": "AWS",
                "aws_model_id": "eu.amazon.nova-pro-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00105,
                "price_per_1000_output_tokens": 0.0042,
                "exclude_files": []
            },
            "latency": 2.586704,
            "uuid": "7e205da9-7252-4018-b754-6c106b58e597"
        },
        "Claude3.7": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The `lazy_ir.py` file is responsible for generating C++ code for the Lazy Tensor Core (LTC) framework in PyTorch, which enables lazy evaluation of tensor operations. It provides classes and functions to generate IR (Intermediate Representation) nodes, shape inference code, and native function definitions that bridge between PyTorch's eager execution and the lazy execution model, supporting backends like XLA.",
            "total_tokens": 8905,
            "model": {
                "known_name": "Claude3.7",
                "provider_name": "AWS",
                "aws_model_id": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
                "aws_region": "eu-central-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.003,
                "price_per_1000_output_tokens": 0.015,
                "exclude_files": []
            },
            "latency": 8.907345,
            "uuid": "d7e15a0b-939b-4724-a1f9-4e0f1ee90754"
        },
        "gpt-4": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The \"lazy_ir.py\" Python file is primarily focused on generating IR (Intermediate Representation) code for lazy tensor operations in PyTorch. It defines classes and functions to handle transformation between Python-level lazy tensors and the corresponding C++ implementation using the Lazy Tensor Core (LTC) framework. The file includes methods to generate node constructor inputs, handle fallbacks to eager execution based on conditions, create Lazy IR nodes, and interfaces for lower-level IR operations including the templated backend-specific implementation. It also covers methods for invoking meta tensor operations for shape inference and translating tensor arguments, as essential parts of integrating native and non-native functions into a lazy evaluation environment.",
            "total_tokens": 6721,
            "model": {
                "known_name": "gpt-4",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.01,
                "price_per_1000_output_tokens": 0.03,
                "exclude_files": []
            },
            "latency": 9.076139,
            "uuid": "f41ddfa9-b9b7-4aab-a324-5d580119329e"
        },
        "mistral-7b": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": " This Python file defines classes and functions for generating lazy IR nodes for TorchScript. The main functionality of the file is to create classes for each native TorchScript operation, which can be used to generate the corresponding lazy IR nodes. These classes handle the conversion of input tensors to meta tensors, shape inference, and the creation of the actual TorchScript nodes. The file also includes functions for generating code for non-native (i.e., custom) operations. The primary goal is to enable the automatic generation of TorchScript code for various operations, making the process more efficient and streamlined.",
            "total_tokens": 9034,
            "model": {
                "known_name": "mistral-7b",
                "provider_name": "AWS",
                "aws_model_id": "mistral.mistral-7b-instruct-v0:2",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 65,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.00015,
                "price_per_1000_output_tokens": 0.0002,
                "exclude_files": []
            },
            "latency": 3.982078,
            "uuid": "25f55a60-9225-4ab5-9040-1e15b4e5134e"
        },
        "gpt-4.1": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The file \"lazy_ir.py\" is a Python code generation module that generates C++ code for \"lazy\" intermediate representation (IR) nodes, shape inference functions, and associated operator methods to support lazy tensor backends (such as XLA or lazy TorchScript) in PyTorch. It defines classes and utility functions that, based on operator schemas and backend metadata, programmatically write C++ classes and methods for constructing, using, and lowering lazy IR nodes, including device management, shape inference (possibly using meta tensors), eager fallback paths, and reuse logic. The module enables backend-specific customization and automation in generating the necessary glue C++ code to efficiently support lazy execution semantics for PyTorch operations.",
            "total_tokens": 6756,
            "model": {
                "known_name": "gpt-4.1",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.1",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.002,
                "price_per_1000_output_tokens": 0.008,
                "exclude_files": []
            },
            "latency": 5.097475,
            "uuid": "847aaf3b-f275-4099-84ce-79224d3c9688"
        },
        "gpt-3.5-turbo": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The given Python file is a script that generates Lazy IR (Intermediate Representation) nodes for a specific backend. It includes functions to construct Lazy IR nodes, handle shape inference, create Lazy Tensors, and return corresponding ATen Tensors. The script also includes the functionality to force eager fallback, obtain device information, and generate metrics. Additionally, it contains classes for Lazy IR node generation and shape inference definitions.",
            "total_tokens": 6671,
            "model": {
                "known_name": "gpt-3.5-turbo",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-35-turbo",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0005,
                "price_per_1000_output_tokens": 0.0015,
                "exclude_files": []
            },
            "latency": 5.847858,
            "uuid": "a2e7363a-4c0f-467b-a5d7-8bebaf3f6851"
        },
        "gpt-4.5": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The provided Python file is part of PyTorch's lazy tensor (LTC) backend generation framework, responsible for generating C++ code for lazy intermediate representation (IR) nodes, including node definitions, lazy-to-eager fallbacks, and shape inference. It defines classes like `GenLazyIR`, `GenTSLazyIR`, and `GenLazyNativeFuncDefinition` to handle code generation for node construction, lazy tensor creation, shape inference through meta tensors, and eager fallback mechanisms when necessary. It also generates the necessary binding methods between PyTorch native function schemas and LTC IR nodes.",
            "total_tokens": 6733,
            "model": {
                "known_name": "gpt-4.5",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4.5-preview",
                "azure_region": "eastus2",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.075,
                "price_per_1000_output_tokens": 0.15,
                "exclude_files": []
            },
            "latency": 22.56716,
            "uuid": "96ddbe05-ae1b-4841-82f9-fcad518a4f4b"
        },
        "gpt-4o": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The `lazy_ir.py` file is responsible for generating lazy IR (Intermediate Representation) nodes and corresponding methods that interact with native PyTorch operations for lazy evaluation. This includes creating IR nodes, handling metadata and shape inference, managing tensor-to-meta conversion, generating code for conditional eager fallbacks, and defining backend-specific lazy operations. It provides tools for customizing lazy IR behavior for backends, while supporting features such as symbolic shapes and tensor wrapping.",
            "total_tokens": 6705,
            "model": {
                "known_name": "gpt-4o",
                "provider_name": "AZURE",
                "aws_model_id": "",
                "aws_region": "",
                "azure_deployment_name": "gpt-4o",
                "azure_region": "eastus",
                "delay_time": 3,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0025,
                "price_per_1000_output_tokens": 0.01,
                "exclude_files": []
            },
            "latency": 6.240872,
            "uuid": "20d1a2c1-6f47-4147-8691-701ecec8818e"
        },
        "cohere-v1": {
            "file_name": "/torchgen/dest/lazy_ir.py",
            "number_of_lines": 708,
            "project_name": "pytorch",
            "file_summary": "The provided Python script appears to be a code generator for creating Lazy IR nodes. It uses templates and various heuristics to generate C++ code for classes representing lazy tensor operations, which delay the actual computation until they are evaluated. The code is customized based on the function schema, input arguments, and output requirements.",
            "total_tokens": 7730,
            "model": {
                "known_name": "cohere-v1",
                "provider_name": "AWS",
                "aws_model_id": "cohere.command-r-v1:0",
                "aws_region": "us-east-1",
                "azure_deployment_name": "",
                "azure_region": "",
                "delay_time": 60,
                "langchain_ready": true,
                "price_per_1000_input_tokens": 0.0015,
                "price_per_1000_output_tokens": 0.002,
                "exclude_files": [],
                "max_tokens": 3072
            },
            "latency": 1.994511,
            "uuid": "b1152842-1e3f-4be0-84e2-8405b90237db"
        }
    }
}